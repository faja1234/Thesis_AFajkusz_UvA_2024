{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-22T15:54:16.965897200Z",
     "start_time": "2024-06-22T15:54:12.875677700Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from torch.utils.data import DataLoader\n",
    "from models.dataset import LCDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                               Customer ID  \\\nLoan ID                                                                      \n273581de-85d8-4332-81a5-19b04ce68666  90a75dde-34d5-419c-90dc-1e58b04b3e35   \n8af915d9-9e91-44a0-b5a2-564a45c12089  af534dea-d27e-4fd6-9de8-efaa52a78ec0   \n2e841c8f-3dc1-464d-91c1-3d3d51e64c38  2ac05980-7848-4692-89ae-9321afe650f8   \n24e8c8bd-d10b-4dac-8b81-1da470ff5ecb  967e8733-7189-49b7-a3ab-6a1d0e1abdac   \n150ebbad-ebed-441e-b70d-2f350ad7dca6  40f729c9-54c7-4768-9fb5-2fa41d074c48   \n\n                                      Loan Status  Current Loan Amount  \\\nLoan ID                                                                  \n273581de-85d8-4332-81a5-19b04ce68666   Fully Paid             217646.0   \n8af915d9-9e91-44a0-b5a2-564a45c12089   Fully Paid             548746.0   \n2e841c8f-3dc1-464d-91c1-3d3d51e64c38   Fully Paid             234124.0   \n24e8c8bd-d10b-4dac-8b81-1da470ff5ecb   Fully Paid             666204.0   \n150ebbad-ebed-441e-b70d-2f350ad7dca6  Charged Off             317108.0   \n\n                                            Term  Credit Score  Annual Income  \\\nLoan ID                                                                         \n273581de-85d8-4332-81a5-19b04ce68666  Short Term         730.0      1184194.0   \n8af915d9-9e91-44a0-b5a2-564a45c12089  Short Term         678.0      2559110.0   \n2e841c8f-3dc1-464d-91c1-3d3d51e64c38  Short Term         727.0       693234.0   \n24e8c8bd-d10b-4dac-8b81-1da470ff5ecb   Long Term         723.0      1821967.0   \n150ebbad-ebed-441e-b70d-2f350ad7dca6   Long Term         687.0      1133274.0   \n\n                                     Home Ownership             Purpose  \\\nLoan ID                                                                   \n273581de-85d8-4332-81a5-19b04ce68666  Home Mortgage  Debt Consolidation   \n8af915d9-9e91-44a0-b5a2-564a45c12089           Rent  Debt Consolidation   \n2e841c8f-3dc1-464d-91c1-3d3d51e64c38           Rent  Debt Consolidation   \n24e8c8bd-d10b-4dac-8b81-1da470ff5ecb  Home Mortgage  Debt Consolidation   \n150ebbad-ebed-441e-b70d-2f350ad7dca6           Rent  Debt Consolidation   \n\n                                      Monthly Debt  Years of Credit History  \\\nLoan ID                                                                       \n273581de-85d8-4332-81a5-19b04ce68666      10855.08                     19.6   \n8af915d9-9e91-44a0-b5a2-564a45c12089      18660.28                     22.6   \n2e841c8f-3dc1-464d-91c1-3d3d51e64c38      14211.24                     24.7   \n24e8c8bd-d10b-4dac-8b81-1da470ff5ecb      17612.24                     22.0   \n150ebbad-ebed-441e-b70d-2f350ad7dca6       9632.81                     17.4   \n\n                                      Months since last delinquent  \\\nLoan ID                                                              \n273581de-85d8-4332-81a5-19b04ce68666                          10.0   \n8af915d9-9e91-44a0-b5a2-564a45c12089                          33.0   \n2e841c8f-3dc1-464d-91c1-3d3d51e64c38                          46.0   \n24e8c8bd-d10b-4dac-8b81-1da470ff5ecb                          34.0   \n150ebbad-ebed-441e-b70d-2f350ad7dca6                          53.0   \n\n                                      Number of Open Accounts  \\\nLoan ID                                                         \n273581de-85d8-4332-81a5-19b04ce68666                     13.0   \n8af915d9-9e91-44a0-b5a2-564a45c12089                      4.0   \n2e841c8f-3dc1-464d-91c1-3d3d51e64c38                     10.0   \n24e8c8bd-d10b-4dac-8b81-1da470ff5ecb                     15.0   \n150ebbad-ebed-441e-b70d-2f350ad7dca6                      4.0   \n\n                                      Number of Credit Problems  \\\nLoan ID                                                           \n273581de-85d8-4332-81a5-19b04ce68666                        1.0   \n8af915d9-9e91-44a0-b5a2-564a45c12089                        0.0   \n2e841c8f-3dc1-464d-91c1-3d3d51e64c38                        1.0   \n24e8c8bd-d10b-4dac-8b81-1da470ff5ecb                        0.0   \n150ebbad-ebed-441e-b70d-2f350ad7dca6                        0.0   \n\n                                      Current Credit Balance  \\\nLoan ID                                                        \n273581de-85d8-4332-81a5-19b04ce68666                122170.0   \n8af915d9-9e91-44a0-b5a2-564a45c12089                437171.0   \n2e841c8f-3dc1-464d-91c1-3d3d51e64c38                 28291.0   \n24e8c8bd-d10b-4dac-8b81-1da470ff5ecb                813694.0   \n150ebbad-ebed-441e-b70d-2f350ad7dca6                 60287.0   \n\n                                      Maximum Open Credit  Bankruptcies  \\\nLoan ID                                                                   \n273581de-85d8-4332-81a5-19b04ce68666             272052.0           1.0   \n8af915d9-9e91-44a0-b5a2-564a45c12089             555038.0           0.0   \n2e841c8f-3dc1-464d-91c1-3d3d51e64c38             107052.0           1.0   \n24e8c8bd-d10b-4dac-8b81-1da470ff5ecb            2004618.0           0.0   \n150ebbad-ebed-441e-b70d-2f350ad7dca6             126940.0           0.0   \n\n                                      Tax Liens  loan_status_num  \nLoan ID                                                           \n273581de-85d8-4332-81a5-19b04ce68666        0.0                0  \n8af915d9-9e91-44a0-b5a2-564a45c12089        0.0                0  \n2e841c8f-3dc1-464d-91c1-3d3d51e64c38        0.0                0  \n24e8c8bd-d10b-4dac-8b81-1da470ff5ecb        0.0                0  \n150ebbad-ebed-441e-b70d-2f350ad7dca6        0.0                1  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Customer ID</th>\n      <th>Loan Status</th>\n      <th>Current Loan Amount</th>\n      <th>Term</th>\n      <th>Credit Score</th>\n      <th>Annual Income</th>\n      <th>Home Ownership</th>\n      <th>Purpose</th>\n      <th>Monthly Debt</th>\n      <th>Years of Credit History</th>\n      <th>Months since last delinquent</th>\n      <th>Number of Open Accounts</th>\n      <th>Number of Credit Problems</th>\n      <th>Current Credit Balance</th>\n      <th>Maximum Open Credit</th>\n      <th>Bankruptcies</th>\n      <th>Tax Liens</th>\n      <th>loan_status_num</th>\n    </tr>\n    <tr>\n      <th>Loan ID</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>273581de-85d8-4332-81a5-19b04ce68666</th>\n      <td>90a75dde-34d5-419c-90dc-1e58b04b3e35</td>\n      <td>Fully Paid</td>\n      <td>217646.0</td>\n      <td>Short Term</td>\n      <td>730.0</td>\n      <td>1184194.0</td>\n      <td>Home Mortgage</td>\n      <td>Debt Consolidation</td>\n      <td>10855.08</td>\n      <td>19.6</td>\n      <td>10.0</td>\n      <td>13.0</td>\n      <td>1.0</td>\n      <td>122170.0</td>\n      <td>272052.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>8af915d9-9e91-44a0-b5a2-564a45c12089</th>\n      <td>af534dea-d27e-4fd6-9de8-efaa52a78ec0</td>\n      <td>Fully Paid</td>\n      <td>548746.0</td>\n      <td>Short Term</td>\n      <td>678.0</td>\n      <td>2559110.0</td>\n      <td>Rent</td>\n      <td>Debt Consolidation</td>\n      <td>18660.28</td>\n      <td>22.6</td>\n      <td>33.0</td>\n      <td>4.0</td>\n      <td>0.0</td>\n      <td>437171.0</td>\n      <td>555038.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2e841c8f-3dc1-464d-91c1-3d3d51e64c38</th>\n      <td>2ac05980-7848-4692-89ae-9321afe650f8</td>\n      <td>Fully Paid</td>\n      <td>234124.0</td>\n      <td>Short Term</td>\n      <td>727.0</td>\n      <td>693234.0</td>\n      <td>Rent</td>\n      <td>Debt Consolidation</td>\n      <td>14211.24</td>\n      <td>24.7</td>\n      <td>46.0</td>\n      <td>10.0</td>\n      <td>1.0</td>\n      <td>28291.0</td>\n      <td>107052.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>24e8c8bd-d10b-4dac-8b81-1da470ff5ecb</th>\n      <td>967e8733-7189-49b7-a3ab-6a1d0e1abdac</td>\n      <td>Fully Paid</td>\n      <td>666204.0</td>\n      <td>Long Term</td>\n      <td>723.0</td>\n      <td>1821967.0</td>\n      <td>Home Mortgage</td>\n      <td>Debt Consolidation</td>\n      <td>17612.24</td>\n      <td>22.0</td>\n      <td>34.0</td>\n      <td>15.0</td>\n      <td>0.0</td>\n      <td>813694.0</td>\n      <td>2004618.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>150ebbad-ebed-441e-b70d-2f350ad7dca6</th>\n      <td>40f729c9-54c7-4768-9fb5-2fa41d074c48</td>\n      <td>Charged Off</td>\n      <td>317108.0</td>\n      <td>Long Term</td>\n      <td>687.0</td>\n      <td>1133274.0</td>\n      <td>Rent</td>\n      <td>Debt Consolidation</td>\n      <td>9632.81</td>\n      <td>17.4</td>\n      <td>53.0</td>\n      <td>4.0</td>\n      <td>0.0</td>\n      <td>60287.0</td>\n      <td>126940.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_pickle('data/backup_data.pkl')\n",
    "data.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-22T15:54:22.220849Z",
     "start_time": "2024-06-22T15:54:22.058772Z"
    }
   },
   "id": "3f55943d9a06710"
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "outputs": [
    {
     "data": {
      "text/plain": "Index(['Customer ID', 'Loan Status', 'Current Loan Amount', 'Term',\n       'Credit Score', 'Annual Income', 'Home Ownership', 'Purpose',\n       'Monthly Debt', 'Years of Credit History',\n       'Months since last delinquent', 'Number of Open Accounts',\n       'Number of Credit Problems', 'Current Credit Balance',\n       'Maximum Open Credit', 'Bankruptcies', 'Tax Liens', 'loan_status_num'],\n      dtype='object')"
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2 = pd.read_pickle('data/backup_data.pkl')\n",
    "data2.columns"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-25T11:14:00.573593Z",
     "start_time": "2024-06-25T11:14:00.458968800Z"
    }
   },
   "id": "805aebf18886341"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "50028"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['loan_status_num'].sum()\n",
    "66242-16214"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-22T15:54:23.170056300Z",
     "start_time": "2024-06-22T15:54:23.091625100Z"
    }
   },
   "id": "dfe478989700d811"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "data.drop(['Loan Status', 'Customer ID'], axis=1, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-22T15:54:23.737974400Z",
     "start_time": "2024-06-22T15:54:23.691744400Z"
    }
   },
   "id": "fdd194905fc1b189"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Term', 'Home Ownership', 'Purpose']\n"
     ]
    }
   ],
   "source": [
    "print([column for column in data.columns if data[column].dtype == object])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-22T15:54:24.394547200Z",
     "start_time": "2024-06-22T15:54:24.300233400Z"
    }
   },
   "id": "4072a1d5701d9351"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "dummies = ['Term', 'Home Ownership', 'Purpose']\n",
    "data = pd.get_dummies(data, columns=dummies, drop_first=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-22T15:54:24.772979100Z",
     "start_time": "2024-06-22T15:54:24.678395500Z"
    }
   },
   "id": "d1fa6c4f1093b0f7"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "                                      Current Loan Amount  Credit Score  \\\nLoan ID                                                                   \n273581de-85d8-4332-81a5-19b04ce68666             217646.0         730.0   \n8af915d9-9e91-44a0-b5a2-564a45c12089             548746.0         678.0   \n2e841c8f-3dc1-464d-91c1-3d3d51e64c38             234124.0         727.0   \n24e8c8bd-d10b-4dac-8b81-1da470ff5ecb             666204.0         723.0   \n150ebbad-ebed-441e-b70d-2f350ad7dca6             317108.0         687.0   \n\n                                      Annual Income  Monthly Debt  \\\nLoan ID                                                             \n273581de-85d8-4332-81a5-19b04ce68666      1184194.0      10855.08   \n8af915d9-9e91-44a0-b5a2-564a45c12089      2559110.0      18660.28   \n2e841c8f-3dc1-464d-91c1-3d3d51e64c38       693234.0      14211.24   \n24e8c8bd-d10b-4dac-8b81-1da470ff5ecb      1821967.0      17612.24   \n150ebbad-ebed-441e-b70d-2f350ad7dca6      1133274.0       9632.81   \n\n                                      Years of Credit History  \\\nLoan ID                                                         \n273581de-85d8-4332-81a5-19b04ce68666                     19.6   \n8af915d9-9e91-44a0-b5a2-564a45c12089                     22.6   \n2e841c8f-3dc1-464d-91c1-3d3d51e64c38                     24.7   \n24e8c8bd-d10b-4dac-8b81-1da470ff5ecb                     22.0   \n150ebbad-ebed-441e-b70d-2f350ad7dca6                     17.4   \n\n                                      Months since last delinquent  \\\nLoan ID                                                              \n273581de-85d8-4332-81a5-19b04ce68666                          10.0   \n8af915d9-9e91-44a0-b5a2-564a45c12089                          33.0   \n2e841c8f-3dc1-464d-91c1-3d3d51e64c38                          46.0   \n24e8c8bd-d10b-4dac-8b81-1da470ff5ecb                          34.0   \n150ebbad-ebed-441e-b70d-2f350ad7dca6                          53.0   \n\n                                      Number of Open Accounts  \\\nLoan ID                                                         \n273581de-85d8-4332-81a5-19b04ce68666                     13.0   \n8af915d9-9e91-44a0-b5a2-564a45c12089                      4.0   \n2e841c8f-3dc1-464d-91c1-3d3d51e64c38                     10.0   \n24e8c8bd-d10b-4dac-8b81-1da470ff5ecb                     15.0   \n150ebbad-ebed-441e-b70d-2f350ad7dca6                      4.0   \n\n                                      Number of Credit Problems  \\\nLoan ID                                                           \n273581de-85d8-4332-81a5-19b04ce68666                        1.0   \n8af915d9-9e91-44a0-b5a2-564a45c12089                        0.0   \n2e841c8f-3dc1-464d-91c1-3d3d51e64c38                        1.0   \n24e8c8bd-d10b-4dac-8b81-1da470ff5ecb                        0.0   \n150ebbad-ebed-441e-b70d-2f350ad7dca6                        0.0   \n\n                                      Current Credit Balance  \\\nLoan ID                                                        \n273581de-85d8-4332-81a5-19b04ce68666                122170.0   \n8af915d9-9e91-44a0-b5a2-564a45c12089                437171.0   \n2e841c8f-3dc1-464d-91c1-3d3d51e64c38                 28291.0   \n24e8c8bd-d10b-4dac-8b81-1da470ff5ecb                813694.0   \n150ebbad-ebed-441e-b70d-2f350ad7dca6                 60287.0   \n\n                                      Maximum Open Credit  ...  \\\nLoan ID                                                    ...   \n273581de-85d8-4332-81a5-19b04ce68666             272052.0  ...   \n8af915d9-9e91-44a0-b5a2-564a45c12089             555038.0  ...   \n2e841c8f-3dc1-464d-91c1-3d3d51e64c38             107052.0  ...   \n24e8c8bd-d10b-4dac-8b81-1da470ff5ecb            2004618.0  ...   \n150ebbad-ebed-441e-b70d-2f350ad7dca6             126940.0  ...   \n\n                                      Purpose_Medical Bills  Purpose_Other  \\\nLoan ID                                                                      \n273581de-85d8-4332-81a5-19b04ce68666                      0              0   \n8af915d9-9e91-44a0-b5a2-564a45c12089                      0              0   \n2e841c8f-3dc1-464d-91c1-3d3d51e64c38                      0              0   \n24e8c8bd-d10b-4dac-8b81-1da470ff5ecb                      0              0   \n150ebbad-ebed-441e-b70d-2f350ad7dca6                      0              0   \n\n                                      Purpose_Take a Trip  \\\nLoan ID                                                     \n273581de-85d8-4332-81a5-19b04ce68666                    0   \n8af915d9-9e91-44a0-b5a2-564a45c12089                    0   \n2e841c8f-3dc1-464d-91c1-3d3d51e64c38                    0   \n24e8c8bd-d10b-4dac-8b81-1da470ff5ecb                    0   \n150ebbad-ebed-441e-b70d-2f350ad7dca6                    0   \n\n                                      Purpose_major_purchase  Purpose_moving  \\\nLoan ID                                                                        \n273581de-85d8-4332-81a5-19b04ce68666                       0               0   \n8af915d9-9e91-44a0-b5a2-564a45c12089                       0               0   \n2e841c8f-3dc1-464d-91c1-3d3d51e64c38                       0               0   \n24e8c8bd-d10b-4dac-8b81-1da470ff5ecb                       0               0   \n150ebbad-ebed-441e-b70d-2f350ad7dca6                       0               0   \n\n                                      Purpose_other  Purpose_renewable_energy  \\\nLoan ID                                                                         \n273581de-85d8-4332-81a5-19b04ce68666              0                         0   \n8af915d9-9e91-44a0-b5a2-564a45c12089              0                         0   \n2e841c8f-3dc1-464d-91c1-3d3d51e64c38              0                         0   \n24e8c8bd-d10b-4dac-8b81-1da470ff5ecb              0                         0   \n150ebbad-ebed-441e-b70d-2f350ad7dca6              0                         0   \n\n                                      Purpose_small_business  \\\nLoan ID                                                        \n273581de-85d8-4332-81a5-19b04ce68666                       0   \n8af915d9-9e91-44a0-b5a2-564a45c12089                       0   \n2e841c8f-3dc1-464d-91c1-3d3d51e64c38                       0   \n24e8c8bd-d10b-4dac-8b81-1da470ff5ecb                       0   \n150ebbad-ebed-441e-b70d-2f350ad7dca6                       0   \n\n                                      Purpose_vacation  Purpose_wedding  \nLoan ID                                                                  \n273581de-85d8-4332-81a5-19b04ce68666                 0                0  \n8af915d9-9e91-44a0-b5a2-564a45c12089                 0                0  \n2e841c8f-3dc1-464d-91c1-3d3d51e64c38                 0                0  \n24e8c8bd-d10b-4dac-8b81-1da470ff5ecb                 0                0  \n150ebbad-ebed-441e-b70d-2f350ad7dca6                 0                0  \n\n[5 rows x 32 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Current Loan Amount</th>\n      <th>Credit Score</th>\n      <th>Annual Income</th>\n      <th>Monthly Debt</th>\n      <th>Years of Credit History</th>\n      <th>Months since last delinquent</th>\n      <th>Number of Open Accounts</th>\n      <th>Number of Credit Problems</th>\n      <th>Current Credit Balance</th>\n      <th>Maximum Open Credit</th>\n      <th>...</th>\n      <th>Purpose_Medical Bills</th>\n      <th>Purpose_Other</th>\n      <th>Purpose_Take a Trip</th>\n      <th>Purpose_major_purchase</th>\n      <th>Purpose_moving</th>\n      <th>Purpose_other</th>\n      <th>Purpose_renewable_energy</th>\n      <th>Purpose_small_business</th>\n      <th>Purpose_vacation</th>\n      <th>Purpose_wedding</th>\n    </tr>\n    <tr>\n      <th>Loan ID</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>273581de-85d8-4332-81a5-19b04ce68666</th>\n      <td>217646.0</td>\n      <td>730.0</td>\n      <td>1184194.0</td>\n      <td>10855.08</td>\n      <td>19.6</td>\n      <td>10.0</td>\n      <td>13.0</td>\n      <td>1.0</td>\n      <td>122170.0</td>\n      <td>272052.0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>8af915d9-9e91-44a0-b5a2-564a45c12089</th>\n      <td>548746.0</td>\n      <td>678.0</td>\n      <td>2559110.0</td>\n      <td>18660.28</td>\n      <td>22.6</td>\n      <td>33.0</td>\n      <td>4.0</td>\n      <td>0.0</td>\n      <td>437171.0</td>\n      <td>555038.0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2e841c8f-3dc1-464d-91c1-3d3d51e64c38</th>\n      <td>234124.0</td>\n      <td>727.0</td>\n      <td>693234.0</td>\n      <td>14211.24</td>\n      <td>24.7</td>\n      <td>46.0</td>\n      <td>10.0</td>\n      <td>1.0</td>\n      <td>28291.0</td>\n      <td>107052.0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>24e8c8bd-d10b-4dac-8b81-1da470ff5ecb</th>\n      <td>666204.0</td>\n      <td>723.0</td>\n      <td>1821967.0</td>\n      <td>17612.24</td>\n      <td>22.0</td>\n      <td>34.0</td>\n      <td>15.0</td>\n      <td>0.0</td>\n      <td>813694.0</td>\n      <td>2004618.0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>150ebbad-ebed-441e-b70d-2f350ad7dca6</th>\n      <td>317108.0</td>\n      <td>687.0</td>\n      <td>1133274.0</td>\n      <td>9632.81</td>\n      <td>17.4</td>\n      <td>53.0</td>\n      <td>4.0</td>\n      <td>0.0</td>\n      <td>60287.0</td>\n      <td>126940.0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 32 columns</p>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-22T15:54:25.229704800Z",
     "start_time": "2024-06-22T15:54:25.025544100Z"
    }
   },
   "id": "c0413e3432b521db"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preparing data for the models"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5022a27542051ec8"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "X = data.drop(columns=['loan_status_num'])\n",
    "y = data['loan_status_num']\n",
    "X_alt = data.drop(columns=['loan_status_num'])\n",
    "X_alt_train, X_alt_temp, y_alt_train, y_alt_temp = train_test_split(X_alt, y, test_size=0.3, random_state=42)\n",
    "X_train_extra, X_test_extra, y_train_extra, y_test_extra = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "scaler = StandardScaler()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-22T15:54:26.229528600Z",
     "start_time": "2024-06-22T15:54:26.103582800Z"
    }
   },
   "id": "211f9e7b83586ec4"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "X_validation, X_temp2, y_validation, y_temp2 = train_test_split(X_temp, y_temp, test_size=0.67, random_state=42)\n",
    "\n",
    "X_calibration, X_test, y_calibration, y_test = train_test_split(X_temp2, y_temp2, test_size=0.5, random_state=42)\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_validation_scaled = scaler.transform(X_validation)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_calibration_scaled = scaler.transform(X_calibration)\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "X_validation_tensor = torch.tensor(X_validation_scaled, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "X_calibration_tensor = torch.tensor(X_calibration_scaled, dtype=torch.float32)\n",
    "\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).reshape(len(y_train),1)\n",
    "y_validation_tensor = torch.tensor(y_validation.values, dtype=torch.float32).reshape(len(y_validation),1)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).reshape(len(y_test),1)\n",
    "y_calibration_tensor = torch.tensor(y_calibration.values, dtype=torch.float32).reshape(len(y_calibration),1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-22T15:54:27.672249200Z",
     "start_time": "2024-06-22T15:54:27.577615500Z"
    }
   },
   "id": "fbf20d91657cb48c"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|--- Credit Score <= 684.50\n",
      "|   |--- Annual Income <= 2489636.50\n",
      "|   |   |--- class: 1\n",
      "|   |--- Annual Income >  2489636.50\n",
      "|   |   |--- class: 0\n",
      "|--- Credit Score >  684.50\n",
      "|   |--- Annual Income <= 1271242.50\n",
      "|   |   |--- class: 1\n",
      "|   |--- Annual Income >  1271242.50\n",
      "|   |   |--- class: 0\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.44      0.57      7509\n",
      "           1       0.27      0.67      0.39      2363\n",
      "\n",
      "    accuracy                           0.49      9872\n",
      "   macro avg       0.54      0.56      0.48      9872\n",
      "weighted avg       0.68      0.49      0.53      9872\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, export_text\n",
    "dt_model = DecisionTreeClassifier(max_depth=2, class_weight='balanced')\n",
    "dt_model.fit(X_train_extra, y_train_extra)\n",
    "tree_rules = export_text(dt_model, feature_names=list(X_train_extra.columns))\n",
    "print(tree_rules)\n",
    "predictions_extra = dt_model.predict(X_test_extra)\n",
    "\n",
    "class_report = classification_report(y_test_extra, predictions_extra)\n",
    "print(\"Classification Report:\\n\", class_report)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-22T15:54:28.561562Z",
     "start_time": "2024-06-22T15:54:28.180379800Z"
    }
   },
   "id": "d94e656bcf411497"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "classlabels = torch.tensor(data['loan_status_num'].values, dtype=torch.long)\n",
    "class_counts = torch.bincount(classlabels)\n",
    "\n",
    "class_weights_original = 1.0 / class_counts.float()\n",
    "\n",
    "class_weights_original /= class_weights_original.sum()\n",
    "class_weights = class_weights_original[y_train_tensor.long()]\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-22T15:54:28.823826900Z",
     "start_time": "2024-06-22T15:54:28.812204400Z"
    }
   },
   "id": "3a6d19cb201f032a"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def evaluate_nn(true, pred, train=True):\n",
    "    if train:\n",
    "        clf_report = pd.DataFrame(classification_report(true, pred, output_dict=True))\n",
    "        print(\"Train Result:\\n================================================\")\n",
    "        print(f\"Accuracy Score: {accuracy_score(true, pred) * 100:.2f}%\")\n",
    "        print(\"_______________________________________________\")\n",
    "        print(f\"CLASSIFICATION REPORT:\\n{clf_report}\")\n",
    "        print(\"_______________________________________________\")\n",
    "        print(f\"Confusion Matrix: \\n {confusion_matrix(true, pred)}\\n\")\n",
    "\n",
    "    elif train==False:\n",
    "        clf_report = pd.DataFrame(classification_report(true, pred, output_dict=True))\n",
    "        print(\"Test Result:\\n================================================\")\n",
    "        print(f\"Accuracy Score: {accuracy_score(true, pred) * 100:.2f}%\")\n",
    "        print(\"_______________________________________________\")\n",
    "        print(f\"CLASSIFICATION REPORT:\\n{clf_report}\")\n",
    "        print(\"_______________________________________________\")\n",
    "        print(f\"Confusion Matrix: \\n {confusion_matrix(true, pred)}\\n\")\n",
    "\n",
    "def plot_learning_evolution(r):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(r.history['loss'], label='Loss')\n",
    "    plt.plot(r.history['val_loss'], label='val_Loss')\n",
    "    plt.title('Loss evolution during trainig')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(r.history['AUC'], label='AUC')\n",
    "    plt.plot(r.history['val_AUC'], label='val_AUC')\n",
    "    plt.title('AUC score evolution during trainig')\n",
    "    plt.legend();\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-22T15:54:29.564053900Z",
     "start_time": "2024-06-22T15:54:29.485232600Z"
    }
   },
   "id": "8991d4a1e48357db"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Logistic Regression"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "19252f911d968649"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "from models.model import Logistic_Regression, MLP\n",
    "from torchinfo import summary"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-22T15:54:30.871950400Z",
     "start_time": "2024-06-22T15:54:30.809211200Z"
    }
   },
   "id": "7f5c972556e5b5f4"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "31"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_features=len(data.drop(columns=['loan_status_num']).columns)\n",
    "n_features"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-22T15:54:31.307181300Z",
     "start_time": "2024-06-22T15:54:31.275090400Z"
    }
   },
   "id": "38f23ce1b2a2f82c"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nLogistic_Regression                      [23034, 1]                --\n├─Sequential: 1-1                        [23034, 1]                --\n│    └─Linear: 2-1                       [23034, 1]                32\n│    └─Sigmoid: 2-2                      [23034, 1]                --\n==========================================================================================\nTotal params: 32\nTrainable params: 32\nNon-trainable params: 0\nTotal mult-adds (Units.MEGABYTES): 0.74\n==========================================================================================\nInput size (MB): 2.86\nForward/backward pass size (MB): 0.18\nParams size (MB): 0.00\nEstimated Total Size (MB): 3.04\n=========================================================================================="
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_model = Logistic_Regression(num_features=n_features)\n",
    "summary(lr_model, input_size=X_train.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-22T15:54:32.212551100Z",
     "start_time": "2024-06-22T15:54:32.133644600Z"
    }
   },
   "id": "933424173a1a4fc0"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.01\n",
    "EPOCHS = 1000"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-22T15:54:32.774677100Z",
     "start_time": "2024-06-22T15:54:32.711696Z"
    }
   },
   "id": "7772e5ddd18d638c"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "loss_function = nn.BCELoss(weight=class_weights)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-22T15:54:33.344856500Z",
     "start_time": "2024-06-22T15:54:33.296300700Z"
    }
   },
   "id": "dc69282fac5d2988"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(lr_model.parameters(), lr=LEARNING_RATE)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-22T15:54:36.088344600Z",
     "start_time": "2024-06-22T15:54:33.791937600Z"
    }
   },
   "id": "8d1b76cbcaf31fe1"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "def calculate_accuracy(preds, actuals):\n",
    "\n",
    "    with torch.no_grad():\n",
    "        rounded_preds = torch.round(preds)\n",
    "        num_correct = torch.sum(rounded_preds == actuals)\n",
    "        accuracy = num_correct/len(preds)\n",
    "\n",
    "    return accuracy"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-22T15:54:36.649690700Z",
     "start_time": "2024-06-22T15:54:36.611357Z"
    }
   },
   "id": "6e9f25c0a8911a7"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \t| Train loss: 0.263 \t| Train acc: 0.53 \t| Test acc: 0.53\n",
      "Epoch: 100 \t| Train loss: 0.246 \t| Train acc: 0.58 \t| Test acc: 0.59\n",
      "Epoch: 200 \t| Train loss: 0.246 \t| Train acc: 0.58 \t| Test acc: 0.59\n",
      "Epoch: 300 \t| Train loss: 0.246 \t| Train acc: 0.58 \t| Test acc: 0.59\n",
      "Epoch: 400 \t| Train loss: 0.246 \t| Train acc: 0.58 \t| Test acc: 0.59\n",
      "Epoch: 500 \t| Train loss: 0.246 \t| Train acc: 0.58 \t| Test acc: 0.59\n",
      "Epoch: 600 \t| Train loss: 0.246 \t| Train acc: 0.58 \t| Test acc: 0.59\n",
      "Epoch: 700 \t| Train loss: 0.246 \t| Train acc: 0.58 \t| Test acc: 0.59\n",
      "Epoch: 800 \t| Train loss: 0.246 \t| Train acc: 0.58 \t| Test acc: 0.59\n",
      "Epoch: 900 \t| Train loss: 0.246 \t| Train acc: 0.58 \t| Test acc: 0.59\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "#test_losses  = []\n",
    "train_accs = []\n",
    "test_accs  = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_preds = lr_model(X_train_tensor)\n",
    "    train_loss  = loss_function(train_preds, y_train_tensor)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        test_preds = lr_model(X_validation_tensor)\n",
    "        #test_loss  = loss_function(test_preds, y_test)\n",
    "\n",
    "    train_acc = calculate_accuracy(train_preds, y_train_tensor)\n",
    "    test_acc  = calculate_accuracy(test_preds, y_validation_tensor)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    train_loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    train_losses.append(train_loss.item())\n",
    "    #test_losses.append(test_loss.item())\n",
    "    train_accs.append(train_acc.item())\n",
    "    test_accs.append(test_acc.item())\n",
    "\n",
    "    if epoch%100==0:\n",
    "        print(f'Epoch: {epoch} \\t|' \\\n",
    "              f' Train loss: {np.round(train_loss.item(),3)} \\t|' \\\n",
    "                  #f' Test loss: {np.round(test_loss.item(),3)} \\t|' \\\n",
    "              f' Train acc: {np.round(train_acc.item(),2)} \\t|' \\\n",
    "              f' Test acc: {np.round(test_acc.item(),2)}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-22T15:54:41.047496200Z",
     "start_time": "2024-06-22T15:54:36.995166900Z"
    }
   },
   "id": "c840057d48645d98"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Result:\n",
      "================================================\n",
      "Accuracy Score: 58.06%\n",
      "_______________________________________________\n",
      "CLASSIFICATION REPORT:\n",
      "                    0.0          1.0  accuracy     macro avg  weighted avg\n",
      "precision      0.815339     0.304955  0.580577      0.560147      0.692850\n",
      "recall         0.579344     0.584479  0.580577      0.581912      0.580577\n",
      "f1-score       0.677375     0.400794  0.580577      0.539085      0.610998\n",
      "support    17506.000000  5528.000000  0.580577  23034.000000  23034.000000\n",
      "_______________________________________________\n",
      "Confusion Matrix: \n",
      " [[10142  7364]\n",
      " [ 2297  3231]]\n",
      "\n",
      "Test Result:\n",
      "================================================\n",
      "Accuracy Score: 59.38%\n",
      "_______________________________________________\n",
      "CLASSIFICATION REPORT:\n",
      "                   0.0         1.0  accuracy    macro avg  weighted avg\n",
      "precision     0.828379    0.310041  0.593798     0.569210      0.706951\n",
      "recall        0.592221    0.598952  0.593798     0.595586      0.593798\n",
      "f1-score      0.690671    0.408583  0.593798     0.549627      0.624588\n",
      "support    2494.000000  763.000000  0.593798  3257.000000   3257.000000\n",
      "_______________________________________________\n",
      "Confusion Matrix: \n",
      " [[1477 1017]\n",
      " [ 306  457]]\n"
     ]
    }
   ],
   "source": [
    "evaluate_nn(y_train_tensor.clone().detach(), train_preds.clone().detach().round(), train=True)\n",
    "evaluate_nn(y_validation_tensor.clone().detach(), test_preds.clone().detach().round(), train=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-22T15:54:42.807915100Z",
     "start_time": "2024-06-22T15:54:42.666431100Z"
    }
   },
   "id": "776cbb301a2ad576"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Result:\n",
      "================================================\n",
      "Accuracy Score: 57.53%\n",
      "_______________________________________________\n",
      "CLASSIFICATION REPORT:\n",
      "                   0.0         1.0  accuracy    macro avg  weighted avg\n",
      "precision     0.801913    0.312214  0.575272     0.557064      0.679044\n",
      "recall        0.575061    0.575904  0.575272     0.575482      0.575272\n",
      "f1-score      0.669800    0.404913  0.575272     0.537357      0.603338\n",
      "support    2478.000000  830.000000  0.575272  3308.000000   3308.000000\n",
      "_______________________________________________\n",
      "Confusion Matrix: \n",
      " [[1425 1053]\n",
      " [ 352  478]]\n"
     ]
    }
   ],
   "source": [
    "test_preds_lr = lr_model(X_test_tensor)\n",
    "evaluate_nn(y_test_tensor.clone().detach(), test_preds_lr.clone().detach().round(), train=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-22T15:54:43.683778500Z",
     "start_time": "2024-06-22T15:54:43.558211500Z"
    }
   },
   "id": "90adf75673abf94b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Semantic Loss in Logistic Regression"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4a094b7846ac6dbb"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import models.loss\n",
    "reload(models.loss)\n",
    "from models.loss import semantic_loss"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-22T15:54:47.178907900Z",
     "start_time": "2024-06-22T15:54:47.131551Z"
    }
   },
   "id": "eab2ed83c8cf5c18"
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "outputs": [
    {
     "data": {
      "text/plain": "==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nLogistic_Regression                      [23034, 1]                --\n├─Sequential: 1-1                        [23034, 1]                --\n│    └─Linear: 2-1                       [23034, 1]                32\n│    └─Sigmoid: 2-2                      [23034, 1]                --\n==========================================================================================\nTotal params: 32\nTrainable params: 32\nNon-trainable params: 0\nTotal mult-adds (Units.MEGABYTES): 0.74\n==========================================================================================\nInput size (MB): 2.86\nForward/backward pass size (MB): 0.18\nParams size (MB): 0.00\nEstimated Total Size (MB): 3.04\n=========================================================================================="
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sl_model = Logistic_Regression(num_features=n_features)\n",
    "summary(sl_model, input_size=X_train.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-22T16:43:17.022907400Z",
     "start_time": "2024-06-22T16:43:16.642297300Z"
    }
   },
   "id": "b07a59f30bddcb7f"
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "outputs": [],
   "source": [
    "optimizer_sl = optim.Adam(sl_model.parameters(), lr=LEARNING_RATE)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-22T16:43:17.453295800Z",
     "start_time": "2024-06-22T16:43:16.963273500Z"
    }
   },
   "id": "d32814576148d7f9"
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "outputs": [],
   "source": [
    "credit_score = torch.tensor(X_alt_train['Credit Score'].values, dtype=torch.float32).reshape(len(y_train),1)\n",
    "short_term = torch.tensor(X_alt_train['Term_Short Term'].values, dtype=torch.float32).reshape(len(y_train),1)\n",
    "annual_income = torch.tensor(X_alt_train['Annual Income'].values, dtype=torch.float32).reshape(len(y_train),1)\n",
    "\n",
    "\n",
    "rule = torch.logical_or(torch.logical_and(credit_score<=684.5,  annual_income<=2489636.5), torch.logical_and(credit_score>684.5, annual_income<=1271242.5))\n",
    "rule=rule.float()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-22T16:43:17.686876300Z",
     "start_time": "2024-06-22T16:43:17.329453100Z"
    }
   },
   "id": "c4008f210984379b"
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        ...,\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.]])\n"
     ]
    }
   ],
   "source": [
    "print(rule)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-22T16:43:18.553133600Z",
     "start_time": "2024-06-22T16:43:18.539556100Z"
    }
   },
   "id": "c7e99e243e47b0ac"
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \t| Train loss: 0.185 \t| Train acc: 0.55 \t| Test acc: 0.56\n",
      "Epoch: 100 \t| Train loss: 0.157 \t| Train acc: 0.55 \t| Test acc: 0.56\n",
      "Epoch: 200 \t| Train loss: 0.157 \t| Train acc: 0.55 \t| Test acc: 0.55\n",
      "Epoch: 300 \t| Train loss: 0.157 \t| Train acc: 0.55 \t| Test acc: 0.55\n",
      "Epoch: 400 \t| Train loss: 0.157 \t| Train acc: 0.55 \t| Test acc: 0.55\n",
      "Epoch: 500 \t| Train loss: 0.157 \t| Train acc: 0.55 \t| Test acc: 0.55\n",
      "Epoch: 600 \t| Train loss: 0.157 \t| Train acc: 0.55 \t| Test acc: 0.55\n",
      "Epoch: 700 \t| Train loss: 0.157 \t| Train acc: 0.55 \t| Test acc: 0.55\n",
      "Epoch: 800 \t| Train loss: 0.157 \t| Train acc: 0.55 \t| Test acc: 0.55\n",
      "Epoch: 900 \t| Train loss: 0.157 \t| Train acc: 0.55 \t| Test acc: 0.55\n"
     ]
    }
   ],
   "source": [
    "train_losses_sl = []\n",
    "#test_losses  = []\n",
    "train_accs_sl = []\n",
    "test_accs_sl  = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_preds = sl_model(X_train_tensor)\n",
    "    #train_loss  = loss_function(train_preds, y_train_tensor)\n",
    "    train_loss = semantic_loss(train_preds, y_train_tensor, rule, class_weights, 0.01)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        test_preds = sl_model(X_validation_tensor)\n",
    "        #test_loss  = loss_function(test_preds, y_test)\n",
    "\n",
    "    train_acc = calculate_accuracy(train_preds, y_train_tensor)\n",
    "    test_acc  = calculate_accuracy(test_preds, y_validation_tensor)\n",
    "\n",
    "    optimizer_sl.zero_grad()\n",
    "    train_loss.backward()\n",
    "\n",
    "    optimizer_sl.step()\n",
    "\n",
    "    train_losses_sl.append(train_loss.item())\n",
    "    #test_losses.append(test_loss.item())\n",
    "    train_accs_sl.append(train_acc.item())\n",
    "    test_accs_sl.append(test_acc.item())\n",
    "\n",
    "    if epoch%100==0:\n",
    "        print(f'Epoch: {epoch} \\t|' \\\n",
    "              f' Train loss: {np.round(train_loss.item(),3)} \\t|' \\\n",
    "                  #f' Test loss: {np.round(test_loss.item(),3)} \\t|' \\\n",
    "              f' Train acc: {np.round(train_acc.item(),2)} \\t|' \\\n",
    "              f' Test acc: {np.round(test_acc.item(),2)}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-22T16:43:32.358284200Z",
     "start_time": "2024-06-22T16:43:20.677511600Z"
    }
   },
   "id": "1b84e380b59acca7"
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Result:\n",
      "================================================\n",
      "Accuracy Score: 54.59%\n",
      "_______________________________________________\n",
      "CLASSIFICATION REPORT:\n",
      "                    0.0          1.0  accuracy     macro avg  weighted avg\n",
      "precision      0.820389     0.295149  0.545889      0.557769      0.694335\n",
      "recall         0.515309     0.642728  0.545889      0.579018      0.545889\n",
      "f1-score       0.633008     0.404531  0.545889      0.518770      0.578175\n",
      "support    17506.000000  5528.000000  0.545889  23034.000000  23034.000000\n",
      "_______________________________________________\n",
      "Confusion Matrix: \n",
      " [[9021 8485]\n",
      " [1975 3553]]\n",
      "\n",
      "Test Result:\n",
      "================================================\n",
      "Accuracy Score: 55.45%\n",
      "_______________________________________________\n",
      "CLASSIFICATION REPORT:\n",
      "                   0.0         1.0  accuracy    macro avg  weighted avg\n",
      "precision     0.833227    0.296690  0.554498     0.564959      0.707535\n",
      "recall        0.522855    0.657929  0.554498     0.590392      0.554498\n",
      "f1-score      0.642523    0.408961  0.554498     0.525742      0.587808\n",
      "support    2494.000000  763.000000  0.554498  3257.000000   3257.000000\n",
      "_______________________________________________\n",
      "Confusion Matrix: \n",
      " [[1304 1190]\n",
      " [ 261  502]]\n"
     ]
    }
   ],
   "source": [
    "evaluate_nn(y_train_tensor.clone().detach(), train_preds.clone().detach().round(), train=True)\n",
    "evaluate_nn(y_validation_tensor.clone().detach(), test_preds.clone().detach().round(), train=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-22T16:43:35.443613700Z",
     "start_time": "2024-06-22T16:43:35.084770900Z"
    }
   },
   "id": "3f92651b92d8c86c"
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Result:\n",
      "================================================\n",
      "Accuracy Score: 54.75%\n",
      "_______________________________________________\n",
      "CLASSIFICATION REPORT:\n",
      "                   0.0         1.0  accuracy    macro avg  weighted avg\n",
      "precision     0.812222    0.308002  0.547461     0.560112      0.685709\n",
      "recall        0.514931    0.644578  0.547461     0.579755      0.547461\n",
      "f1-score      0.630279    0.416829  0.547461     0.523554      0.576723\n",
      "support    2478.000000  830.000000  0.547461  3308.000000   3308.000000\n",
      "_______________________________________________\n",
      "Confusion Matrix: \n",
      " [[1276 1202]\n",
      " [ 295  535]]\n"
     ]
    }
   ],
   "source": [
    "test_preds_sl = sl_model(X_test_tensor)\n",
    "evaluate_nn(y_test_tensor.clone().detach(), test_preds_sl.clone().detach().round(), train=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-22T16:43:36.477455100Z",
     "start_time": "2024-06-22T16:43:36.321546900Z"
    }
   },
   "id": "6ec2bb6eba9b77db"
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "data": {
      "text/plain": "1918"
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "credit_score_test = torch.tensor(X_test['Credit Score'].values, dtype=torch.float32).reshape(len(X_test), 1)\n",
    "short_term_test = torch.tensor(X_test['Term_Short Term'].values, dtype=torch.float32).reshape(len(X_test), 1)\n",
    "annual_income_test = torch.tensor(X_test['Annual Income'].values, dtype=torch.float32).reshape(len(X_test), 1)\n",
    "\n",
    "rule_test = torch.logical_or(\n",
    "    torch.logical_and(credit_score_test <= 684.5, annual_income_test <= 2489636.5),\n",
    "    torch.logical_and(credit_score_test > 684.5, annual_income_test <= 1271242.5)\n",
    ")\n",
    "rule_matched = torch.nonzero(rule_test.float(), as_tuple=True)[0].tolist()\n",
    "yespred = torch.nonzero(test_preds_sl.round(), as_tuple=True)[0].tolist()\n",
    "same = [value for value in yespred if value in rule_matched]\n",
    "len(same)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-22T16:08:50.154820900Z",
     "start_time": "2024-06-22T16:08:49.949210Z"
    }
   },
   "id": "b0290874e1f6e4e"
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([3308, 1])"
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds_sl.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-22T16:09:02.335498400Z",
     "start_time": "2024-06-22T16:09:02.221083200Z"
    }
   },
   "id": "76c1a6106f849dfa"
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "rounded_preds_lr = torch.round(test_preds_lr)\n",
    "rounded_preds_sl = torch.round(test_preds_sl)\n",
    "\n",
    "different_predictions = (rounded_preds_lr != rounded_preds_sl)\n",
    "\n",
    "correct_predictions_lr = (rounded_preds_lr == y_test_tensor)\n",
    "correct_predictions_sl = (rounded_preds_sl == y_test_tensor)\n",
    "\n",
    "different_and_correct_lr = different_predictions & correct_predictions_lr\n",
    "different_and_correct_sl = different_predictions & correct_predictions_sl\n",
    "\n",
    "#indices where predictions are different and correct\n",
    "indices_different_and_correct_lr = torch.nonzero(different_and_correct_lr).flatten()\n",
    "indices_different_and_correct_sl = torch.nonzero(different_and_correct_sl).flatten()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-22T16:09:02.992168200Z",
     "start_time": "2024-06-22T16:09:02.833811400Z"
    }
   },
   "id": "d6832e307d7e3c3b"
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "only_sl_correct = torch.nonzero(different_and_correct_sl.float(), as_tuple=True)[0].tolist()\n",
    "only_lr_correct = torch.nonzero(different_and_correct_lr.float(), as_tuple=True)[0].tolist()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-22T16:09:29.868089Z",
     "start_time": "2024-06-22T16:09:29.740837100Z"
    }
   },
   "id": "aec1eec68eb181a1"
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "data": {
      "text/plain": "                                      Current Loan Amount  Credit Score  \\\nLoan ID                                                                   \n8deb9448-0939-4b4a-add5-89f4442fddf7             155628.0         645.0   \nb4f3602e-8187-4076-966a-cfd71084e8f4             161810.0         724.0   \n1103946b-1671-4c39-9bf5-923c10ce0cb1             130064.0         726.0   \n8040dec3-ef55-4baa-bc20-1c2853ccace7             117282.0         734.0   \nf4ce26ef-0e52-475c-9d62-75a78c68a322             215578.0         723.0   \n...                                                   ...           ...   \n9baefe4e-d64e-4db9-a303-5ad61a7b7381             220528.0         664.0   \n1471a3f6-63ed-43b4-9b21-58d3ce05b0a1             251944.0         744.0   \n33299529-29f2-4eb1-a730-16ac4ef0d57e             133672.0         707.0   \n12f11253-2e09-4ced-8b5e-83439cba7eb0             257532.0         740.0   \nfb49cc3e-b551-413d-80b6-47db7f903ede             263692.0         731.0   \n\n                                      Annual Income  Monthly Debt  \\\nLoan ID                                                             \n8deb9448-0939-4b4a-add5-89f4442fddf7      1263956.0      10216.87   \nb4f3602e-8187-4076-966a-cfd71084e8f4      1397507.0      21311.92   \n1103946b-1671-4c39-9bf5-923c10ce0cb1      1310582.0      18784.92   \n8040dec3-ef55-4baa-bc20-1c2853ccace7       489953.0       7961.76   \nf4ce26ef-0e52-475c-9d62-75a78c68a322      1008444.0      18236.01   \n...                                             ...           ...   \n9baefe4e-d64e-4db9-a303-5ad61a7b7381       914185.0      13103.35   \n1471a3f6-63ed-43b4-9b21-58d3ce05b0a1       946010.0      12613.53   \n33299529-29f2-4eb1-a730-16ac4ef0d57e       423263.0       9488.22   \n12f11253-2e09-4ced-8b5e-83439cba7eb0      1056476.0      12237.33   \nfb49cc3e-b551-413d-80b6-47db7f903ede      1138632.0      19280.82   \n\n                                      Years of Credit History  \\\nLoan ID                                                         \n8deb9448-0939-4b4a-add5-89f4442fddf7                     13.5   \nb4f3602e-8187-4076-966a-cfd71084e8f4                     18.0   \n1103946b-1671-4c39-9bf5-923c10ce0cb1                     14.0   \n8040dec3-ef55-4baa-bc20-1c2853ccace7                     13.8   \nf4ce26ef-0e52-475c-9d62-75a78c68a322                     22.5   \n...                                                       ...   \n9baefe4e-d64e-4db9-a303-5ad61a7b7381                     32.5   \n1471a3f6-63ed-43b4-9b21-58d3ce05b0a1                     11.9   \n33299529-29f2-4eb1-a730-16ac4ef0d57e                     45.5   \n12f11253-2e09-4ced-8b5e-83439cba7eb0                     15.3   \nfb49cc3e-b551-413d-80b6-47db7f903ede                     18.4   \n\n                                      Months since last delinquent  \\\nLoan ID                                                              \n8deb9448-0939-4b4a-add5-89f4442fddf7                          43.0   \nb4f3602e-8187-4076-966a-cfd71084e8f4                           5.0   \n1103946b-1671-4c39-9bf5-923c10ce0cb1                          25.0   \n8040dec3-ef55-4baa-bc20-1c2853ccace7                           9.0   \nf4ce26ef-0e52-475c-9d62-75a78c68a322                          42.0   \n...                                                            ...   \n9baefe4e-d64e-4db9-a303-5ad61a7b7381                           6.0   \n1471a3f6-63ed-43b4-9b21-58d3ce05b0a1                          13.0   \n33299529-29f2-4eb1-a730-16ac4ef0d57e                          26.0   \n12f11253-2e09-4ced-8b5e-83439cba7eb0                          12.0   \nfb49cc3e-b551-413d-80b6-47db7f903ede                          31.0   \n\n                                      Number of Open Accounts  \\\nLoan ID                                                         \n8deb9448-0939-4b4a-add5-89f4442fddf7                     12.0   \nb4f3602e-8187-4076-966a-cfd71084e8f4                     12.0   \n1103946b-1671-4c39-9bf5-923c10ce0cb1                     15.0   \n8040dec3-ef55-4baa-bc20-1c2853ccace7                     13.0   \nf4ce26ef-0e52-475c-9d62-75a78c68a322                     13.0   \n...                                                       ...   \n9baefe4e-d64e-4db9-a303-5ad61a7b7381                      8.0   \n1471a3f6-63ed-43b4-9b21-58d3ce05b0a1                     22.0   \n33299529-29f2-4eb1-a730-16ac4ef0d57e                      9.0   \n12f11253-2e09-4ced-8b5e-83439cba7eb0                      5.0   \nfb49cc3e-b551-413d-80b6-47db7f903ede                      6.0   \n\n                                      Number of Credit Problems  \\\nLoan ID                                                           \n8deb9448-0939-4b4a-add5-89f4442fddf7                        0.0   \nb4f3602e-8187-4076-966a-cfd71084e8f4                        0.0   \n1103946b-1671-4c39-9bf5-923c10ce0cb1                        0.0   \n8040dec3-ef55-4baa-bc20-1c2853ccace7                        0.0   \nf4ce26ef-0e52-475c-9d62-75a78c68a322                        0.0   \n...                                                         ...   \n9baefe4e-d64e-4db9-a303-5ad61a7b7381                        0.0   \n1471a3f6-63ed-43b4-9b21-58d3ce05b0a1                        0.0   \n33299529-29f2-4eb1-a730-16ac4ef0d57e                        0.0   \n12f11253-2e09-4ced-8b5e-83439cba7eb0                        0.0   \nfb49cc3e-b551-413d-80b6-47db7f903ede                        0.0   \n\n                                      Current Credit Balance  \\\nLoan ID                                                        \n8deb9448-0939-4b4a-add5-89f4442fddf7                174781.0   \nb4f3602e-8187-4076-966a-cfd71084e8f4                108395.0   \n1103946b-1671-4c39-9bf5-923c10ce0cb1                536769.0   \n8040dec3-ef55-4baa-bc20-1c2853ccace7                195225.0   \nf4ce26ef-0e52-475c-9d62-75a78c68a322                569943.0   \n...                                                      ...   \n9baefe4e-d64e-4db9-a303-5ad61a7b7381                 78261.0   \n1471a3f6-63ed-43b4-9b21-58d3ce05b0a1                170050.0   \n33299529-29f2-4eb1-a730-16ac4ef0d57e                223820.0   \n12f11253-2e09-4ced-8b5e-83439cba7eb0                153387.0   \nfb49cc3e-b551-413d-80b6-47db7f903ede                120669.0   \n\n                                      Maximum Open Credit  ...  \\\nLoan ID                                                    ...   \n8deb9448-0939-4b4a-add5-89f4442fddf7             417274.0  ...   \nb4f3602e-8187-4076-966a-cfd71084e8f4             189860.0  ...   \n1103946b-1671-4c39-9bf5-923c10ce0cb1            1195260.0  ...   \n8040dec3-ef55-4baa-bc20-1c2853ccace7             314402.0  ...   \nf4ce26ef-0e52-475c-9d62-75a78c68a322             920414.0  ...   \n...                                                   ...  ...   \n9baefe4e-d64e-4db9-a303-5ad61a7b7381             187594.0  ...   \n1471a3f6-63ed-43b4-9b21-58d3ce05b0a1             674344.0  ...   \n33299529-29f2-4eb1-a730-16ac4ef0d57e             362956.0  ...   \n12f11253-2e09-4ced-8b5e-83439cba7eb0             315480.0  ...   \nfb49cc3e-b551-413d-80b6-47db7f903ede             289894.0  ...   \n\n                                      Purpose_Medical Bills  Purpose_Other  \\\nLoan ID                                                                      \n8deb9448-0939-4b4a-add5-89f4442fddf7                      0              0   \nb4f3602e-8187-4076-966a-cfd71084e8f4                      0              0   \n1103946b-1671-4c39-9bf5-923c10ce0cb1                      0              0   \n8040dec3-ef55-4baa-bc20-1c2853ccace7                      0              0   \nf4ce26ef-0e52-475c-9d62-75a78c68a322                      0              0   \n...                                                     ...            ...   \n9baefe4e-d64e-4db9-a303-5ad61a7b7381                      0              1   \n1471a3f6-63ed-43b4-9b21-58d3ce05b0a1                      0              0   \n33299529-29f2-4eb1-a730-16ac4ef0d57e                      0              0   \n12f11253-2e09-4ced-8b5e-83439cba7eb0                      0              0   \nfb49cc3e-b551-413d-80b6-47db7f903ede                      0              0   \n\n                                      Purpose_Take a Trip  \\\nLoan ID                                                     \n8deb9448-0939-4b4a-add5-89f4442fddf7                    0   \nb4f3602e-8187-4076-966a-cfd71084e8f4                    0   \n1103946b-1671-4c39-9bf5-923c10ce0cb1                    0   \n8040dec3-ef55-4baa-bc20-1c2853ccace7                    0   \nf4ce26ef-0e52-475c-9d62-75a78c68a322                    0   \n...                                                   ...   \n9baefe4e-d64e-4db9-a303-5ad61a7b7381                    0   \n1471a3f6-63ed-43b4-9b21-58d3ce05b0a1                    0   \n33299529-29f2-4eb1-a730-16ac4ef0d57e                    0   \n12f11253-2e09-4ced-8b5e-83439cba7eb0                    0   \nfb49cc3e-b551-413d-80b6-47db7f903ede                    0   \n\n                                      Purpose_major_purchase  Purpose_moving  \\\nLoan ID                                                                        \n8deb9448-0939-4b4a-add5-89f4442fddf7                       0               0   \nb4f3602e-8187-4076-966a-cfd71084e8f4                       0               0   \n1103946b-1671-4c39-9bf5-923c10ce0cb1                       0               0   \n8040dec3-ef55-4baa-bc20-1c2853ccace7                       0               0   \nf4ce26ef-0e52-475c-9d62-75a78c68a322                       0               0   \n...                                                      ...             ...   \n9baefe4e-d64e-4db9-a303-5ad61a7b7381                       0               0   \n1471a3f6-63ed-43b4-9b21-58d3ce05b0a1                       0               0   \n33299529-29f2-4eb1-a730-16ac4ef0d57e                       0               0   \n12f11253-2e09-4ced-8b5e-83439cba7eb0                       0               0   \nfb49cc3e-b551-413d-80b6-47db7f903ede                       0               0   \n\n                                      Purpose_other  Purpose_renewable_energy  \\\nLoan ID                                                                         \n8deb9448-0939-4b4a-add5-89f4442fddf7              0                         0   \nb4f3602e-8187-4076-966a-cfd71084e8f4              0                         0   \n1103946b-1671-4c39-9bf5-923c10ce0cb1              0                         0   \n8040dec3-ef55-4baa-bc20-1c2853ccace7              0                         0   \nf4ce26ef-0e52-475c-9d62-75a78c68a322              0                         0   \n...                                             ...                       ...   \n9baefe4e-d64e-4db9-a303-5ad61a7b7381              0                         0   \n1471a3f6-63ed-43b4-9b21-58d3ce05b0a1              0                         0   \n33299529-29f2-4eb1-a730-16ac4ef0d57e              1                         0   \n12f11253-2e09-4ced-8b5e-83439cba7eb0              0                         0   \nfb49cc3e-b551-413d-80b6-47db7f903ede              0                         0   \n\n                                      Purpose_small_business  \\\nLoan ID                                                        \n8deb9448-0939-4b4a-add5-89f4442fddf7                       0   \nb4f3602e-8187-4076-966a-cfd71084e8f4                       0   \n1103946b-1671-4c39-9bf5-923c10ce0cb1                       0   \n8040dec3-ef55-4baa-bc20-1c2853ccace7                       0   \nf4ce26ef-0e52-475c-9d62-75a78c68a322                       0   \n...                                                      ...   \n9baefe4e-d64e-4db9-a303-5ad61a7b7381                       0   \n1471a3f6-63ed-43b4-9b21-58d3ce05b0a1                       0   \n33299529-29f2-4eb1-a730-16ac4ef0d57e                       0   \n12f11253-2e09-4ced-8b5e-83439cba7eb0                       0   \nfb49cc3e-b551-413d-80b6-47db7f903ede                       0   \n\n                                      Purpose_vacation  Purpose_wedding  \nLoan ID                                                                  \n8deb9448-0939-4b4a-add5-89f4442fddf7                 0                0  \nb4f3602e-8187-4076-966a-cfd71084e8f4                 0                0  \n1103946b-1671-4c39-9bf5-923c10ce0cb1                 0                0  \n8040dec3-ef55-4baa-bc20-1c2853ccace7                 0                0  \nf4ce26ef-0e52-475c-9d62-75a78c68a322                 0                0  \n...                                                ...              ...  \n9baefe4e-d64e-4db9-a303-5ad61a7b7381                 0                0  \n1471a3f6-63ed-43b4-9b21-58d3ce05b0a1                 0                0  \n33299529-29f2-4eb1-a730-16ac4ef0d57e                 0                0  \n12f11253-2e09-4ced-8b5e-83439cba7eb0                 0                0  \nfb49cc3e-b551-413d-80b6-47db7f903ede                 0                0  \n\n[266 rows x 31 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Current Loan Amount</th>\n      <th>Credit Score</th>\n      <th>Annual Income</th>\n      <th>Monthly Debt</th>\n      <th>Years of Credit History</th>\n      <th>Months since last delinquent</th>\n      <th>Number of Open Accounts</th>\n      <th>Number of Credit Problems</th>\n      <th>Current Credit Balance</th>\n      <th>Maximum Open Credit</th>\n      <th>...</th>\n      <th>Purpose_Medical Bills</th>\n      <th>Purpose_Other</th>\n      <th>Purpose_Take a Trip</th>\n      <th>Purpose_major_purchase</th>\n      <th>Purpose_moving</th>\n      <th>Purpose_other</th>\n      <th>Purpose_renewable_energy</th>\n      <th>Purpose_small_business</th>\n      <th>Purpose_vacation</th>\n      <th>Purpose_wedding</th>\n    </tr>\n    <tr>\n      <th>Loan ID</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>8deb9448-0939-4b4a-add5-89f4442fddf7</th>\n      <td>155628.0</td>\n      <td>645.0</td>\n      <td>1263956.0</td>\n      <td>10216.87</td>\n      <td>13.5</td>\n      <td>43.0</td>\n      <td>12.0</td>\n      <td>0.0</td>\n      <td>174781.0</td>\n      <td>417274.0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>b4f3602e-8187-4076-966a-cfd71084e8f4</th>\n      <td>161810.0</td>\n      <td>724.0</td>\n      <td>1397507.0</td>\n      <td>21311.92</td>\n      <td>18.0</td>\n      <td>5.0</td>\n      <td>12.0</td>\n      <td>0.0</td>\n      <td>108395.0</td>\n      <td>189860.0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1103946b-1671-4c39-9bf5-923c10ce0cb1</th>\n      <td>130064.0</td>\n      <td>726.0</td>\n      <td>1310582.0</td>\n      <td>18784.92</td>\n      <td>14.0</td>\n      <td>25.0</td>\n      <td>15.0</td>\n      <td>0.0</td>\n      <td>536769.0</td>\n      <td>1195260.0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>8040dec3-ef55-4baa-bc20-1c2853ccace7</th>\n      <td>117282.0</td>\n      <td>734.0</td>\n      <td>489953.0</td>\n      <td>7961.76</td>\n      <td>13.8</td>\n      <td>9.0</td>\n      <td>13.0</td>\n      <td>0.0</td>\n      <td>195225.0</td>\n      <td>314402.0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>f4ce26ef-0e52-475c-9d62-75a78c68a322</th>\n      <td>215578.0</td>\n      <td>723.0</td>\n      <td>1008444.0</td>\n      <td>18236.01</td>\n      <td>22.5</td>\n      <td>42.0</td>\n      <td>13.0</td>\n      <td>0.0</td>\n      <td>569943.0</td>\n      <td>920414.0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>9baefe4e-d64e-4db9-a303-5ad61a7b7381</th>\n      <td>220528.0</td>\n      <td>664.0</td>\n      <td>914185.0</td>\n      <td>13103.35</td>\n      <td>32.5</td>\n      <td>6.0</td>\n      <td>8.0</td>\n      <td>0.0</td>\n      <td>78261.0</td>\n      <td>187594.0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1471a3f6-63ed-43b4-9b21-58d3ce05b0a1</th>\n      <td>251944.0</td>\n      <td>744.0</td>\n      <td>946010.0</td>\n      <td>12613.53</td>\n      <td>11.9</td>\n      <td>13.0</td>\n      <td>22.0</td>\n      <td>0.0</td>\n      <td>170050.0</td>\n      <td>674344.0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>33299529-29f2-4eb1-a730-16ac4ef0d57e</th>\n      <td>133672.0</td>\n      <td>707.0</td>\n      <td>423263.0</td>\n      <td>9488.22</td>\n      <td>45.5</td>\n      <td>26.0</td>\n      <td>9.0</td>\n      <td>0.0</td>\n      <td>223820.0</td>\n      <td>362956.0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>12f11253-2e09-4ced-8b5e-83439cba7eb0</th>\n      <td>257532.0</td>\n      <td>740.0</td>\n      <td>1056476.0</td>\n      <td>12237.33</td>\n      <td>15.3</td>\n      <td>12.0</td>\n      <td>5.0</td>\n      <td>0.0</td>\n      <td>153387.0</td>\n      <td>315480.0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>fb49cc3e-b551-413d-80b6-47db7f903ede</th>\n      <td>263692.0</td>\n      <td>731.0</td>\n      <td>1138632.0</td>\n      <td>19280.82</td>\n      <td>18.4</td>\n      <td>31.0</td>\n      <td>6.0</td>\n      <td>0.0</td>\n      <td>120669.0</td>\n      <td>289894.0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>266 rows × 31 columns</p>\n</div>"
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.iloc[only_sl_correct]\n",
    "#y_test.iloc[only_sl_correct]\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-22T16:09:31.078589300Z",
     "start_time": "2024-06-22T16:09:30.750305900Z"
    }
   },
   "id": "c422023b68603d7"
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "data": {
      "text/plain": "Empty DataFrame\nColumns: [Current Loan Amount, Credit Score, Annual Income, Monthly Debt, Years of Credit History, Months since last delinquent, Number of Open Accounts, Number of Credit Problems, Current Credit Balance, Maximum Open Credit, Bankruptcies, Tax Liens, Term_Short Term, Home Ownership_Home Mortgage, Home Ownership_Own Home, Home Ownership_Rent, Purpose_Buy House, Purpose_Buy a Car, Purpose_Debt Consolidation, Purpose_Educational Expenses, Purpose_Home Improvements, Purpose_Medical Bills, Purpose_Other, Purpose_Take a Trip, Purpose_major_purchase, Purpose_moving, Purpose_other, Purpose_renewable_energy, Purpose_small_business, Purpose_vacation, Purpose_wedding]\nIndex: []\n\n[0 rows x 31 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Current Loan Amount</th>\n      <th>Credit Score</th>\n      <th>Annual Income</th>\n      <th>Monthly Debt</th>\n      <th>Years of Credit History</th>\n      <th>Months since last delinquent</th>\n      <th>Number of Open Accounts</th>\n      <th>Number of Credit Problems</th>\n      <th>Current Credit Balance</th>\n      <th>Maximum Open Credit</th>\n      <th>...</th>\n      <th>Purpose_Medical Bills</th>\n      <th>Purpose_Other</th>\n      <th>Purpose_Take a Trip</th>\n      <th>Purpose_major_purchase</th>\n      <th>Purpose_moving</th>\n      <th>Purpose_other</th>\n      <th>Purpose_renewable_energy</th>\n      <th>Purpose_small_business</th>\n      <th>Purpose_vacation</th>\n      <th>Purpose_wedding</th>\n    </tr>\n    <tr>\n      <th>Loan ID</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table>\n<p>0 rows × 31 columns</p>\n</div>"
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[X_train['Credit Score']>850]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-22T16:09:33.192234300Z",
     "start_time": "2024-06-22T16:09:31.988126400Z"
    }
   },
   "id": "fae263803ab9ed70"
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "rule_matched = torch.nonzero(rule, as_tuple=True)[0].tolist()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-22T16:09:33.553904600Z",
     "start_time": "2024-06-22T16:09:32.677895800Z"
    }
   },
   "id": "ebbce3782208f229"
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "intersection_sl = [value for value in only_sl_correct if value in rule_matched]\n",
    "intersection_lr = [value for value in only_lr_correct if value in rule_matched]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-22T16:09:34.350538100Z",
     "start_time": "2024-06-22T16:09:33.382536500Z"
    }
   },
   "id": "ddc92b0ad27b4b1d"
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "data": {
      "text/plain": "                                      Current Loan Amount  Credit Score  \\\nLoan ID                                                                   \n8deb9448-0939-4b4a-add5-89f4442fddf7             155628.0         645.0   \nf4ce26ef-0e52-475c-9d62-75a78c68a322             215578.0         723.0   \n1f5af938-7a12-4a47-a7b4-6ffdce0f25a6             462704.0         735.0   \n36909248-ba22-4b16-8e2f-a238229064f8              51744.0         706.0   \na35bc423-f2d0-4300-860c-39695faccf8f             388520.0         639.0   \n...                                                   ...           ...   \n5cf1e7c7-8b29-4495-a778-540275568d1d             108130.0         737.0   \nd557a41b-14fb-4a3f-b4eb-1d35d7325dea             224818.0         745.0   \n19bac196-ba5a-4473-980a-1d351c02f98f             659934.0         714.0   \n849a2097-c1cb-4d69-8aab-725b562e5aa2             196130.0         711.0   \n12f11253-2e09-4ced-8b5e-83439cba7eb0             257532.0         740.0   \n\n                                      Annual Income  Monthly Debt  \\\nLoan ID                                                             \n8deb9448-0939-4b4a-add5-89f4442fddf7      1263956.0      10216.87   \nf4ce26ef-0e52-475c-9d62-75a78c68a322      1008444.0      18236.01   \n1f5af938-7a12-4a47-a7b4-6ffdce0f25a6      2473705.0      26179.91   \n36909248-ba22-4b16-8e2f-a238229064f8      1042606.0       3371.17   \na35bc423-f2d0-4300-860c-39695faccf8f      3914589.0      51215.83   \n...                                             ...           ...   \n5cf1e7c7-8b29-4495-a778-540275568d1d       644290.0       9020.06   \nd557a41b-14fb-4a3f-b4eb-1d35d7325dea      1359127.0      22312.46   \n19bac196-ba5a-4473-980a-1d351c02f98f      2754867.0      39716.08   \n849a2097-c1cb-4d69-8aab-725b562e5aa2      1242087.0      16250.70   \n12f11253-2e09-4ced-8b5e-83439cba7eb0      1056476.0      12237.33   \n\n                                      Years of Credit History  \\\nLoan ID                                                         \n8deb9448-0939-4b4a-add5-89f4442fddf7                     13.5   \nf4ce26ef-0e52-475c-9d62-75a78c68a322                     22.5   \n1f5af938-7a12-4a47-a7b4-6ffdce0f25a6                     27.9   \n36909248-ba22-4b16-8e2f-a238229064f8                     41.8   \na35bc423-f2d0-4300-860c-39695faccf8f                     16.0   \n...                                                       ...   \n5cf1e7c7-8b29-4495-a778-540275568d1d                     13.0   \nd557a41b-14fb-4a3f-b4eb-1d35d7325dea                     15.4   \n19bac196-ba5a-4473-980a-1d351c02f98f                     29.2   \n849a2097-c1cb-4d69-8aab-725b562e5aa2                     20.6   \n12f11253-2e09-4ced-8b5e-83439cba7eb0                     15.3   \n\n                                      Months since last delinquent  \\\nLoan ID                                                              \n8deb9448-0939-4b4a-add5-89f4442fddf7                          43.0   \nf4ce26ef-0e52-475c-9d62-75a78c68a322                          42.0   \n1f5af938-7a12-4a47-a7b4-6ffdce0f25a6                          43.0   \n36909248-ba22-4b16-8e2f-a238229064f8                          35.0   \na35bc423-f2d0-4300-860c-39695faccf8f                          10.0   \n...                                                            ...   \n5cf1e7c7-8b29-4495-a778-540275568d1d                           6.0   \nd557a41b-14fb-4a3f-b4eb-1d35d7325dea                          63.0   \n19bac196-ba5a-4473-980a-1d351c02f98f                          24.0   \n849a2097-c1cb-4d69-8aab-725b562e5aa2                          36.0   \n12f11253-2e09-4ced-8b5e-83439cba7eb0                          12.0   \n\n                                      Number of Open Accounts  \\\nLoan ID                                                         \n8deb9448-0939-4b4a-add5-89f4442fddf7                     12.0   \nf4ce26ef-0e52-475c-9d62-75a78c68a322                     13.0   \n1f5af938-7a12-4a47-a7b4-6ffdce0f25a6                     13.0   \n36909248-ba22-4b16-8e2f-a238229064f8                      8.0   \na35bc423-f2d0-4300-860c-39695faccf8f                     24.0   \n...                                                       ...   \n5cf1e7c7-8b29-4495-a778-540275568d1d                     11.0   \nd557a41b-14fb-4a3f-b4eb-1d35d7325dea                      9.0   \n19bac196-ba5a-4473-980a-1d351c02f98f                      8.0   \n849a2097-c1cb-4d69-8aab-725b562e5aa2                     17.0   \n12f11253-2e09-4ced-8b5e-83439cba7eb0                      5.0   \n\n                                      Number of Credit Problems  \\\nLoan ID                                                           \n8deb9448-0939-4b4a-add5-89f4442fddf7                        0.0   \nf4ce26ef-0e52-475c-9d62-75a78c68a322                        0.0   \n1f5af938-7a12-4a47-a7b4-6ffdce0f25a6                        0.0   \n36909248-ba22-4b16-8e2f-a238229064f8                        0.0   \na35bc423-f2d0-4300-860c-39695faccf8f                        0.0   \n...                                                         ...   \n5cf1e7c7-8b29-4495-a778-540275568d1d                        0.0   \nd557a41b-14fb-4a3f-b4eb-1d35d7325dea                        1.0   \n19bac196-ba5a-4473-980a-1d351c02f98f                        0.0   \n849a2097-c1cb-4d69-8aab-725b562e5aa2                        1.0   \n12f11253-2e09-4ced-8b5e-83439cba7eb0                        0.0   \n\n                                      Current Credit Balance  \\\nLoan ID                                                        \n8deb9448-0939-4b4a-add5-89f4442fddf7                174781.0   \nf4ce26ef-0e52-475c-9d62-75a78c68a322                569943.0   \n1f5af938-7a12-4a47-a7b4-6ffdce0f25a6                622136.0   \n36909248-ba22-4b16-8e2f-a238229064f8                 29545.0   \na35bc423-f2d0-4300-860c-39695faccf8f                292448.0   \n...                                                      ...   \n5cf1e7c7-8b29-4495-a778-540275568d1d                108547.0   \nd557a41b-14fb-4a3f-b4eb-1d35d7325dea                108262.0   \n19bac196-ba5a-4473-980a-1d351c02f98f                926478.0   \n849a2097-c1cb-4d69-8aab-725b562e5aa2                181336.0   \n12f11253-2e09-4ced-8b5e-83439cba7eb0                153387.0   \n\n                                      Maximum Open Credit  ...  \\\nLoan ID                                                    ...   \n8deb9448-0939-4b4a-add5-89f4442fddf7             417274.0  ...   \nf4ce26ef-0e52-475c-9d62-75a78c68a322             920414.0  ...   \n1f5af938-7a12-4a47-a7b4-6ffdce0f25a6            1549174.0  ...   \n36909248-ba22-4b16-8e2f-a238229064f8              53878.0  ...   \na35bc423-f2d0-4300-860c-39695faccf8f             314710.0  ...   \n...                                                   ...  ...   \n5cf1e7c7-8b29-4495-a778-540275568d1d             287606.0  ...   \nd557a41b-14fb-4a3f-b4eb-1d35d7325dea             220330.0  ...   \n19bac196-ba5a-4473-980a-1d351c02f98f            1347698.0  ...   \n849a2097-c1cb-4d69-8aab-725b562e5aa2             613954.0  ...   \n12f11253-2e09-4ced-8b5e-83439cba7eb0             315480.0  ...   \n\n                                      Purpose_Medical Bills  Purpose_Other  \\\nLoan ID                                                                      \n8deb9448-0939-4b4a-add5-89f4442fddf7                      0              0   \nf4ce26ef-0e52-475c-9d62-75a78c68a322                      0              0   \n1f5af938-7a12-4a47-a7b4-6ffdce0f25a6                      0              0   \n36909248-ba22-4b16-8e2f-a238229064f8                      0              0   \na35bc423-f2d0-4300-860c-39695faccf8f                      0              0   \n...                                                     ...            ...   \n5cf1e7c7-8b29-4495-a778-540275568d1d                      0              0   \nd557a41b-14fb-4a3f-b4eb-1d35d7325dea                      0              0   \n19bac196-ba5a-4473-980a-1d351c02f98f                      0              0   \n849a2097-c1cb-4d69-8aab-725b562e5aa2                      0              0   \n12f11253-2e09-4ced-8b5e-83439cba7eb0                      0              0   \n\n                                      Purpose_Take a Trip  \\\nLoan ID                                                     \n8deb9448-0939-4b4a-add5-89f4442fddf7                    0   \nf4ce26ef-0e52-475c-9d62-75a78c68a322                    0   \n1f5af938-7a12-4a47-a7b4-6ffdce0f25a6                    0   \n36909248-ba22-4b16-8e2f-a238229064f8                    0   \na35bc423-f2d0-4300-860c-39695faccf8f                    0   \n...                                                   ...   \n5cf1e7c7-8b29-4495-a778-540275568d1d                    0   \nd557a41b-14fb-4a3f-b4eb-1d35d7325dea                    0   \n19bac196-ba5a-4473-980a-1d351c02f98f                    0   \n849a2097-c1cb-4d69-8aab-725b562e5aa2                    0   \n12f11253-2e09-4ced-8b5e-83439cba7eb0                    0   \n\n                                      Purpose_major_purchase  Purpose_moving  \\\nLoan ID                                                                        \n8deb9448-0939-4b4a-add5-89f4442fddf7                       0               0   \nf4ce26ef-0e52-475c-9d62-75a78c68a322                       0               0   \n1f5af938-7a12-4a47-a7b4-6ffdce0f25a6                       0               0   \n36909248-ba22-4b16-8e2f-a238229064f8                       0               0   \na35bc423-f2d0-4300-860c-39695faccf8f                       0               0   \n...                                                      ...             ...   \n5cf1e7c7-8b29-4495-a778-540275568d1d                       0               0   \nd557a41b-14fb-4a3f-b4eb-1d35d7325dea                       0               0   \n19bac196-ba5a-4473-980a-1d351c02f98f                       0               0   \n849a2097-c1cb-4d69-8aab-725b562e5aa2                       0               0   \n12f11253-2e09-4ced-8b5e-83439cba7eb0                       0               0   \n\n                                      Purpose_other  Purpose_renewable_energy  \\\nLoan ID                                                                         \n8deb9448-0939-4b4a-add5-89f4442fddf7              0                         0   \nf4ce26ef-0e52-475c-9d62-75a78c68a322              0                         0   \n1f5af938-7a12-4a47-a7b4-6ffdce0f25a6              0                         0   \n36909248-ba22-4b16-8e2f-a238229064f8              0                         0   \na35bc423-f2d0-4300-860c-39695faccf8f              0                         0   \n...                                             ...                       ...   \n5cf1e7c7-8b29-4495-a778-540275568d1d              0                         0   \nd557a41b-14fb-4a3f-b4eb-1d35d7325dea              0                         0   \n19bac196-ba5a-4473-980a-1d351c02f98f              0                         0   \n849a2097-c1cb-4d69-8aab-725b562e5aa2              0                         0   \n12f11253-2e09-4ced-8b5e-83439cba7eb0              0                         0   \n\n                                      Purpose_small_business  \\\nLoan ID                                                        \n8deb9448-0939-4b4a-add5-89f4442fddf7                       0   \nf4ce26ef-0e52-475c-9d62-75a78c68a322                       0   \n1f5af938-7a12-4a47-a7b4-6ffdce0f25a6                       0   \n36909248-ba22-4b16-8e2f-a238229064f8                       0   \na35bc423-f2d0-4300-860c-39695faccf8f                       0   \n...                                                      ...   \n5cf1e7c7-8b29-4495-a778-540275568d1d                       0   \nd557a41b-14fb-4a3f-b4eb-1d35d7325dea                       0   \n19bac196-ba5a-4473-980a-1d351c02f98f                       0   \n849a2097-c1cb-4d69-8aab-725b562e5aa2                       0   \n12f11253-2e09-4ced-8b5e-83439cba7eb0                       0   \n\n                                      Purpose_vacation  Purpose_wedding  \nLoan ID                                                                  \n8deb9448-0939-4b4a-add5-89f4442fddf7                 0                0  \nf4ce26ef-0e52-475c-9d62-75a78c68a322                 0                0  \n1f5af938-7a12-4a47-a7b4-6ffdce0f25a6                 0                0  \n36909248-ba22-4b16-8e2f-a238229064f8                 0                0  \na35bc423-f2d0-4300-860c-39695faccf8f                 0                0  \n...                                                ...              ...  \n5cf1e7c7-8b29-4495-a778-540275568d1d                 0                0  \nd557a41b-14fb-4a3f-b4eb-1d35d7325dea                 0                0  \n19bac196-ba5a-4473-980a-1d351c02f98f                 0                0  \n849a2097-c1cb-4d69-8aab-725b562e5aa2                 0                0  \n12f11253-2e09-4ced-8b5e-83439cba7eb0                 0                0  \n\n[167 rows x 31 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Current Loan Amount</th>\n      <th>Credit Score</th>\n      <th>Annual Income</th>\n      <th>Monthly Debt</th>\n      <th>Years of Credit History</th>\n      <th>Months since last delinquent</th>\n      <th>Number of Open Accounts</th>\n      <th>Number of Credit Problems</th>\n      <th>Current Credit Balance</th>\n      <th>Maximum Open Credit</th>\n      <th>...</th>\n      <th>Purpose_Medical Bills</th>\n      <th>Purpose_Other</th>\n      <th>Purpose_Take a Trip</th>\n      <th>Purpose_major_purchase</th>\n      <th>Purpose_moving</th>\n      <th>Purpose_other</th>\n      <th>Purpose_renewable_energy</th>\n      <th>Purpose_small_business</th>\n      <th>Purpose_vacation</th>\n      <th>Purpose_wedding</th>\n    </tr>\n    <tr>\n      <th>Loan ID</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>8deb9448-0939-4b4a-add5-89f4442fddf7</th>\n      <td>155628.0</td>\n      <td>645.0</td>\n      <td>1263956.0</td>\n      <td>10216.87</td>\n      <td>13.5</td>\n      <td>43.0</td>\n      <td>12.0</td>\n      <td>0.0</td>\n      <td>174781.0</td>\n      <td>417274.0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>f4ce26ef-0e52-475c-9d62-75a78c68a322</th>\n      <td>215578.0</td>\n      <td>723.0</td>\n      <td>1008444.0</td>\n      <td>18236.01</td>\n      <td>22.5</td>\n      <td>42.0</td>\n      <td>13.0</td>\n      <td>0.0</td>\n      <td>569943.0</td>\n      <td>920414.0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1f5af938-7a12-4a47-a7b4-6ffdce0f25a6</th>\n      <td>462704.0</td>\n      <td>735.0</td>\n      <td>2473705.0</td>\n      <td>26179.91</td>\n      <td>27.9</td>\n      <td>43.0</td>\n      <td>13.0</td>\n      <td>0.0</td>\n      <td>622136.0</td>\n      <td>1549174.0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>36909248-ba22-4b16-8e2f-a238229064f8</th>\n      <td>51744.0</td>\n      <td>706.0</td>\n      <td>1042606.0</td>\n      <td>3371.17</td>\n      <td>41.8</td>\n      <td>35.0</td>\n      <td>8.0</td>\n      <td>0.0</td>\n      <td>29545.0</td>\n      <td>53878.0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>a35bc423-f2d0-4300-860c-39695faccf8f</th>\n      <td>388520.0</td>\n      <td>639.0</td>\n      <td>3914589.0</td>\n      <td>51215.83</td>\n      <td>16.0</td>\n      <td>10.0</td>\n      <td>24.0</td>\n      <td>0.0</td>\n      <td>292448.0</td>\n      <td>314710.0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>5cf1e7c7-8b29-4495-a778-540275568d1d</th>\n      <td>108130.0</td>\n      <td>737.0</td>\n      <td>644290.0</td>\n      <td>9020.06</td>\n      <td>13.0</td>\n      <td>6.0</td>\n      <td>11.0</td>\n      <td>0.0</td>\n      <td>108547.0</td>\n      <td>287606.0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>d557a41b-14fb-4a3f-b4eb-1d35d7325dea</th>\n      <td>224818.0</td>\n      <td>745.0</td>\n      <td>1359127.0</td>\n      <td>22312.46</td>\n      <td>15.4</td>\n      <td>63.0</td>\n      <td>9.0</td>\n      <td>1.0</td>\n      <td>108262.0</td>\n      <td>220330.0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>19bac196-ba5a-4473-980a-1d351c02f98f</th>\n      <td>659934.0</td>\n      <td>714.0</td>\n      <td>2754867.0</td>\n      <td>39716.08</td>\n      <td>29.2</td>\n      <td>24.0</td>\n      <td>8.0</td>\n      <td>0.0</td>\n      <td>926478.0</td>\n      <td>1347698.0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>849a2097-c1cb-4d69-8aab-725b562e5aa2</th>\n      <td>196130.0</td>\n      <td>711.0</td>\n      <td>1242087.0</td>\n      <td>16250.70</td>\n      <td>20.6</td>\n      <td>36.0</td>\n      <td>17.0</td>\n      <td>1.0</td>\n      <td>181336.0</td>\n      <td>613954.0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>12f11253-2e09-4ced-8b5e-83439cba7eb0</th>\n      <td>257532.0</td>\n      <td>740.0</td>\n      <td>1056476.0</td>\n      <td>12237.33</td>\n      <td>15.3</td>\n      <td>12.0</td>\n      <td>5.0</td>\n      <td>0.0</td>\n      <td>153387.0</td>\n      <td>315480.0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>167 rows × 31 columns</p>\n</div>"
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.iloc[intersection_sl]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-22T16:09:36.969294200Z",
     "start_time": "2024-06-22T16:09:36.750682400Z"
    }
   },
   "id": "a131c6799e0c6729"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Multilayer perceptrons"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9588f5ad65a8d9fc"
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [
    "from importlib import reload  \n",
    "import models.loss\n",
    "reload(models.loss)\n",
    "from models.loss import semantic_loss"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-22T16:09:38.977282800Z",
     "start_time": "2024-06-22T16:09:38.761576900Z"
    }
   },
   "id": "b7bedaae61d6eb10"
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "from importlib import reload  \n",
    "import models.model\n",
    "reload(models.model)\n",
    "from models.model import MLP"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-22T16:09:40.455978600Z",
     "start_time": "2024-06-22T16:09:40.295006300Z"
    }
   },
   "id": "6f3ed7b5dcc89fe"
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [],
   "source": [
    "credit_score_index = X_alt_train.columns.get_loc('Credit Score')\n",
    "short_term_index = X_alt_train.columns.get_loc('Term_Short Term')\n",
    "annual_income_index = X_alt_train.columns.get_loc('Annual Income')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-22T16:09:44.091809100Z",
     "start_time": "2024-06-22T16:09:43.468426300Z"
    }
   },
   "id": "42c540d1c26b57af"
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [],
   "source": [
    "dataset = LCDataset(X_train_tensor, y_train_tensor)\n",
    "\n",
    "data_loader = DataLoader(dataset, batch_size=5000, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-22T16:09:44.371504Z",
     "start_time": "2024-06-22T16:09:43.850448100Z"
    }
   },
   "id": "10e7d8b1ef14dc64"
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "data": {
      "text/plain": "==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nMLP                                      [23034, 1]                --\n├─Sequential: 1-1                        [23034, 1]                --\n│    └─Linear: 2-1                       [23034, 3]                96\n│    └─ReLU: 2-2                         [23034, 3]                --\n│    └─Linear: 2-3                       [23034, 1]                4\n│    └─Sigmoid: 2-4                      [23034, 1]                --\n==========================================================================================\nTotal params: 100\nTrainable params: 100\nNon-trainable params: 0\nTotal mult-adds (Units.MEGABYTES): 2.30\n==========================================================================================\nInput size (MB): 2.86\nForward/backward pass size (MB): 0.74\nParams size (MB): 0.00\nEstimated Total Size (MB): 3.59\n=========================================================================================="
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_model = MLP(num_features=n_features)\n",
    "summary(mlp_model, input_size=X_train.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-22T16:09:45.372024Z",
     "start_time": "2024-06-22T16:09:45.057684100Z"
    }
   },
   "id": "d8fc4f5e8a062773"
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [
    "optimizer_mlp = optim.Adam(mlp_model.parameters())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-22T16:09:46.210586Z",
     "start_time": "2024-06-22T16:09:45.995227Z"
    }
   },
   "id": "64975a352a636960"
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \t| Train loss: 0.256 \t| Train acc: 0.3 \t| Test acc: 0.29\n",
      "Epoch: 1 \t| Train loss: 0.26 \t| Train acc: 0.31 \t| Test acc: 0.3\n",
      "Epoch: 2 \t| Train loss: 0.256 \t| Train acc: 0.32 \t| Test acc: 0.31\n",
      "Epoch: 3 \t| Train loss: 0.256 \t| Train acc: 0.34 \t| Test acc: 0.32\n",
      "Epoch: 4 \t| Train loss: 0.257 \t| Train acc: 0.35 \t| Test acc: 0.34\n",
      "Epoch: 5 \t| Train loss: 0.255 \t| Train acc: 0.36 \t| Test acc: 0.35\n",
      "Epoch: 6 \t| Train loss: 0.251 \t| Train acc: 0.37 \t| Test acc: 0.37\n",
      "Epoch: 7 \t| Train loss: 0.25 \t| Train acc: 0.38 \t| Test acc: 0.38\n",
      "Epoch: 8 \t| Train loss: 0.253 \t| Train acc: 0.4 \t| Test acc: 0.4\n",
      "Epoch: 9 \t| Train loss: 0.256 \t| Train acc: 0.42 \t| Test acc: 0.42\n",
      "Epoch: 10 \t| Train loss: 0.246 \t| Train acc: 0.43 \t| Test acc: 0.44\n",
      "Epoch: 11 \t| Train loss: 0.255 \t| Train acc: 0.46 \t| Test acc: 0.46\n",
      "Epoch: 12 \t| Train loss: 0.253 \t| Train acc: 0.47 \t| Test acc: 0.48\n",
      "Epoch: 13 \t| Train loss: 0.254 \t| Train acc: 0.48 \t| Test acc: 0.49\n",
      "Epoch: 14 \t| Train loss: 0.255 \t| Train acc: 0.52 \t| Test acc: 0.51\n",
      "Epoch: 15 \t| Train loss: 0.253 \t| Train acc: 0.51 \t| Test acc: 0.52\n",
      "Epoch: 16 \t| Train loss: 0.251 \t| Train acc: 0.53 \t| Test acc: 0.53\n",
      "Epoch: 17 \t| Train loss: 0.254 \t| Train acc: 0.52 \t| Test acc: 0.54\n",
      "Epoch: 18 \t| Train loss: 0.247 \t| Train acc: 0.53 \t| Test acc: 0.54\n",
      "Epoch: 19 \t| Train loss: 0.248 \t| Train acc: 0.55 \t| Test acc: 0.55\n",
      "Epoch: 20 \t| Train loss: 0.245 \t| Train acc: 0.55 \t| Test acc: 0.55\n",
      "Epoch: 21 \t| Train loss: 0.247 \t| Train acc: 0.56 \t| Test acc: 0.56\n",
      "Epoch: 22 \t| Train loss: 0.249 \t| Train acc: 0.55 \t| Test acc: 0.56\n",
      "Epoch: 23 \t| Train loss: 0.246 \t| Train acc: 0.56 \t| Test acc: 0.56\n",
      "Epoch: 24 \t| Train loss: 0.251 \t| Train acc: 0.56 \t| Test acc: 0.57\n",
      "Epoch: 25 \t| Train loss: 0.248 \t| Train acc: 0.56 \t| Test acc: 0.57\n",
      "Epoch: 26 \t| Train loss: 0.249 \t| Train acc: 0.55 \t| Test acc: 0.57\n",
      "Epoch: 27 \t| Train loss: 0.249 \t| Train acc: 0.58 \t| Test acc: 0.57\n",
      "Epoch: 28 \t| Train loss: 0.251 \t| Train acc: 0.55 \t| Test acc: 0.57\n",
      "Epoch: 29 \t| Train loss: 0.248 \t| Train acc: 0.55 \t| Test acc: 0.57\n",
      "Epoch: 30 \t| Train loss: 0.248 \t| Train acc: 0.57 \t| Test acc: 0.57\n",
      "Epoch: 31 \t| Train loss: 0.246 \t| Train acc: 0.56 \t| Test acc: 0.57\n",
      "Epoch: 32 \t| Train loss: 0.252 \t| Train acc: 0.58 \t| Test acc: 0.58\n",
      "Epoch: 33 \t| Train loss: 0.248 \t| Train acc: 0.56 \t| Test acc: 0.58\n",
      "Epoch: 34 \t| Train loss: 0.246 \t| Train acc: 0.58 \t| Test acc: 0.58\n",
      "Epoch: 35 \t| Train loss: 0.246 \t| Train acc: 0.56 \t| Test acc: 0.58\n",
      "Epoch: 36 \t| Train loss: 0.245 \t| Train acc: 0.56 \t| Test acc: 0.58\n",
      "Epoch: 37 \t| Train loss: 0.246 \t| Train acc: 0.56 \t| Test acc: 0.58\n",
      "Epoch: 38 \t| Train loss: 0.25 \t| Train acc: 0.56 \t| Test acc: 0.58\n",
      "Epoch: 39 \t| Train loss: 0.247 \t| Train acc: 0.57 \t| Test acc: 0.58\n",
      "Epoch: 40 \t| Train loss: 0.246 \t| Train acc: 0.58 \t| Test acc: 0.58\n",
      "Epoch: 41 \t| Train loss: 0.247 \t| Train acc: 0.57 \t| Test acc: 0.59\n",
      "Epoch: 42 \t| Train loss: 0.25 \t| Train acc: 0.55 \t| Test acc: 0.59\n",
      "Epoch: 43 \t| Train loss: 0.244 \t| Train acc: 0.57 \t| Test acc: 0.59\n",
      "Epoch: 44 \t| Train loss: 0.248 \t| Train acc: 0.57 \t| Test acc: 0.59\n",
      "Epoch: 45 \t| Train loss: 0.25 \t| Train acc: 0.57 \t| Test acc: 0.59\n",
      "Epoch: 46 \t| Train loss: 0.251 \t| Train acc: 0.57 \t| Test acc: 0.59\n",
      "Epoch: 47 \t| Train loss: 0.251 \t| Train acc: 0.57 \t| Test acc: 0.59\n",
      "Epoch: 48 \t| Train loss: 0.252 \t| Train acc: 0.58 \t| Test acc: 0.59\n",
      "Epoch: 49 \t| Train loss: 0.246 \t| Train acc: 0.6 \t| Test acc: 0.59\n",
      "Epoch: 50 \t| Train loss: 0.247 \t| Train acc: 0.59 \t| Test acc: 0.59\n",
      "Epoch: 51 \t| Train loss: 0.247 \t| Train acc: 0.58 \t| Test acc: 0.59\n",
      "Epoch: 52 \t| Train loss: 0.246 \t| Train acc: 0.57 \t| Test acc: 0.59\n",
      "Epoch: 53 \t| Train loss: 0.246 \t| Train acc: 0.59 \t| Test acc: 0.59\n",
      "Epoch: 54 \t| Train loss: 0.249 \t| Train acc: 0.59 \t| Test acc: 0.59\n",
      "Epoch: 55 \t| Train loss: 0.24 \t| Train acc: 0.59 \t| Test acc: 0.59\n",
      "Epoch: 56 \t| Train loss: 0.243 \t| Train acc: 0.59 \t| Test acc: 0.6\n",
      "Epoch: 57 \t| Train loss: 0.249 \t| Train acc: 0.59 \t| Test acc: 0.6\n",
      "Epoch: 58 \t| Train loss: 0.245 \t| Train acc: 0.59 \t| Test acc: 0.6\n",
      "Epoch: 59 \t| Train loss: 0.248 \t| Train acc: 0.58 \t| Test acc: 0.6\n",
      "Epoch: 60 \t| Train loss: 0.245 \t| Train acc: 0.58 \t| Test acc: 0.6\n",
      "Epoch: 61 \t| Train loss: 0.247 \t| Train acc: 0.58 \t| Test acc: 0.6\n",
      "Epoch: 62 \t| Train loss: 0.245 \t| Train acc: 0.59 \t| Test acc: 0.6\n",
      "Epoch: 63 \t| Train loss: 0.245 \t| Train acc: 0.58 \t| Test acc: 0.6\n",
      "Epoch: 64 \t| Train loss: 0.25 \t| Train acc: 0.6 \t| Test acc: 0.6\n",
      "Epoch: 65 \t| Train loss: 0.246 \t| Train acc: 0.61 \t| Test acc: 0.6\n",
      "Epoch: 66 \t| Train loss: 0.246 \t| Train acc: 0.59 \t| Test acc: 0.6\n",
      "Epoch: 67 \t| Train loss: 0.245 \t| Train acc: 0.6 \t| Test acc: 0.6\n",
      "Epoch: 68 \t| Train loss: 0.251 \t| Train acc: 0.59 \t| Test acc: 0.6\n",
      "Epoch: 69 \t| Train loss: 0.245 \t| Train acc: 0.59 \t| Test acc: 0.6\n",
      "Epoch: 70 \t| Train loss: 0.248 \t| Train acc: 0.58 \t| Test acc: 0.6\n",
      "Epoch: 71 \t| Train loss: 0.242 \t| Train acc: 0.6 \t| Test acc: 0.6\n",
      "Epoch: 72 \t| Train loss: 0.244 \t| Train acc: 0.6 \t| Test acc: 0.6\n",
      "Epoch: 73 \t| Train loss: 0.249 \t| Train acc: 0.57 \t| Test acc: 0.6\n",
      "Epoch: 74 \t| Train loss: 0.248 \t| Train acc: 0.59 \t| Test acc: 0.6\n",
      "Epoch: 75 \t| Train loss: 0.245 \t| Train acc: 0.6 \t| Test acc: 0.6\n",
      "Epoch: 76 \t| Train loss: 0.246 \t| Train acc: 0.59 \t| Test acc: 0.6\n",
      "Epoch: 77 \t| Train loss: 0.253 \t| Train acc: 0.58 \t| Test acc: 0.6\n",
      "Epoch: 78 \t| Train loss: 0.246 \t| Train acc: 0.6 \t| Test acc: 0.6\n",
      "Epoch: 79 \t| Train loss: 0.245 \t| Train acc: 0.6 \t| Test acc: 0.6\n",
      "Epoch: 80 \t| Train loss: 0.246 \t| Train acc: 0.59 \t| Test acc: 0.6\n",
      "Epoch: 81 \t| Train loss: 0.243 \t| Train acc: 0.59 \t| Test acc: 0.6\n",
      "Epoch: 82 \t| Train loss: 0.245 \t| Train acc: 0.59 \t| Test acc: 0.6\n",
      "Epoch: 83 \t| Train loss: 0.246 \t| Train acc: 0.6 \t| Test acc: 0.6\n",
      "Epoch: 84 \t| Train loss: 0.247 \t| Train acc: 0.61 \t| Test acc: 0.6\n",
      "Epoch: 85 \t| Train loss: 0.244 \t| Train acc: 0.59 \t| Test acc: 0.6\n",
      "Epoch: 86 \t| Train loss: 0.241 \t| Train acc: 0.6 \t| Test acc: 0.6\n",
      "Epoch: 87 \t| Train loss: 0.245 \t| Train acc: 0.59 \t| Test acc: 0.6\n",
      "Epoch: 88 \t| Train loss: 0.243 \t| Train acc: 0.6 \t| Test acc: 0.6\n",
      "Epoch: 89 \t| Train loss: 0.247 \t| Train acc: 0.61 \t| Test acc: 0.61\n",
      "Epoch: 90 \t| Train loss: 0.248 \t| Train acc: 0.61 \t| Test acc: 0.61\n",
      "Epoch: 91 \t| Train loss: 0.247 \t| Train acc: 0.58 \t| Test acc: 0.61\n",
      "Epoch: 92 \t| Train loss: 0.244 \t| Train acc: 0.6 \t| Test acc: 0.61\n",
      "Epoch: 93 \t| Train loss: 0.242 \t| Train acc: 0.6 \t| Test acc: 0.61\n",
      "Epoch: 94 \t| Train loss: 0.243 \t| Train acc: 0.6 \t| Test acc: 0.61\n",
      "Epoch: 95 \t| Train loss: 0.246 \t| Train acc: 0.6 \t| Test acc: 0.61\n",
      "Epoch: 96 \t| Train loss: 0.246 \t| Train acc: 0.6 \t| Test acc: 0.61\n",
      "Epoch: 97 \t| Train loss: 0.247 \t| Train acc: 0.59 \t| Test acc: 0.61\n",
      "Epoch: 98 \t| Train loss: 0.249 \t| Train acc: 0.58 \t| Test acc: 0.61\n",
      "Epoch: 99 \t| Train loss: 0.241 \t| Train acc: 0.59 \t| Test acc: 0.61\n",
      "Epoch: 100 \t| Train loss: 0.244 \t| Train acc: 0.6 \t| Test acc: 0.61\n",
      "Epoch: 101 \t| Train loss: 0.24 \t| Train acc: 0.61 \t| Test acc: 0.6\n",
      "Epoch: 102 \t| Train loss: 0.243 \t| Train acc: 0.6 \t| Test acc: 0.61\n",
      "Epoch: 103 \t| Train loss: 0.243 \t| Train acc: 0.61 \t| Test acc: 0.61\n",
      "Epoch: 104 \t| Train loss: 0.242 \t| Train acc: 0.6 \t| Test acc: 0.61\n",
      "Epoch: 105 \t| Train loss: 0.244 \t| Train acc: 0.6 \t| Test acc: 0.61\n",
      "Epoch: 106 \t| Train loss: 0.243 \t| Train acc: 0.61 \t| Test acc: 0.61\n",
      "Epoch: 107 \t| Train loss: 0.248 \t| Train acc: 0.6 \t| Test acc: 0.61\n",
      "Epoch: 108 \t| Train loss: 0.248 \t| Train acc: 0.6 \t| Test acc: 0.61\n",
      "Epoch: 109 \t| Train loss: 0.244 \t| Train acc: 0.6 \t| Test acc: 0.61\n",
      "Epoch: 110 \t| Train loss: 0.247 \t| Train acc: 0.6 \t| Test acc: 0.61\n",
      "Epoch: 111 \t| Train loss: 0.245 \t| Train acc: 0.61 \t| Test acc: 0.6\n",
      "Epoch: 112 \t| Train loss: 0.245 \t| Train acc: 0.6 \t| Test acc: 0.6\n",
      "Epoch: 113 \t| Train loss: 0.247 \t| Train acc: 0.6 \t| Test acc: 0.61\n",
      "Epoch: 114 \t| Train loss: 0.245 \t| Train acc: 0.59 \t| Test acc: 0.6\n",
      "Epoch: 115 \t| Train loss: 0.247 \t| Train acc: 0.59 \t| Test acc: 0.6\n",
      "Epoch: 116 \t| Train loss: 0.247 \t| Train acc: 0.6 \t| Test acc: 0.6\n",
      "Epoch: 117 \t| Train loss: 0.248 \t| Train acc: 0.59 \t| Test acc: 0.6\n",
      "Epoch: 118 \t| Train loss: 0.244 \t| Train acc: 0.62 \t| Test acc: 0.6\n",
      "Epoch: 119 \t| Train loss: 0.24 \t| Train acc: 0.6 \t| Test acc: 0.6\n",
      "Epoch: 120 \t| Train loss: 0.245 \t| Train acc: 0.6 \t| Test acc: 0.6\n",
      "Epoch: 121 \t| Train loss: 0.241 \t| Train acc: 0.61 \t| Test acc: 0.6\n",
      "Epoch: 122 \t| Train loss: 0.24 \t| Train acc: 0.58 \t| Test acc: 0.6\n",
      "Epoch: 123 \t| Train loss: 0.242 \t| Train acc: 0.59 \t| Test acc: 0.6\n",
      "Epoch: 124 \t| Train loss: 0.242 \t| Train acc: 0.59 \t| Test acc: 0.6\n",
      "Epoch: 125 \t| Train loss: 0.244 \t| Train acc: 0.61 \t| Test acc: 0.6\n",
      "Epoch: 126 \t| Train loss: 0.249 \t| Train acc: 0.59 \t| Test acc: 0.6\n",
      "Epoch: 127 \t| Train loss: 0.243 \t| Train acc: 0.59 \t| Test acc: 0.6\n",
      "Epoch: 128 \t| Train loss: 0.245 \t| Train acc: 0.6 \t| Test acc: 0.6\n",
      "Epoch: 129 \t| Train loss: 0.244 \t| Train acc: 0.58 \t| Test acc: 0.6\n",
      "Epoch: 130 \t| Train loss: 0.242 \t| Train acc: 0.59 \t| Test acc: 0.6\n",
      "Epoch: 131 \t| Train loss: 0.243 \t| Train acc: 0.6 \t| Test acc: 0.6\n",
      "Epoch: 132 \t| Train loss: 0.249 \t| Train acc: 0.59 \t| Test acc: 0.61\n",
      "Epoch: 133 \t| Train loss: 0.24 \t| Train acc: 0.59 \t| Test acc: 0.61\n",
      "Epoch: 134 \t| Train loss: 0.253 \t| Train acc: 0.6 \t| Test acc: 0.61\n",
      "Epoch: 135 \t| Train loss: 0.247 \t| Train acc: 0.61 \t| Test acc: 0.61\n",
      "Epoch: 136 \t| Train loss: 0.245 \t| Train acc: 0.6 \t| Test acc: 0.6\n",
      "Epoch: 137 \t| Train loss: 0.243 \t| Train acc: 0.6 \t| Test acc: 0.6\n",
      "Epoch: 138 \t| Train loss: 0.246 \t| Train acc: 0.59 \t| Test acc: 0.6\n",
      "Epoch: 139 \t| Train loss: 0.245 \t| Train acc: 0.61 \t| Test acc: 0.6\n",
      "Epoch: 140 \t| Train loss: 0.244 \t| Train acc: 0.6 \t| Test acc: 0.6\n",
      "Epoch: 141 \t| Train loss: 0.241 \t| Train acc: 0.6 \t| Test acc: 0.6\n",
      "Epoch: 142 \t| Train loss: 0.246 \t| Train acc: 0.6 \t| Test acc: 0.6\n",
      "Epoch: 143 \t| Train loss: 0.249 \t| Train acc: 0.6 \t| Test acc: 0.6\n",
      "Epoch: 144 \t| Train loss: 0.25 \t| Train acc: 0.6 \t| Test acc: 0.6\n",
      "Epoch: 145 \t| Train loss: 0.245 \t| Train acc: 0.59 \t| Test acc: 0.6\n",
      "Epoch: 146 \t| Train loss: 0.244 \t| Train acc: 0.59 \t| Test acc: 0.6\n",
      "Epoch: 147 \t| Train loss: 0.241 \t| Train acc: 0.6 \t| Test acc: 0.6\n",
      "Epoch: 148 \t| Train loss: 0.241 \t| Train acc: 0.6 \t| Test acc: 0.6\n",
      "Epoch: 149 \t| Train loss: 0.239 \t| Train acc: 0.61 \t| Test acc: 0.6\n",
      "Epoch: 150 \t| Train loss: 0.246 \t| Train acc: 0.59 \t| Test acc: 0.6\n",
      "Epoch: 151 \t| Train loss: 0.239 \t| Train acc: 0.61 \t| Test acc: 0.6\n",
      "Epoch: 152 \t| Train loss: 0.242 \t| Train acc: 0.61 \t| Test acc: 0.6\n",
      "Epoch: 153 \t| Train loss: 0.25 \t| Train acc: 0.59 \t| Test acc: 0.6\n",
      "Epoch: 154 \t| Train loss: 0.242 \t| Train acc: 0.6 \t| Test acc: 0.6\n",
      "Epoch: 155 \t| Train loss: 0.244 \t| Train acc: 0.6 \t| Test acc: 0.6\n",
      "Epoch: 156 \t| Train loss: 0.242 \t| Train acc: 0.61 \t| Test acc: 0.6\n",
      "Epoch: 157 \t| Train loss: 0.249 \t| Train acc: 0.6 \t| Test acc: 0.6\n",
      "Epoch: 158 \t| Train loss: 0.241 \t| Train acc: 0.59 \t| Test acc: 0.6\n",
      "Epoch: 159 \t| Train loss: 0.255 \t| Train acc: 0.6 \t| Test acc: 0.6\n",
      "Epoch: 160 \t| Train loss: 0.246 \t| Train acc: 0.6 \t| Test acc: 0.6\n",
      "Epoch: 161 \t| Train loss: 0.243 \t| Train acc: 0.61 \t| Test acc: 0.6\n",
      "Epoch: 162 \t| Train loss: 0.245 \t| Train acc: 0.59 \t| Test acc: 0.6\n",
      "Epoch: 163 \t| Train loss: 0.246 \t| Train acc: 0.59 \t| Test acc: 0.6\n",
      "Epoch: 164 \t| Train loss: 0.246 \t| Train acc: 0.6 \t| Test acc: 0.6\n",
      "Epoch: 165 \t| Train loss: 0.251 \t| Train acc: 0.61 \t| Test acc: 0.6\n",
      "Epoch: 166 \t| Train loss: 0.244 \t| Train acc: 0.6 \t| Test acc: 0.6\n",
      "Epoch: 167 \t| Train loss: 0.244 \t| Train acc: 0.59 \t| Test acc: 0.6\n",
      "Epoch: 168 \t| Train loss: 0.245 \t| Train acc: 0.59 \t| Test acc: 0.6\n",
      "Epoch: 169 \t| Train loss: 0.24 \t| Train acc: 0.6 \t| Test acc: 0.6\n",
      "Epoch: 170 \t| Train loss: 0.25 \t| Train acc: 0.6 \t| Test acc: 0.6\n",
      "Epoch: 171 \t| Train loss: 0.241 \t| Train acc: 0.61 \t| Test acc: 0.6\n",
      "Epoch: 172 \t| Train loss: 0.243 \t| Train acc: 0.6 \t| Test acc: 0.6\n",
      "Epoch: 173 \t| Train loss: 0.245 \t| Train acc: 0.6 \t| Test acc: 0.6\n",
      "Epoch: 174 \t| Train loss: 0.247 \t| Train acc: 0.6 \t| Test acc: 0.6\n",
      "Epoch: 175 \t| Train loss: 0.238 \t| Train acc: 0.59 \t| Test acc: 0.6\n",
      "Epoch: 176 \t| Train loss: 0.253 \t| Train acc: 0.59 \t| Test acc: 0.6\n",
      "Epoch: 177 \t| Train loss: 0.243 \t| Train acc: 0.6 \t| Test acc: 0.6\n",
      "Epoch: 178 \t| Train loss: 0.243 \t| Train acc: 0.6 \t| Test acc: 0.6\n",
      "Epoch: 179 \t| Train loss: 0.24 \t| Train acc: 0.6 \t| Test acc: 0.6\n",
      "Epoch: 180 \t| Train loss: 0.248 \t| Train acc: 0.61 \t| Test acc: 0.6\n",
      "Epoch: 181 \t| Train loss: 0.244 \t| Train acc: 0.6 \t| Test acc: 0.6\n",
      "Epoch: 182 \t| Train loss: 0.246 \t| Train acc: 0.6 \t| Test acc: 0.6\n",
      "Epoch: 183 \t| Train loss: 0.247 \t| Train acc: 0.58 \t| Test acc: 0.6\n",
      "Epoch: 184 \t| Train loss: 0.249 \t| Train acc: 0.59 \t| Test acc: 0.6\n",
      "Epoch: 185 \t| Train loss: 0.242 \t| Train acc: 0.61 \t| Test acc: 0.6\n",
      "Epoch: 186 \t| Train loss: 0.24 \t| Train acc: 0.6 \t| Test acc: 0.6\n",
      "Epoch: 187 \t| Train loss: 0.245 \t| Train acc: 0.6 \t| Test acc: 0.6\n",
      "Epoch: 188 \t| Train loss: 0.242 \t| Train acc: 0.61 \t| Test acc: 0.6\n",
      "Epoch: 189 \t| Train loss: 0.246 \t| Train acc: 0.59 \t| Test acc: 0.6\n",
      "Epoch: 190 \t| Train loss: 0.244 \t| Train acc: 0.6 \t| Test acc: 0.6\n",
      "Epoch: 191 \t| Train loss: 0.244 \t| Train acc: 0.6 \t| Test acc: 0.6\n",
      "Epoch: 192 \t| Train loss: 0.24 \t| Train acc: 0.59 \t| Test acc: 0.6\n",
      "Epoch: 193 \t| Train loss: 0.246 \t| Train acc: 0.6 \t| Test acc: 0.6\n",
      "Epoch: 194 \t| Train loss: 0.243 \t| Train acc: 0.6 \t| Test acc: 0.6\n",
      "Epoch: 195 \t| Train loss: 0.241 \t| Train acc: 0.6 \t| Test acc: 0.6\n",
      "Epoch: 196 \t| Train loss: 0.242 \t| Train acc: 0.6 \t| Test acc: 0.6\n",
      "Epoch: 197 \t| Train loss: 0.241 \t| Train acc: 0.6 \t| Test acc: 0.6\n",
      "Epoch: 198 \t| Train loss: 0.243 \t| Train acc: 0.6 \t| Test acc: 0.6\n",
      "Epoch: 199 \t| Train loss: 0.242 \t| Train acc: 0.6 \t| Test acc: 0.6\n"
     ]
    }
   ],
   "source": [
    "train_losses_mlp = []\n",
    "#test_losses  = []\n",
    "train_accs_mlp = []\n",
    "test_accs_mlp  = []\n",
    "\n",
    "for epoch in range(200):\n",
    "    for features, labels in data_loader:\n",
    "        train_preds_mlp = mlp_model(features)\n",
    "        class_weights_batch = class_weights_original[labels.long()]\n",
    "        loss_function = nn.BCELoss(weight=class_weights_batch)\n",
    "\n",
    "        train_loss_mlp  = loss_function(train_preds_mlp, labels)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            test_preds_mlp = mlp_model(X_validation_tensor)\n",
    "            #test_loss  = loss_function(test_preds, y_test)\n",
    "\n",
    "        train_acc = calculate_accuracy(train_preds_mlp, labels)\n",
    "        test_acc  = calculate_accuracy(test_preds_mlp, y_validation_tensor)\n",
    "\n",
    "        optimizer_mlp.zero_grad()\n",
    "        train_loss_mlp.backward()\n",
    "\n",
    "        optimizer_mlp.step()\n",
    "\n",
    "        train_losses_mlp.append(train_loss_mlp.item())\n",
    "        #test_losses.append(test_loss.item())\n",
    "        train_accs_mlp.append(train_acc.item())\n",
    "        test_accs_mlp.append(test_acc.item())\n",
    "\n",
    "    #if epoch%100==0:\n",
    "    print(f'Epoch: {epoch} \\t|' \\\n",
    "          f' Train loss: {np.round(train_loss_mlp.item(),3)} \\t|' \\\n",
    "              #f' Test loss: {np.round(test_loss.item(),3)} \\t|' \\\n",
    "          f' Train acc: {np.round(train_acc.item(),2)} \\t|' \\\n",
    "          f' Test acc: {np.round(test_acc.item(),2)}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-22T16:11:59.562941500Z",
     "start_time": "2024-06-22T16:09:47.355931Z"
    }
   },
   "id": "718bb73160f9cee1"
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Result:\n",
      "================================================\n",
      "Accuracy Score: 60.18%\n",
      "_______________________________________________\n",
      "CLASSIFICATION REPORT:\n",
      "                   0.0         1.0  accuracy    macro avg  weighted avg\n",
      "precision     0.819541    0.307081  0.601781     0.563311      0.699490\n",
      "recall        0.615477    0.557012  0.601781     0.586244      0.601781\n",
      "f1-score      0.703000    0.395901  0.601781     0.549451      0.631057\n",
      "support    2494.000000  763.000000  0.601781  3257.000000   3257.000000\n",
      "_______________________________________________\n",
      "Confusion Matrix: \n",
      " [[1535  959]\n",
      " [ 338  425]]\n"
     ]
    }
   ],
   "source": [
    "evaluate_nn(y_validation_tensor.clone().detach(), test_preds_mlp.clone().detach().round(), train=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-22T16:12:25.901127700Z",
     "start_time": "2024-06-22T16:12:25.686840100Z"
    }
   },
   "id": "996699d17abfca1c"
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Result:\n",
      "================================================\n",
      "Accuracy Score: 59.19%\n",
      "_______________________________________________\n",
      "CLASSIFICATION REPORT:\n",
      "                   0.0         1.0  accuracy    macro avg  weighted avg\n",
      "precision     0.799363    0.317416  0.591898     0.558389      0.678439\n",
      "recall        0.607748    0.544578  0.591898     0.576163      0.591898\n",
      "f1-score      0.690509    0.401065  0.591898     0.545787      0.617885\n",
      "support    2478.000000  830.000000  0.591898  3308.000000   3308.000000\n",
      "_______________________________________________\n",
      "Confusion Matrix: \n",
      " [[1506  972]\n",
      " [ 378  452]]\n"
     ]
    }
   ],
   "source": [
    "test_preds_mlp = mlp_model(X_test_tensor)\n",
    "evaluate_nn(y_test_tensor.clone().detach(), test_preds_mlp.clone().detach().round(), train=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-22T16:12:26.945076Z",
     "start_time": "2024-06-22T16:12:26.775296400Z"
    }
   },
   "id": "b4140036003b14e6"
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "outputs": [
    {
     "data": {
      "text/plain": "==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nMLP                                      [23034, 1]                --\n├─Sequential: 1-1                        [23034, 1]                --\n│    └─Linear: 2-1                       [23034, 3]                96\n│    └─ReLU: 2-2                         [23034, 3]                --\n│    └─Linear: 2-3                       [23034, 1]                4\n│    └─Sigmoid: 2-4                      [23034, 1]                --\n==========================================================================================\nTotal params: 100\nTrainable params: 100\nNon-trainable params: 0\nTotal mult-adds (Units.MEGABYTES): 2.30\n==========================================================================================\nInput size (MB): 2.86\nForward/backward pass size (MB): 0.74\nParams size (MB): 0.00\nEstimated Total Size (MB): 3.59\n=========================================================================================="
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlpsl_model = MLP(num_features=n_features)\n",
    "summary(mlpsl_model, input_size=X_train.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-22T16:43:48.980766500Z",
     "start_time": "2024-06-22T16:43:48.768929700Z"
    }
   },
   "id": "fc49c948f7d3404c"
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "outputs": [],
   "source": [
    "optimizer_mlpsl = optim.Adam(mlpsl_model.parameters())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-22T16:43:49.823636700Z",
     "start_time": "2024-06-22T16:43:49.802390800Z"
    }
   },
   "id": "e603715e2d52a06a"
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \t| Train loss: 0.225 \t| Train acc: 0.57 \t| Test acc: 0.56\n",
      "Epoch: 1 \t| Train loss: 0.221 \t| Train acc: 0.55 \t| Test acc: 0.55\n",
      "Epoch: 2 \t| Train loss: 0.22 \t| Train acc: 0.53 \t| Test acc: 0.54\n",
      "Epoch: 3 \t| Train loss: 0.225 \t| Train acc: 0.51 \t| Test acc: 0.53\n",
      "Epoch: 4 \t| Train loss: 0.223 \t| Train acc: 0.5 \t| Test acc: 0.51\n",
      "Epoch: 5 \t| Train loss: 0.223 \t| Train acc: 0.49 \t| Test acc: 0.5\n",
      "Epoch: 6 \t| Train loss: 0.219 \t| Train acc: 0.49 \t| Test acc: 0.49\n",
      "Epoch: 7 \t| Train loss: 0.223 \t| Train acc: 0.48 \t| Test acc: 0.49\n",
      "Epoch: 8 \t| Train loss: 0.222 \t| Train acc: 0.48 \t| Test acc: 0.48\n",
      "Epoch: 9 \t| Train loss: 0.221 \t| Train acc: 0.51 \t| Test acc: 0.48\n",
      "Epoch: 10 \t| Train loss: 0.22 \t| Train acc: 0.5 \t| Test acc: 0.48\n",
      "Epoch: 11 \t| Train loss: 0.224 \t| Train acc: 0.5 \t| Test acc: 0.49\n",
      "Epoch: 12 \t| Train loss: 0.221 \t| Train acc: 0.5 \t| Test acc: 0.49\n",
      "Epoch: 13 \t| Train loss: 0.22 \t| Train acc: 0.49 \t| Test acc: 0.5\n",
      "Epoch: 14 \t| Train loss: 0.219 \t| Train acc: 0.5 \t| Test acc: 0.5\n",
      "Epoch: 15 \t| Train loss: 0.216 \t| Train acc: 0.51 \t| Test acc: 0.5\n",
      "Epoch: 16 \t| Train loss: 0.223 \t| Train acc: 0.53 \t| Test acc: 0.51\n",
      "Epoch: 17 \t| Train loss: 0.219 \t| Train acc: 0.51 \t| Test acc: 0.51\n",
      "Epoch: 18 \t| Train loss: 0.219 \t| Train acc: 0.51 \t| Test acc: 0.52\n",
      "Epoch: 19 \t| Train loss: 0.217 \t| Train acc: 0.54 \t| Test acc: 0.52\n",
      "Epoch: 20 \t| Train loss: 0.216 \t| Train acc: 0.51 \t| Test acc: 0.52\n",
      "Epoch: 21 \t| Train loss: 0.222 \t| Train acc: 0.54 \t| Test acc: 0.53\n",
      "Epoch: 22 \t| Train loss: 0.224 \t| Train acc: 0.53 \t| Test acc: 0.53\n",
      "Epoch: 23 \t| Train loss: 0.217 \t| Train acc: 0.52 \t| Test acc: 0.54\n",
      "Epoch: 24 \t| Train loss: 0.222 \t| Train acc: 0.51 \t| Test acc: 0.54\n",
      "Epoch: 25 \t| Train loss: 0.22 \t| Train acc: 0.54 \t| Test acc: 0.54\n",
      "Epoch: 26 \t| Train loss: 0.217 \t| Train acc: 0.54 \t| Test acc: 0.54\n",
      "Epoch: 27 \t| Train loss: 0.219 \t| Train acc: 0.55 \t| Test acc: 0.55\n",
      "Epoch: 28 \t| Train loss: 0.216 \t| Train acc: 0.55 \t| Test acc: 0.55\n",
      "Epoch: 29 \t| Train loss: 0.216 \t| Train acc: 0.53 \t| Test acc: 0.55\n",
      "Epoch: 30 \t| Train loss: 0.221 \t| Train acc: 0.55 \t| Test acc: 0.55\n",
      "Epoch: 31 \t| Train loss: 0.219 \t| Train acc: 0.54 \t| Test acc: 0.55\n",
      "Epoch: 32 \t| Train loss: 0.221 \t| Train acc: 0.53 \t| Test acc: 0.55\n",
      "Epoch: 33 \t| Train loss: 0.215 \t| Train acc: 0.54 \t| Test acc: 0.56\n",
      "Epoch: 34 \t| Train loss: 0.222 \t| Train acc: 0.55 \t| Test acc: 0.56\n",
      "Epoch: 35 \t| Train loss: 0.219 \t| Train acc: 0.54 \t| Test acc: 0.56\n",
      "Epoch: 36 \t| Train loss: 0.215 \t| Train acc: 0.54 \t| Test acc: 0.56\n",
      "Epoch: 37 \t| Train loss: 0.211 \t| Train acc: 0.56 \t| Test acc: 0.56\n",
      "Epoch: 38 \t| Train loss: 0.215 \t| Train acc: 0.55 \t| Test acc: 0.56\n",
      "Epoch: 39 \t| Train loss: 0.214 \t| Train acc: 0.56 \t| Test acc: 0.57\n",
      "Epoch: 40 \t| Train loss: 0.217 \t| Train acc: 0.57 \t| Test acc: 0.57\n",
      "Epoch: 41 \t| Train loss: 0.216 \t| Train acc: 0.57 \t| Test acc: 0.57\n",
      "Epoch: 42 \t| Train loss: 0.22 \t| Train acc: 0.56 \t| Test acc: 0.57\n",
      "Epoch: 43 \t| Train loss: 0.214 \t| Train acc: 0.57 \t| Test acc: 0.57\n",
      "Epoch: 44 \t| Train loss: 0.219 \t| Train acc: 0.55 \t| Test acc: 0.57\n",
      "Epoch: 45 \t| Train loss: 0.218 \t| Train acc: 0.57 \t| Test acc: 0.58\n",
      "Epoch: 46 \t| Train loss: 0.215 \t| Train acc: 0.57 \t| Test acc: 0.58\n",
      "Epoch: 47 \t| Train loss: 0.217 \t| Train acc: 0.56 \t| Test acc: 0.58\n",
      "Epoch: 48 \t| Train loss: 0.212 \t| Train acc: 0.58 \t| Test acc: 0.58\n",
      "Epoch: 49 \t| Train loss: 0.217 \t| Train acc: 0.57 \t| Test acc: 0.58\n",
      "Epoch: 50 \t| Train loss: 0.217 \t| Train acc: 0.58 \t| Test acc: 0.58\n",
      "Epoch: 51 \t| Train loss: 0.217 \t| Train acc: 0.56 \t| Test acc: 0.58\n",
      "Epoch: 52 \t| Train loss: 0.219 \t| Train acc: 0.57 \t| Test acc: 0.59\n",
      "Epoch: 53 \t| Train loss: 0.217 \t| Train acc: 0.57 \t| Test acc: 0.59\n",
      "Epoch: 54 \t| Train loss: 0.214 \t| Train acc: 0.57 \t| Test acc: 0.59\n",
      "Epoch: 55 \t| Train loss: 0.216 \t| Train acc: 0.58 \t| Test acc: 0.59\n",
      "Epoch: 56 \t| Train loss: 0.218 \t| Train acc: 0.56 \t| Test acc: 0.59\n",
      "Epoch: 57 \t| Train loss: 0.214 \t| Train acc: 0.57 \t| Test acc: 0.59\n",
      "Epoch: 58 \t| Train loss: 0.21 \t| Train acc: 0.58 \t| Test acc: 0.59\n",
      "Epoch: 59 \t| Train loss: 0.22 \t| Train acc: 0.57 \t| Test acc: 0.59\n",
      "Epoch: 60 \t| Train loss: 0.215 \t| Train acc: 0.58 \t| Test acc: 0.59\n",
      "Epoch: 61 \t| Train loss: 0.214 \t| Train acc: 0.58 \t| Test acc: 0.59\n",
      "Epoch: 62 \t| Train loss: 0.214 \t| Train acc: 0.57 \t| Test acc: 0.59\n",
      "Epoch: 63 \t| Train loss: 0.21 \t| Train acc: 0.59 \t| Test acc: 0.59\n",
      "Epoch: 64 \t| Train loss: 0.214 \t| Train acc: 0.59 \t| Test acc: 0.59\n",
      "Epoch: 65 \t| Train loss: 0.213 \t| Train acc: 0.58 \t| Test acc: 0.59\n",
      "Epoch: 66 \t| Train loss: 0.215 \t| Train acc: 0.58 \t| Test acc: 0.59\n",
      "Epoch: 67 \t| Train loss: 0.21 \t| Train acc: 0.59 \t| Test acc: 0.59\n",
      "Epoch: 68 \t| Train loss: 0.215 \t| Train acc: 0.59 \t| Test acc: 0.59\n",
      "Epoch: 69 \t| Train loss: 0.218 \t| Train acc: 0.58 \t| Test acc: 0.59\n",
      "Epoch: 70 \t| Train loss: 0.214 \t| Train acc: 0.59 \t| Test acc: 0.59\n",
      "Epoch: 71 \t| Train loss: 0.215 \t| Train acc: 0.58 \t| Test acc: 0.59\n",
      "Epoch: 72 \t| Train loss: 0.216 \t| Train acc: 0.59 \t| Test acc: 0.59\n",
      "Epoch: 73 \t| Train loss: 0.209 \t| Train acc: 0.59 \t| Test acc: 0.59\n",
      "Epoch: 74 \t| Train loss: 0.211 \t| Train acc: 0.59 \t| Test acc: 0.59\n",
      "Epoch: 75 \t| Train loss: 0.212 \t| Train acc: 0.58 \t| Test acc: 0.59\n",
      "Epoch: 76 \t| Train loss: 0.213 \t| Train acc: 0.59 \t| Test acc: 0.59\n",
      "Epoch: 77 \t| Train loss: 0.211 \t| Train acc: 0.57 \t| Test acc: 0.59\n",
      "Epoch: 78 \t| Train loss: 0.213 \t| Train acc: 0.6 \t| Test acc: 0.59\n",
      "Epoch: 79 \t| Train loss: 0.213 \t| Train acc: 0.58 \t| Test acc: 0.59\n",
      "Epoch: 80 \t| Train loss: 0.214 \t| Train acc: 0.58 \t| Test acc: 0.59\n",
      "Epoch: 81 \t| Train loss: 0.221 \t| Train acc: 0.59 \t| Test acc: 0.59\n",
      "Epoch: 82 \t| Train loss: 0.215 \t| Train acc: 0.57 \t| Test acc: 0.59\n",
      "Epoch: 83 \t| Train loss: 0.213 \t| Train acc: 0.58 \t| Test acc: 0.59\n",
      "Epoch: 84 \t| Train loss: 0.216 \t| Train acc: 0.59 \t| Test acc: 0.59\n",
      "Epoch: 85 \t| Train loss: 0.215 \t| Train acc: 0.59 \t| Test acc: 0.59\n",
      "Epoch: 86 \t| Train loss: 0.212 \t| Train acc: 0.59 \t| Test acc: 0.59\n",
      "Epoch: 87 \t| Train loss: 0.214 \t| Train acc: 0.59 \t| Test acc: 0.59\n",
      "Epoch: 88 \t| Train loss: 0.216 \t| Train acc: 0.59 \t| Test acc: 0.59\n",
      "Epoch: 89 \t| Train loss: 0.215 \t| Train acc: 0.59 \t| Test acc: 0.59\n",
      "Epoch: 90 \t| Train loss: 0.218 \t| Train acc: 0.59 \t| Test acc: 0.59\n",
      "Epoch: 91 \t| Train loss: 0.211 \t| Train acc: 0.59 \t| Test acc: 0.59\n",
      "Epoch: 92 \t| Train loss: 0.215 \t| Train acc: 0.58 \t| Test acc: 0.59\n",
      "Epoch: 93 \t| Train loss: 0.216 \t| Train acc: 0.59 \t| Test acc: 0.59\n",
      "Epoch: 94 \t| Train loss: 0.211 \t| Train acc: 0.59 \t| Test acc: 0.59\n",
      "Epoch: 95 \t| Train loss: 0.213 \t| Train acc: 0.59 \t| Test acc: 0.59\n",
      "Epoch: 96 \t| Train loss: 0.212 \t| Train acc: 0.6 \t| Test acc: 0.59\n",
      "Epoch: 97 \t| Train loss: 0.219 \t| Train acc: 0.59 \t| Test acc: 0.6\n",
      "Epoch: 98 \t| Train loss: 0.213 \t| Train acc: 0.59 \t| Test acc: 0.6\n",
      "Epoch: 99 \t| Train loss: 0.212 \t| Train acc: 0.58 \t| Test acc: 0.6\n",
      "Epoch: 100 \t| Train loss: 0.214 \t| Train acc: 0.59 \t| Test acc: 0.6\n",
      "Epoch: 101 \t| Train loss: 0.216 \t| Train acc: 0.58 \t| Test acc: 0.59\n",
      "Epoch: 102 \t| Train loss: 0.212 \t| Train acc: 0.59 \t| Test acc: 0.59\n",
      "Epoch: 103 \t| Train loss: 0.213 \t| Train acc: 0.6 \t| Test acc: 0.59\n",
      "Epoch: 104 \t| Train loss: 0.21 \t| Train acc: 0.6 \t| Test acc: 0.59\n",
      "Epoch: 105 \t| Train loss: 0.21 \t| Train acc: 0.59 \t| Test acc: 0.59\n",
      "Epoch: 106 \t| Train loss: 0.212 \t| Train acc: 0.59 \t| Test acc: 0.59\n",
      "Epoch: 107 \t| Train loss: 0.213 \t| Train acc: 0.59 \t| Test acc: 0.59\n",
      "Epoch: 108 \t| Train loss: 0.214 \t| Train acc: 0.58 \t| Test acc: 0.6\n",
      "Epoch: 109 \t| Train loss: 0.216 \t| Train acc: 0.58 \t| Test acc: 0.6\n",
      "Epoch: 110 \t| Train loss: 0.215 \t| Train acc: 0.58 \t| Test acc: 0.6\n",
      "Epoch: 111 \t| Train loss: 0.216 \t| Train acc: 0.57 \t| Test acc: 0.59\n",
      "Epoch: 112 \t| Train loss: 0.213 \t| Train acc: 0.58 \t| Test acc: 0.59\n",
      "Epoch: 113 \t| Train loss: 0.215 \t| Train acc: 0.58 \t| Test acc: 0.59\n",
      "Epoch: 114 \t| Train loss: 0.213 \t| Train acc: 0.59 \t| Test acc: 0.6\n",
      "Epoch: 115 \t| Train loss: 0.215 \t| Train acc: 0.57 \t| Test acc: 0.6\n",
      "Epoch: 116 \t| Train loss: 0.214 \t| Train acc: 0.6 \t| Test acc: 0.59\n",
      "Epoch: 117 \t| Train loss: 0.213 \t| Train acc: 0.59 \t| Test acc: 0.59\n",
      "Epoch: 118 \t| Train loss: 0.216 \t| Train acc: 0.58 \t| Test acc: 0.59\n",
      "Epoch: 119 \t| Train loss: 0.212 \t| Train acc: 0.59 \t| Test acc: 0.59\n",
      "Epoch: 120 \t| Train loss: 0.217 \t| Train acc: 0.59 \t| Test acc: 0.59\n",
      "Epoch: 121 \t| Train loss: 0.216 \t| Train acc: 0.58 \t| Test acc: 0.59\n",
      "Epoch: 122 \t| Train loss: 0.214 \t| Train acc: 0.59 \t| Test acc: 0.59\n",
      "Epoch: 123 \t| Train loss: 0.215 \t| Train acc: 0.58 \t| Test acc: 0.59\n",
      "Epoch: 124 \t| Train loss: 0.21 \t| Train acc: 0.58 \t| Test acc: 0.59\n",
      "Epoch: 125 \t| Train loss: 0.215 \t| Train acc: 0.59 \t| Test acc: 0.59\n",
      "Epoch: 126 \t| Train loss: 0.211 \t| Train acc: 0.59 \t| Test acc: 0.59\n",
      "Epoch: 127 \t| Train loss: 0.211 \t| Train acc: 0.58 \t| Test acc: 0.6\n",
      "Epoch: 128 \t| Train loss: 0.211 \t| Train acc: 0.59 \t| Test acc: 0.6\n",
      "Epoch: 129 \t| Train loss: 0.213 \t| Train acc: 0.59 \t| Test acc: 0.59\n",
      "Epoch: 130 \t| Train loss: 0.215 \t| Train acc: 0.57 \t| Test acc: 0.59\n",
      "Epoch: 131 \t| Train loss: 0.215 \t| Train acc: 0.58 \t| Test acc: 0.59\n",
      "Epoch: 132 \t| Train loss: 0.219 \t| Train acc: 0.59 \t| Test acc: 0.6\n",
      "Epoch: 133 \t| Train loss: 0.218 \t| Train acc: 0.56 \t| Test acc: 0.59\n",
      "Epoch: 134 \t| Train loss: 0.21 \t| Train acc: 0.58 \t| Test acc: 0.59\n",
      "Epoch: 135 \t| Train loss: 0.218 \t| Train acc: 0.57 \t| Test acc: 0.59\n",
      "Epoch: 136 \t| Train loss: 0.217 \t| Train acc: 0.59 \t| Test acc: 0.59\n",
      "Epoch: 137 \t| Train loss: 0.213 \t| Train acc: 0.58 \t| Test acc: 0.59\n",
      "Epoch: 138 \t| Train loss: 0.211 \t| Train acc: 0.58 \t| Test acc: 0.6\n",
      "Epoch: 139 \t| Train loss: 0.215 \t| Train acc: 0.57 \t| Test acc: 0.6\n",
      "Epoch: 140 \t| Train loss: 0.213 \t| Train acc: 0.59 \t| Test acc: 0.59\n",
      "Epoch: 141 \t| Train loss: 0.213 \t| Train acc: 0.6 \t| Test acc: 0.6\n",
      "Epoch: 142 \t| Train loss: 0.209 \t| Train acc: 0.59 \t| Test acc: 0.59\n",
      "Epoch: 143 \t| Train loss: 0.21 \t| Train acc: 0.59 \t| Test acc: 0.6\n",
      "Epoch: 144 \t| Train loss: 0.208 \t| Train acc: 0.59 \t| Test acc: 0.59\n",
      "Epoch: 145 \t| Train loss: 0.208 \t| Train acc: 0.6 \t| Test acc: 0.6\n",
      "Epoch: 146 \t| Train loss: 0.211 \t| Train acc: 0.59 \t| Test acc: 0.6\n",
      "Epoch: 147 \t| Train loss: 0.214 \t| Train acc: 0.6 \t| Test acc: 0.6\n",
      "Epoch: 148 \t| Train loss: 0.212 \t| Train acc: 0.57 \t| Test acc: 0.6\n",
      "Epoch: 149 \t| Train loss: 0.21 \t| Train acc: 0.6 \t| Test acc: 0.6\n",
      "Epoch: 150 \t| Train loss: 0.213 \t| Train acc: 0.58 \t| Test acc: 0.6\n",
      "Epoch: 151 \t| Train loss: 0.215 \t| Train acc: 0.58 \t| Test acc: 0.59\n",
      "Epoch: 152 \t| Train loss: 0.211 \t| Train acc: 0.57 \t| Test acc: 0.59\n",
      "Epoch: 153 \t| Train loss: 0.208 \t| Train acc: 0.6 \t| Test acc: 0.59\n",
      "Epoch: 154 \t| Train loss: 0.216 \t| Train acc: 0.57 \t| Test acc: 0.6\n",
      "Epoch: 155 \t| Train loss: 0.211 \t| Train acc: 0.57 \t| Test acc: 0.6\n",
      "Epoch: 156 \t| Train loss: 0.209 \t| Train acc: 0.58 \t| Test acc: 0.59\n",
      "Epoch: 157 \t| Train loss: 0.215 \t| Train acc: 0.57 \t| Test acc: 0.59\n",
      "Epoch: 158 \t| Train loss: 0.215 \t| Train acc: 0.57 \t| Test acc: 0.59\n",
      "Epoch: 159 \t| Train loss: 0.212 \t| Train acc: 0.59 \t| Test acc: 0.59\n",
      "Epoch: 160 \t| Train loss: 0.21 \t| Train acc: 0.58 \t| Test acc: 0.59\n",
      "Epoch: 161 \t| Train loss: 0.211 \t| Train acc: 0.58 \t| Test acc: 0.59\n",
      "Epoch: 162 \t| Train loss: 0.215 \t| Train acc: 0.58 \t| Test acc: 0.6\n",
      "Epoch: 163 \t| Train loss: 0.22 \t| Train acc: 0.59 \t| Test acc: 0.6\n",
      "Epoch: 164 \t| Train loss: 0.213 \t| Train acc: 0.59 \t| Test acc: 0.59\n",
      "Epoch: 165 \t| Train loss: 0.211 \t| Train acc: 0.6 \t| Test acc: 0.59\n",
      "Epoch: 166 \t| Train loss: 0.214 \t| Train acc: 0.57 \t| Test acc: 0.59\n",
      "Epoch: 167 \t| Train loss: 0.21 \t| Train acc: 0.57 \t| Test acc: 0.59\n",
      "Epoch: 168 \t| Train loss: 0.215 \t| Train acc: 0.58 \t| Test acc: 0.59\n",
      "Epoch: 169 \t| Train loss: 0.208 \t| Train acc: 0.58 \t| Test acc: 0.59\n",
      "Epoch: 170 \t| Train loss: 0.214 \t| Train acc: 0.59 \t| Test acc: 0.59\n",
      "Epoch: 171 \t| Train loss: 0.213 \t| Train acc: 0.58 \t| Test acc: 0.59\n",
      "Epoch: 172 \t| Train loss: 0.214 \t| Train acc: 0.58 \t| Test acc: 0.59\n",
      "Epoch: 173 \t| Train loss: 0.215 \t| Train acc: 0.58 \t| Test acc: 0.59\n",
      "Epoch: 174 \t| Train loss: 0.213 \t| Train acc: 0.59 \t| Test acc: 0.59\n",
      "Epoch: 175 \t| Train loss: 0.215 \t| Train acc: 0.61 \t| Test acc: 0.59\n",
      "Epoch: 176 \t| Train loss: 0.213 \t| Train acc: 0.56 \t| Test acc: 0.59\n",
      "Epoch: 177 \t| Train loss: 0.209 \t| Train acc: 0.57 \t| Test acc: 0.59\n",
      "Epoch: 178 \t| Train loss: 0.212 \t| Train acc: 0.58 \t| Test acc: 0.59\n",
      "Epoch: 179 \t| Train loss: 0.212 \t| Train acc: 0.58 \t| Test acc: 0.59\n",
      "Epoch: 180 \t| Train loss: 0.209 \t| Train acc: 0.59 \t| Test acc: 0.59\n",
      "Epoch: 181 \t| Train loss: 0.214 \t| Train acc: 0.57 \t| Test acc: 0.59\n",
      "Epoch: 182 \t| Train loss: 0.213 \t| Train acc: 0.57 \t| Test acc: 0.59\n",
      "Epoch: 183 \t| Train loss: 0.209 \t| Train acc: 0.57 \t| Test acc: 0.59\n",
      "Epoch: 184 \t| Train loss: 0.21 \t| Train acc: 0.57 \t| Test acc: 0.59\n",
      "Epoch: 185 \t| Train loss: 0.213 \t| Train acc: 0.58 \t| Test acc: 0.59\n",
      "Epoch: 186 \t| Train loss: 0.212 \t| Train acc: 0.57 \t| Test acc: 0.59\n",
      "Epoch: 187 \t| Train loss: 0.208 \t| Train acc: 0.58 \t| Test acc: 0.59\n",
      "Epoch: 188 \t| Train loss: 0.217 \t| Train acc: 0.56 \t| Test acc: 0.59\n",
      "Epoch: 189 \t| Train loss: 0.221 \t| Train acc: 0.56 \t| Test acc: 0.59\n",
      "Epoch: 190 \t| Train loss: 0.212 \t| Train acc: 0.58 \t| Test acc: 0.59\n",
      "Epoch: 191 \t| Train loss: 0.217 \t| Train acc: 0.57 \t| Test acc: 0.59\n",
      "Epoch: 192 \t| Train loss: 0.219 \t| Train acc: 0.57 \t| Test acc: 0.59\n",
      "Epoch: 193 \t| Train loss: 0.211 \t| Train acc: 0.58 \t| Test acc: 0.59\n",
      "Epoch: 194 \t| Train loss: 0.215 \t| Train acc: 0.57 \t| Test acc: 0.59\n",
      "Epoch: 195 \t| Train loss: 0.215 \t| Train acc: 0.56 \t| Test acc: 0.59\n",
      "Epoch: 196 \t| Train loss: 0.208 \t| Train acc: 0.58 \t| Test acc: 0.59\n",
      "Epoch: 197 \t| Train loss: 0.209 \t| Train acc: 0.59 \t| Test acc: 0.59\n",
      "Epoch: 198 \t| Train loss: 0.212 \t| Train acc: 0.57 \t| Test acc: 0.59\n",
      "Epoch: 199 \t| Train loss: 0.216 \t| Train acc: 0.57 \t| Test acc: 0.59\n"
     ]
    }
   ],
   "source": [
    "train_losses_mlpsl = []\n",
    "#test_losses  = []\n",
    "train_accs_mlpsl = []\n",
    "test_accs_mlpsl  = []\n",
    "\n",
    "for epoch in range(200):\n",
    "    for features, labels in data_loader:\n",
    "        train_preds_mlpsl = mlpsl_model(features)\n",
    "        class_weights_batch = class_weights_original[labels.long()]\n",
    "        loss_function = nn.BCELoss(weight=class_weights_batch)\n",
    "        credit_score_mlpsl = features[:, credit_score_index].reshape(-1, 1)\n",
    "        short_term_mlpsl = features[:, short_term_index].reshape(-1, 1)\n",
    "        annual_income_mlpsl = features[:, annual_income_index].reshape(-1, 1)\n",
    "        \n",
    "        rule_mlpsl = torch.logical_or(credit_score_mlpsl>3300.5, torch.logical_and(credit_score_mlpsl<=3300.5,torch.logical_and(short_term_mlpsl<=0.5, annual_income_mlpsl<=1405107))).float()\n",
    "        #train_loss_mlp  = loss_function(train_preds_mlp, labels)\n",
    "        train_loss_mlpsl = semantic_loss(train_preds_mlpsl, labels, rule_mlpsl, class_weights_batch, 0.005)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            test_preds_mlpsl = mlpsl_model(X_validation_tensor)\n",
    "            #test_loss  = loss_function(test_preds, y_test)\n",
    "\n",
    "        train_acc = calculate_accuracy(train_preds_mlpsl, labels)\n",
    "        test_acc  = calculate_accuracy(test_preds_mlpsl, y_validation_tensor)\n",
    "\n",
    "        optimizer_mlpsl.zero_grad()\n",
    "        train_loss_mlpsl.backward()\n",
    "\n",
    "        optimizer_mlpsl.step()\n",
    "\n",
    "        train_losses_mlpsl.append(train_loss_mlpsl.item())\n",
    "        #test_losses.append(test_loss.item())\n",
    "        train_accs_mlpsl.append(train_acc.item())\n",
    "        test_accs_mlpsl.append(test_acc.item())\n",
    "\n",
    "    #if epoch%100==0:\n",
    "    print(f'Epoch: {epoch} \\t|' \\\n",
    "          f' Train loss: {np.round(train_loss_mlpsl.item(),3)} \\t|' \\\n",
    "              #f' Test loss: {np.round(test_loss.item(),3)} \\t|' \\\n",
    "          f' Train acc: {np.round(train_acc.item(),2)} \\t|' \\\n",
    "          f' Test acc: {np.round(test_acc.item(),2)}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-22T16:46:11.559682800Z",
     "start_time": "2024-06-22T16:43:54.812148100Z"
    }
   },
   "id": "c5bc6befd8dc27a5"
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Result:\n",
      "================================================\n",
      "Accuracy Score: 58.89%\n",
      "_______________________________________________\n",
      "CLASSIFICATION REPORT:\n",
      "                   0.0         1.0  accuracy    macro avg  weighted avg\n",
      "precision     0.832470    0.310526  0.588885     0.571498      0.710197\n",
      "recall        0.579791    0.618611  0.588885     0.599201      0.588885\n",
      "f1-score      0.683526    0.413491  0.588885     0.548509      0.620267\n",
      "support    2494.000000  763.000000  0.588885  3257.000000   3257.000000\n",
      "_______________________________________________\n",
      "Confusion Matrix: \n",
      " [[1446 1048]\n",
      " [ 291  472]]\n"
     ]
    }
   ],
   "source": [
    "evaluate_nn(y_validation_tensor.clone().detach(), test_preds_mlpsl.clone().detach().round(), train=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-22T16:46:38.535115800Z",
     "start_time": "2024-06-22T16:46:38.160155100Z"
    }
   },
   "id": "73d4f72a1a024935"
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Result:\n",
      "================================================\n",
      "Accuracy Score: 57.53%\n",
      "_______________________________________________\n",
      "CLASSIFICATION REPORT:\n",
      "                   0.0         1.0  accuracy    macro avg  weighted avg\n",
      "precision     0.806047    0.315113  0.575272     0.560580      0.682868\n",
      "recall        0.570218    0.590361  0.575272     0.580290      0.575272\n",
      "f1-score      0.667927    0.410901  0.575272     0.539414      0.603438\n",
      "support    2478.000000  830.000000  0.575272  3308.000000   3308.000000\n",
      "_______________________________________________\n",
      "Confusion Matrix: \n",
      " [[1413 1065]\n",
      " [ 340  490]]\n"
     ]
    }
   ],
   "source": [
    "test_preds_mlpsl = mlpsl_model(X_test_tensor)\n",
    "evaluate_nn(y_test_tensor.clone().detach(), test_preds_mlpsl.clone().detach().round(), train=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-22T16:46:38.888770800Z",
     "start_time": "2024-06-22T16:46:38.472628500Z"
    }
   },
   "id": "b54127431d199cbd"
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "outputs": [
    {
     "data": {
      "text/plain": "1952"
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "credit_score_test = torch.tensor(X_test['Credit Score'].values, dtype=torch.float32).reshape(len(X_test), 1)\n",
    "short_term_test = torch.tensor(X_test['Term_Short Term'].values, dtype=torch.float32).reshape(len(X_test), 1)\n",
    "annual_income_test = torch.tensor(X_test['Annual Income'].values, dtype=torch.float32).reshape(len(X_test), 1)\n",
    "\n",
    "rule_test = torch.logical_or(\n",
    "    torch.logical_and(credit_score_test <= 684.5, annual_income_test <= 2489636.5),\n",
    "    torch.logical_and(credit_score_test > 684.5, annual_income_test <= 1271242.5)\n",
    ")\n",
    "rule_matched = torch.nonzero(rule_test.float(), as_tuple=True)[0].tolist()\n",
    "yespred = torch.nonzero(test_preds_mlpsl.round(), as_tuple=True)[0].tolist()\n",
    "same = [value for value in yespred if value in rule_matched]\n",
    "len(same)\n",
    "len(rule_matched)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-22T16:25:09.367938900Z",
     "start_time": "2024-06-22T16:25:09.091333700Z"
    }
   },
   "id": "231b2a0a0fac0a6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tree-based predictions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cdfa39e3e1f8d00f"
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Result:\n",
      "================================================\n",
      "Accuracy Score: 69.05%\n",
      "_______________________________________________\n",
      "CLASSIFICATION REPORT:\n",
      "                   0.0         1.0  accuracy    macro avg  weighted avg\n",
      "precision     0.790689    0.325250  0.690513     0.557969      0.681653\n",
      "recall        0.810345    0.298820  0.690513     0.554583      0.690513\n",
      "f1-score      0.800396    0.311475  0.690513     0.555936      0.685859\n",
      "support    2494.000000  763.000000  0.690513  3257.000000   3257.000000\n",
      "_______________________________________________\n",
      "Confusion Matrix: \n",
      " [[2021  473]\n",
      " [ 535  228]]\n"
     ]
    }
   ],
   "source": [
    "dt_model = DecisionTreeClassifier(class_weight='balanced')\n",
    "dt_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "predictions_dt = dt_model.predict(X_validation_scaled)\n",
    "\n",
    "class_report = classification_report(y_validation, predictions_dt)\n",
    "evaluate_nn(y_validation_tensor.clone().detach(), predictions_dt.round(), train=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-22T16:26:05.240705800Z",
     "start_time": "2024-06-22T16:26:02.330678300Z"
    }
   },
   "id": "80c220832c16762f"
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Result:\n",
      "================================================\n",
      "Accuracy Score: 69.74%\n",
      "_______________________________________________\n",
      "CLASSIFICATION REPORT:\n",
      "                   0.0         1.0  accuracy    macro avg  weighted avg\n",
      "precision     0.781978    0.375907    0.6974     0.578942      0.680092\n",
      "recall        0.826473    0.312048    0.6974     0.569261      0.697400\n",
      "f1-score      0.803610    0.341014    0.6974     0.572312      0.687541\n",
      "support    2478.000000  830.000000    0.6974  3308.000000   3308.000000\n",
      "_______________________________________________\n",
      "Confusion Matrix: \n",
      " [[2048  430]\n",
      " [ 571  259]]\n"
     ]
    }
   ],
   "source": [
    "test_preds_dt = dt_model.predict(X_test_scaled)\n",
    "evaluate_nn(y_test_tensor.clone().detach(), test_preds_dt.round(), train=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-22T16:26:05.462264900Z",
     "start_time": "2024-06-22T16:26:05.219384900Z"
    }
   },
   "id": "d37ff0047c0f8de6"
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.71      0.75      2494\n",
      "           1       0.31      0.44      0.37       763\n",
      "\n",
      "    accuracy                           0.65      3257\n",
      "   macro avg       0.56      0.57      0.56      3257\n",
      "weighted avg       0.69      0.65      0.66      3257\n",
      "\n",
      "Test Result:\n",
      "================================================\n",
      "Accuracy Score: 64.57%\n",
      "_______________________________________________\n",
      "CLASSIFICATION REPORT:\n",
      "                   0.0         1.0  accuracy    macro avg  weighted avg\n",
      "precision     0.804269    0.314692  0.645686     0.559480      0.689578\n",
      "recall        0.710104    0.435125  0.645686     0.572614      0.645686\n",
      "f1-score      0.754259    0.365237  0.645686     0.559748      0.663125\n",
      "support    2494.000000  763.000000  0.645686  3257.000000   3257.000000\n",
      "_______________________________________________\n",
      "Confusion Matrix: \n",
      " [[1771  723]\n",
      " [ 431  332]]\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(scale_pos_weight=(len(y_train) - sum(y_train)) / sum(y_train))\n",
    "\n",
    "xgb_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "predictions_xgb = xgb_model.predict(X_validation_scaled)\n",
    "\n",
    "class_report_xgb = classification_report(y_validation, predictions_xgb)\n",
    "print(\"XGBoost Classification Report:\\n\", class_report_xgb)\n",
    "evaluate_nn(y_validation_tensor.clone().detach(), predictions_xgb.round(), train=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-22T16:58:17.303217800Z",
     "start_time": "2024-06-22T16:58:14.809175900Z"
    }
   },
   "id": "14ea60a51e676b56"
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Result:\n",
      "================================================\n",
      "Accuracy Score: 65.63%\n",
      "_______________________________________________\n",
      "CLASSIFICATION REPORT:\n",
      "                   0.0         1.0  accuracy    macro avg  weighted avg\n",
      "precision     0.795765    0.352546  0.656288     0.574155      0.684558\n",
      "recall        0.728006    0.442169  0.656288     0.585088      0.656288\n",
      "f1-score      0.760379    0.392304  0.656288     0.576341      0.668027\n",
      "support    2478.000000  830.000000  0.656288  3308.000000   3308.000000\n",
      "_______________________________________________\n",
      "Confusion Matrix: \n",
      " [[1804  674]\n",
      " [ 463  367]]\n"
     ]
    }
   ],
   "source": [
    "test_preds_xgb = xgb_model.predict(X_test_scaled)\n",
    "evaluate_nn(y_test_tensor.clone().detach(), test_preds_xgb.round(), train=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-22T16:58:18.722891100Z",
     "start_time": "2024-06-22T16:58:18.542504300Z"
    }
   },
   "id": "87de10f5c61be50b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Limited data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7d13ee8d32a079de"
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "outputs": [
    {
     "data": {
      "text/plain": "Index(['Current Loan Amount', 'Credit Score', 'Annual Income', 'Monthly Debt',\n       'Years of Credit History', 'Months since last delinquent',\n       'Number of Open Accounts', 'Number of Credit Problems',\n       'Current Credit Balance', 'Maximum Open Credit', 'Bankruptcies',\n       'Tax Liens', 'Term_Short Term', 'Home Ownership_Home Mortgage',\n       'Home Ownership_Own Home', 'Home Ownership_Rent', 'Purpose_Buy House',\n       'Purpose_Buy a Car', 'Purpose_Debt Consolidation',\n       'Purpose_Educational Expenses', 'Purpose_Home Improvements',\n       'Purpose_Medical Bills', 'Purpose_Other', 'Purpose_Take a Trip',\n       'Purpose_major_purchase', 'Purpose_moving', 'Purpose_other',\n       'Purpose_renewable_energy', 'Purpose_small_business',\n       'Purpose_vacation', 'Purpose_wedding'],\n      dtype='object')"
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.columns"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-22T16:26:10.315117600Z",
     "start_time": "2024-06-22T16:26:10.109790Z"
    }
   },
   "id": "b26fb4167f5a951d"
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([115, 1])"
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_alt = X_train[['Current Loan Amount', 'Term_Short Term', 'Credit Score', 'Annual Income', 'Tax Liens', 'Years of Credit History']]\n",
    "X_calib_alt = X_calibration[['Current Loan Amount', 'Term_Short Term', 'Credit Score', 'Annual Income', 'Tax Liens', 'Years of Credit History']]\n",
    "X_train_lim, X_remain, y_train_lim, y_remain = train_test_split(X_train_alt, y_train, test_size=0.995, random_state=42)\n",
    "X_train_lim_scaled = scaler.fit_transform(X_train_lim)\n",
    "X_calibration_lim_scaled = scaler.transform(X_calib_alt)\n",
    "X_validation_alt = X_validation[['Current Loan Amount', 'Term_Short Term', 'Credit Score', 'Annual Income', 'Tax Liens', 'Years of Credit History']]\n",
    "X_validation_lim_scaled = scaler.transform(X_validation_alt)\n",
    "X_validation_tensor_lim = torch.tensor(X_validation_lim_scaled, dtype=torch.float32)\n",
    "X_calibration_tensor_lim = torch.tensor(X_calibration_lim_scaled, dtype=torch.float32)\n",
    "X_train_tensor_lim = torch.tensor(X_train_lim_scaled, dtype=torch.float32)\n",
    "y_train_tensor_lim = torch.tensor(y_train_lim.values, dtype=torch.float32).reshape(len(y_train_lim),1)\n",
    "X_train_tensor_lim.shape\n",
    "y_train_tensor_lim.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-22T16:26:11.808642Z",
     "start_time": "2024-06-22T16:26:11.629908800Z"
    }
   },
   "id": "3a0ad88ea95a8570"
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "outputs": [],
   "source": [
    "X_test_alt = X_test[['Current Loan Amount', 'Term_Short Term', 'Credit Score', 'Annual Income', 'Tax Liens', 'Years of Credit History']]\n",
    "X_test_lim_scaled = scaler.transform(X_test_alt)\n",
    "X_test_tensor_lim = torch.tensor(X_test_lim_scaled, dtype=torch.float32)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-22T16:26:12.756929100Z",
     "start_time": "2024-06-22T16:26:12.581356600Z"
    }
   },
   "id": "2e0ef190f8f371cc"
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "outputs": [],
   "source": [
    "credit_score_index_lim = X_train_lim.columns.get_loc('Credit Score')\n",
    "short_term_index_lim = X_train_lim.columns.get_loc('Term_Short Term')\n",
    "annual_income_index_lim = X_train_lim.columns.get_loc('Annual Income')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-22T16:26:13.546658200Z",
     "start_time": "2024-06-22T16:26:13.502935400Z"
    }
   },
   "id": "31cef1c4a16d0d11"
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [],
   "source": [
    "credit_score_lim = torch.tensor(X_train_lim['Credit Score'].values, dtype=torch.float32).reshape(len(y_train_lim),1)\n",
    "short_term_lim = torch.tensor(X_train_lim['Term_Short Term'].values, dtype=torch.float32).reshape(len(y_train_lim),1)\n",
    "annual_income_lim = torch.tensor(X_train_lim['Annual Income'].values, dtype=torch.float32).reshape(len(y_train_lim),1)\n",
    "\n",
    "#rule=credit_score>3300.5\n",
    "rule_lim = torch.logical_or(torch.logical_and(credit_score_lim<=684.5,  annual_income_lim<=2489636.5), torch.logical_and(credit_score_lim>684.5, annual_income_lim<=1271242.5))\n",
    "#rule_lim = torch.logical_or(credit_score_lim>3300.5, torch.logical_and(credit_score_lim<=3300.5,torch.logical_and(short_term_lim<=0.5, annual_income_lim<=1405107)))\n",
    "rule_lim=rule_lim.float()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-22T16:26:16.203682800Z",
     "start_time": "2024-06-22T16:26:16.108398600Z"
    }
   },
   "id": "e82ba00f6971e02b"
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \t| Train loss: 0.272 \t| Train acc: 0.38 \t| Test acc: 0.4\n",
      "Epoch: 100 \t| Train loss: 0.225 \t| Train acc: 0.64 \t| Test acc: 0.57\n",
      "Epoch: 200 \t| Train loss: 0.224 \t| Train acc: 0.64 \t| Test acc: 0.58\n",
      "Epoch: 300 \t| Train loss: 0.224 \t| Train acc: 0.64 \t| Test acc: 0.58\n",
      "Epoch: 400 \t| Train loss: 0.224 \t| Train acc: 0.64 \t| Test acc: 0.58\n",
      "Epoch: 500 \t| Train loss: 0.224 \t| Train acc: 0.64 \t| Test acc: 0.58\n",
      "Epoch: 600 \t| Train loss: 0.224 \t| Train acc: 0.65 \t| Test acc: 0.58\n",
      "Epoch: 700 \t| Train loss: 0.224 \t| Train acc: 0.64 \t| Test acc: 0.58\n",
      "Epoch: 800 \t| Train loss: 0.224 \t| Train acc: 0.64 \t| Test acc: 0.58\n",
      "Epoch: 900 \t| Train loss: 0.224 \t| Train acc: 0.64 \t| Test acc: 0.58\n"
     ]
    }
   ],
   "source": [
    "small_model_lr = Logistic_Regression(num_features=6)\n",
    "optimizer_small_lr = optim.Adam(small_model_lr.parameters(), lr=LEARNING_RATE)\n",
    "class_weights_lim = class_weights_original[y_train_tensor_lim.long()]\n",
    "loss_function_lim=nn.BCELoss(weight=class_weights_lim)\n",
    "train_losses = []\n",
    "#test_losses  = []\n",
    "train_accs = []\n",
    "test_accs  = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_preds_lim = small_model_lr(X_train_tensor_lim)\n",
    "    #train_loss  = loss_function(train_preds, y_train_tensor)\n",
    "\n",
    "    #print(torch.min(train_preds_lim), torch.max(train_preds_lim))\n",
    "    #train_loss = semantic_loss(train_preds_lim, y_train_tensor_lim, rule_lim, class_weights_lim, 0.03)\n",
    "    train_loss = loss_function_lim(train_preds_lim, y_train_tensor_lim)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        test_preds = small_model_lr(X_validation_tensor_lim)\n",
    "        #test_loss  = loss_function(test_preds, y_test)\n",
    "\n",
    "    train_acc = calculate_accuracy(train_preds_lim, y_train_tensor_lim)\n",
    "    test_acc  = calculate_accuracy(test_preds, y_validation_tensor)\n",
    "\n",
    "    optimizer_small_lr.zero_grad()\n",
    "    train_loss.backward()\n",
    "\n",
    "    optimizer_small_lr.step()\n",
    "\n",
    "    train_losses.append(train_loss.item())\n",
    "    #test_losses.append(test_loss.item())\n",
    "    train_accs.append(train_acc.item())\n",
    "    test_accs.append(test_acc.item())\n",
    "\n",
    "    if epoch%100==0:\n",
    "        print(f'Epoch: {epoch} \\t|' \\\n",
    "              f' Train loss: {np.round(train_loss.item(),3)} \\t|' \\\n",
    "                  #f' Test loss: {np.round(test_loss.item(),3)} \\t|' \\\n",
    "              f' Train acc: {np.round(train_acc.item(),2)} \\t|' \\\n",
    "              f' Test acc: {np.round(test_acc.item(),2)}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-22T16:26:23.109588700Z",
     "start_time": "2024-06-22T16:26:20.407162Z"
    }
   },
   "id": "2f6aca513c3b14f1"
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Result:\n",
      "================================================\n",
      "Accuracy Score: 57.35%\n",
      "_______________________________________________\n",
      "CLASSIFICATION REPORT:\n",
      "                   0.0         1.0  accuracy    macro avg  weighted avg\n",
      "precision     0.777431    0.290253  0.573458     0.533842      0.655195\n",
      "recall        0.603309    0.484337  0.573458     0.543823      0.573458\n",
      "f1-score      0.679391    0.362980  0.573458     0.521185      0.600001\n",
      "support    2478.000000  830.000000  0.573458  3308.000000   3308.000000\n",
      "_______________________________________________\n",
      "Confusion Matrix: \n",
      " [[1495  983]\n",
      " [ 428  402]]\n"
     ]
    }
   ],
   "source": [
    "test_preds_lim_lr = small_model_lr(X_test_tensor_lim)\n",
    "evaluate_nn(y_test_tensor.clone().detach(), test_preds_lim_lr.clone().detach().round(), train=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-22T16:26:26.003049Z",
     "start_time": "2024-06-22T16:26:25.770234400Z"
    }
   },
   "id": "3bae11c1ce2f870a"
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \t| Train loss: -0.079 \t| Train acc: 0.34 \t| Test acc: 0.35\n",
      "Epoch: 100 \t| Train loss: -0.145 \t| Train acc: 0.56 \t| Test acc: 0.47\n",
      "Epoch: 200 \t| Train loss: -0.145 \t| Train acc: 0.55 \t| Test acc: 0.47\n",
      "Epoch: 300 \t| Train loss: -0.145 \t| Train acc: 0.55 \t| Test acc: 0.47\n",
      "Epoch: 400 \t| Train loss: -0.145 \t| Train acc: 0.55 \t| Test acc: 0.47\n",
      "Epoch: 500 \t| Train loss: -0.145 \t| Train acc: 0.55 \t| Test acc: 0.47\n",
      "Epoch: 600 \t| Train loss: -0.145 \t| Train acc: 0.55 \t| Test acc: 0.47\n",
      "Epoch: 700 \t| Train loss: -0.145 \t| Train acc: 0.55 \t| Test acc: 0.47\n",
      "Epoch: 800 \t| Train loss: -0.145 \t| Train acc: 0.55 \t| Test acc: 0.47\n",
      "Epoch: 900 \t| Train loss: -0.145 \t| Train acc: 0.55 \t| Test acc: 0.47\n"
     ]
    }
   ],
   "source": [
    "small_model = Logistic_Regression(num_features=6)\n",
    "optimizer_small = optim.Adam(small_model.parameters(), lr=LEARNING_RATE)\n",
    "class_weights_lim = class_weights_original[y_train_tensor_lim.long()]\n",
    "loss_function_lim=nn.BCELoss(weight=class_weights_lim)\n",
    "train_losses = []\n",
    "#test_losses  = []\n",
    "train_accs = []\n",
    "test_accs  = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_preds_lim = small_model(X_train_tensor_lim)\n",
    "    #train_loss  = loss_function(train_preds, y_train_tensor)\n",
    "\n",
    "    #print(torch.min(train_preds_lim), torch.max(train_preds_lim))\n",
    "    train_loss = semantic_loss(train_preds_lim, y_train_tensor_lim, rule_lim, class_weights_lim, 0.1)\n",
    "    #train_loss = loss_function_lim(train_preds_lim, y_train_tensor_lim)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        test_preds = small_model(X_validation_tensor_lim)\n",
    "        #test_loss  = loss_function(test_preds, y_test)\n",
    "\n",
    "    train_acc = calculate_accuracy(train_preds_lim, y_train_tensor_lim)\n",
    "    test_acc  = calculate_accuracy(test_preds, y_validation_tensor)\n",
    "\n",
    "    optimizer_small.zero_grad()\n",
    "    train_loss.backward()\n",
    "\n",
    "    optimizer_small.step()\n",
    "\n",
    "    train_losses.append(train_loss.item())\n",
    "    #test_losses.append(test_loss.item())\n",
    "    train_accs.append(train_acc.item())\n",
    "    test_accs.append(test_acc.item())\n",
    "\n",
    "    if epoch%100==0:\n",
    "        print(f'Epoch: {epoch} \\t|' \\\n",
    "              f' Train loss: {np.round(train_loss.item(),3)} \\t|' \\\n",
    "                  #f' Test loss: {np.round(test_loss.item(),3)} \\t|' \\\n",
    "              f' Train acc: {np.round(train_acc.item(),2)} \\t|' \\\n",
    "              f' Test acc: {np.round(test_acc.item(),2)}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-22T16:47:53.557683Z",
     "start_time": "2024-06-22T16:47:50.226010500Z"
    }
   },
   "id": "89ad52068c6bce72"
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Result:\n",
      "================================================\n",
      "Accuracy Score: 54.78%\n",
      "_______________________________________________\n",
      "CLASSIFICATION REPORT:\n",
      "                 0.0        1.0  accuracy   macro avg  weighted avg\n",
      "precision   0.875000   0.313433  0.547826    0.594216      0.743154\n",
      "recall      0.477273   0.777778  0.547826    0.627525      0.547826\n",
      "f1-score    0.617647   0.446809  0.547826    0.532228      0.577537\n",
      "support    88.000000  27.000000  0.547826  115.000000    115.000000\n",
      "_______________________________________________\n",
      "Confusion Matrix: \n",
      " [[42 46]\n",
      " [ 6 21]]\n",
      "\n",
      "Test Result:\n",
      "================================================\n",
      "Accuracy Score: 46.98%\n",
      "_______________________________________________\n",
      "CLASSIFICATION REPORT:\n",
      "                   0.0         1.0  accuracy    macro avg  weighted avg\n",
      "precision     0.819317    0.265564  0.469757     0.542441      0.689592\n",
      "recall        0.394547    0.715596  0.469757     0.555072      0.469757\n",
      "f1-score      0.532612    0.387371  0.469757     0.459992      0.498587\n",
      "support    2494.000000  763.000000  0.469757  3257.000000   3257.000000\n",
      "_______________________________________________\n",
      "Confusion Matrix: \n",
      " [[ 984 1510]\n",
      " [ 217  546]]\n"
     ]
    }
   ],
   "source": [
    "evaluate_nn(y_train_tensor_lim.clone().detach(), train_preds_lim.clone().detach().round(), train=True)\n",
    "evaluate_nn(y_validation_tensor.clone().detach(), test_preds.clone().detach().round(), train=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-22T16:47:55.881425100Z",
     "start_time": "2024-06-22T16:47:55.674641800Z"
    }
   },
   "id": "e43be9fd06bc073"
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Result:\n",
      "================================================\n",
      "Accuracy Score: 46.92%\n",
      "_______________________________________________\n",
      "CLASSIFICATION REPORT:\n",
      "                   0.0         1.0  accuracy    macro avg  weighted avg\n",
      "precision     0.789263    0.275243  0.469166     0.532253      0.660292\n",
      "recall        0.397498    0.683133  0.469166     0.540315      0.469166\n",
      "f1-score      0.528717    0.392388  0.469166     0.460552      0.494511\n",
      "support    2478.000000  830.000000  0.469166  3308.000000   3308.000000\n",
      "_______________________________________________\n",
      "Confusion Matrix: \n",
      " [[ 985 1493]\n",
      " [ 263  567]]\n"
     ]
    }
   ],
   "source": [
    "test_preds_lim = small_model(X_test_tensor_lim)\n",
    "evaluate_nn(y_test_tensor.clone().detach(), test_preds_lim.clone().detach().round(), train=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-22T16:47:57.150142600Z",
     "start_time": "2024-06-22T16:47:56.967631800Z"
    }
   },
   "id": "a862e41dd8e102ad"
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "outputs": [
    {
     "data": {
      "text/plain": "1234"
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "credit_score_test = torch.tensor(X_test['Credit Score'].values, dtype=torch.float32).reshape(len(X_test), 1)\n",
    "short_term_test = torch.tensor(X_test['Term_Short Term'].values, dtype=torch.float32).reshape(len(X_test), 1)\n",
    "annual_income_test = torch.tensor(X_test['Annual Income'].values, dtype=torch.float32).reshape(len(X_test), 1)\n",
    "\n",
    "rule_test = torch.logical_or(\n",
    "    torch.logical_and(credit_score_test <= 684.5, annual_income_test <= 2489636.5),\n",
    "    torch.logical_and(credit_score_test > 684.5, annual_income_test <= 1271242.5)\n",
    ")\n",
    "rule_matched = torch.nonzero(rule_test.float(), as_tuple=True)[0].tolist()\n",
    "yespred = torch.nonzero(test_preds_lim.round(), as_tuple=True)[0].tolist()\n",
    "same = [value for value in yespred if value in rule_matched]\n",
    "len(same)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-22T16:28:32.374881Z",
     "start_time": "2024-06-22T16:28:32.222117100Z"
    }
   },
   "id": "a5cdc5fce1c38d53"
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "outputs": [],
   "source": [
    "dataset_lim = LCDataset(X_train_tensor_lim, y_train_tensor_lim)\n",
    "\n",
    "data_loader_lim = DataLoader(dataset_lim, batch_size=300, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-22T16:28:41.089716Z",
     "start_time": "2024-06-22T16:28:40.981773300Z"
    }
   },
   "id": "4c2bb3735e909785"
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "outputs": [
    {
     "data": {
      "text/plain": "==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nMLP                                      [115, 1]                  --\n├─Sequential: 1-1                        [115, 1]                  --\n│    └─Linear: 2-1                       [115, 3]                  21\n│    └─ReLU: 2-2                         [115, 3]                  --\n│    └─Linear: 2-3                       [115, 1]                  4\n│    └─Sigmoid: 2-4                      [115, 1]                  --\n==========================================================================================\nTotal params: 25\nTrainable params: 25\nNon-trainable params: 0\nTotal mult-adds (Units.MEGABYTES): 0.00\n==========================================================================================\nInput size (MB): 0.00\nForward/backward pass size (MB): 0.00\nParams size (MB): 0.00\nEstimated Total Size (MB): 0.01\n=========================================================================================="
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_model_lim = MLP(num_features=6)\n",
    "summary(mlp_model_lim, input_size=X_train_lim.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-22T16:28:41.939753600Z",
     "start_time": "2024-06-22T16:28:41.734595300Z"
    }
   },
   "id": "c2ad199e102a005b"
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "outputs": [],
   "source": [
    "optimizer_mlp_lim = optim.Adam(mlp_model_lim.parameters())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-22T16:28:43.018527600Z",
     "start_time": "2024-06-22T16:28:42.871004800Z"
    }
   },
   "id": "8a5dc2a52e9a07c7"
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \t| Train loss: 0.253 \t| Train acc: 0.23 \t| Test acc: 0.23\n",
      "Epoch: 1 \t| Train loss: 0.253 \t| Train acc: 0.23 \t| Test acc: 0.23\n",
      "Epoch: 2 \t| Train loss: 0.253 \t| Train acc: 0.23 \t| Test acc: 0.23\n",
      "Epoch: 3 \t| Train loss: 0.253 \t| Train acc: 0.23 \t| Test acc: 0.23\n",
      "Epoch: 4 \t| Train loss: 0.253 \t| Train acc: 0.23 \t| Test acc: 0.23\n",
      "Epoch: 5 \t| Train loss: 0.253 \t| Train acc: 0.23 \t| Test acc: 0.23\n",
      "Epoch: 6 \t| Train loss: 0.253 \t| Train acc: 0.23 \t| Test acc: 0.23\n",
      "Epoch: 7 \t| Train loss: 0.253 \t| Train acc: 0.23 \t| Test acc: 0.23\n",
      "Epoch: 8 \t| Train loss: 0.253 \t| Train acc: 0.23 \t| Test acc: 0.23\n",
      "Epoch: 9 \t| Train loss: 0.253 \t| Train acc: 0.23 \t| Test acc: 0.23\n",
      "Epoch: 10 \t| Train loss: 0.253 \t| Train acc: 0.23 \t| Test acc: 0.23\n",
      "Epoch: 11 \t| Train loss: 0.252 \t| Train acc: 0.23 \t| Test acc: 0.23\n",
      "Epoch: 12 \t| Train loss: 0.252 \t| Train acc: 0.23 \t| Test acc: 0.23\n",
      "Epoch: 13 \t| Train loss: 0.252 \t| Train acc: 0.23 \t| Test acc: 0.23\n",
      "Epoch: 14 \t| Train loss: 0.252 \t| Train acc: 0.23 \t| Test acc: 0.23\n",
      "Epoch: 15 \t| Train loss: 0.252 \t| Train acc: 0.23 \t| Test acc: 0.23\n",
      "Epoch: 16 \t| Train loss: 0.252 \t| Train acc: 0.23 \t| Test acc: 0.23\n",
      "Epoch: 17 \t| Train loss: 0.252 \t| Train acc: 0.23 \t| Test acc: 0.23\n",
      "Epoch: 18 \t| Train loss: 0.252 \t| Train acc: 0.23 \t| Test acc: 0.23\n",
      "Epoch: 19 \t| Train loss: 0.252 \t| Train acc: 0.23 \t| Test acc: 0.23\n",
      "Epoch: 20 \t| Train loss: 0.252 \t| Train acc: 0.23 \t| Test acc: 0.23\n",
      "Epoch: 21 \t| Train loss: 0.252 \t| Train acc: 0.23 \t| Test acc: 0.23\n",
      "Epoch: 22 \t| Train loss: 0.252 \t| Train acc: 0.23 \t| Test acc: 0.23\n",
      "Epoch: 23 \t| Train loss: 0.252 \t| Train acc: 0.23 \t| Test acc: 0.23\n",
      "Epoch: 24 \t| Train loss: 0.252 \t| Train acc: 0.23 \t| Test acc: 0.23\n",
      "Epoch: 25 \t| Train loss: 0.252 \t| Train acc: 0.23 \t| Test acc: 0.23\n",
      "Epoch: 26 \t| Train loss: 0.252 \t| Train acc: 0.23 \t| Test acc: 0.23\n",
      "Epoch: 27 \t| Train loss: 0.252 \t| Train acc: 0.23 \t| Test acc: 0.23\n",
      "Epoch: 28 \t| Train loss: 0.252 \t| Train acc: 0.23 \t| Test acc: 0.23\n",
      "Epoch: 29 \t| Train loss: 0.252 \t| Train acc: 0.23 \t| Test acc: 0.23\n",
      "Epoch: 30 \t| Train loss: 0.252 \t| Train acc: 0.23 \t| Test acc: 0.23\n",
      "Epoch: 31 \t| Train loss: 0.251 \t| Train acc: 0.23 \t| Test acc: 0.23\n",
      "Epoch: 32 \t| Train loss: 0.251 \t| Train acc: 0.23 \t| Test acc: 0.23\n",
      "Epoch: 33 \t| Train loss: 0.251 \t| Train acc: 0.23 \t| Test acc: 0.23\n",
      "Epoch: 34 \t| Train loss: 0.251 \t| Train acc: 0.23 \t| Test acc: 0.23\n",
      "Epoch: 35 \t| Train loss: 0.251 \t| Train acc: 0.23 \t| Test acc: 0.23\n",
      "Epoch: 36 \t| Train loss: 0.251 \t| Train acc: 0.23 \t| Test acc: 0.23\n",
      "Epoch: 37 \t| Train loss: 0.251 \t| Train acc: 0.23 \t| Test acc: 0.23\n",
      "Epoch: 38 \t| Train loss: 0.251 \t| Train acc: 0.23 \t| Test acc: 0.23\n",
      "Epoch: 39 \t| Train loss: 0.251 \t| Train acc: 0.23 \t| Test acc: 0.23\n",
      "Epoch: 40 \t| Train loss: 0.251 \t| Train acc: 0.23 \t| Test acc: 0.23\n",
      "Epoch: 41 \t| Train loss: 0.251 \t| Train acc: 0.23 \t| Test acc: 0.23\n",
      "Epoch: 42 \t| Train loss: 0.251 \t| Train acc: 0.23 \t| Test acc: 0.23\n",
      "Epoch: 43 \t| Train loss: 0.251 \t| Train acc: 0.23 \t| Test acc: 0.23\n",
      "Epoch: 44 \t| Train loss: 0.251 \t| Train acc: 0.23 \t| Test acc: 0.23\n",
      "Epoch: 45 \t| Train loss: 0.251 \t| Train acc: 0.23 \t| Test acc: 0.23\n",
      "Epoch: 46 \t| Train loss: 0.251 \t| Train acc: 0.23 \t| Test acc: 0.23\n",
      "Epoch: 47 \t| Train loss: 0.251 \t| Train acc: 0.23 \t| Test acc: 0.23\n",
      "Epoch: 48 \t| Train loss: 0.251 \t| Train acc: 0.23 \t| Test acc: 0.23\n",
      "Epoch: 49 \t| Train loss: 0.25 \t| Train acc: 0.23 \t| Test acc: 0.23\n",
      "Epoch: 50 \t| Train loss: 0.25 \t| Train acc: 0.23 \t| Test acc: 0.23\n",
      "Epoch: 51 \t| Train loss: 0.25 \t| Train acc: 0.23 \t| Test acc: 0.23\n",
      "Epoch: 52 \t| Train loss: 0.25 \t| Train acc: 0.23 \t| Test acc: 0.23\n",
      "Epoch: 53 \t| Train loss: 0.25 \t| Train acc: 0.23 \t| Test acc: 0.23\n",
      "Epoch: 54 \t| Train loss: 0.25 \t| Train acc: 0.23 \t| Test acc: 0.23\n",
      "Epoch: 55 \t| Train loss: 0.25 \t| Train acc: 0.23 \t| Test acc: 0.23\n",
      "Epoch: 56 \t| Train loss: 0.25 \t| Train acc: 0.23 \t| Test acc: 0.23\n",
      "Epoch: 57 \t| Train loss: 0.25 \t| Train acc: 0.23 \t| Test acc: 0.23\n",
      "Epoch: 58 \t| Train loss: 0.25 \t| Train acc: 0.23 \t| Test acc: 0.23\n",
      "Epoch: 59 \t| Train loss: 0.25 \t| Train acc: 0.23 \t| Test acc: 0.24\n",
      "Epoch: 60 \t| Train loss: 0.25 \t| Train acc: 0.23 \t| Test acc: 0.24\n",
      "Epoch: 61 \t| Train loss: 0.25 \t| Train acc: 0.23 \t| Test acc: 0.24\n",
      "Epoch: 62 \t| Train loss: 0.25 \t| Train acc: 0.23 \t| Test acc: 0.24\n",
      "Epoch: 63 \t| Train loss: 0.25 \t| Train acc: 0.23 \t| Test acc: 0.24\n",
      "Epoch: 64 \t| Train loss: 0.249 \t| Train acc: 0.23 \t| Test acc: 0.24\n",
      "Epoch: 65 \t| Train loss: 0.249 \t| Train acc: 0.23 \t| Test acc: 0.24\n",
      "Epoch: 66 \t| Train loss: 0.249 \t| Train acc: 0.23 \t| Test acc: 0.24\n",
      "Epoch: 67 \t| Train loss: 0.249 \t| Train acc: 0.23 \t| Test acc: 0.24\n",
      "Epoch: 68 \t| Train loss: 0.249 \t| Train acc: 0.23 \t| Test acc: 0.24\n",
      "Epoch: 69 \t| Train loss: 0.249 \t| Train acc: 0.23 \t| Test acc: 0.24\n",
      "Epoch: 70 \t| Train loss: 0.249 \t| Train acc: 0.23 \t| Test acc: 0.24\n",
      "Epoch: 71 \t| Train loss: 0.249 \t| Train acc: 0.23 \t| Test acc: 0.24\n",
      "Epoch: 72 \t| Train loss: 0.249 \t| Train acc: 0.23 \t| Test acc: 0.24\n",
      "Epoch: 73 \t| Train loss: 0.249 \t| Train acc: 0.23 \t| Test acc: 0.24\n",
      "Epoch: 74 \t| Train loss: 0.249 \t| Train acc: 0.23 \t| Test acc: 0.24\n",
      "Epoch: 75 \t| Train loss: 0.249 \t| Train acc: 0.23 \t| Test acc: 0.24\n",
      "Epoch: 76 \t| Train loss: 0.249 \t| Train acc: 0.23 \t| Test acc: 0.24\n",
      "Epoch: 77 \t| Train loss: 0.248 \t| Train acc: 0.23 \t| Test acc: 0.24\n",
      "Epoch: 78 \t| Train loss: 0.248 \t| Train acc: 0.23 \t| Test acc: 0.24\n",
      "Epoch: 79 \t| Train loss: 0.248 \t| Train acc: 0.23 \t| Test acc: 0.24\n",
      "Epoch: 80 \t| Train loss: 0.248 \t| Train acc: 0.23 \t| Test acc: 0.24\n",
      "Epoch: 81 \t| Train loss: 0.248 \t| Train acc: 0.23 \t| Test acc: 0.24\n",
      "Epoch: 82 \t| Train loss: 0.248 \t| Train acc: 0.23 \t| Test acc: 0.24\n",
      "Epoch: 83 \t| Train loss: 0.248 \t| Train acc: 0.23 \t| Test acc: 0.25\n",
      "Epoch: 84 \t| Train loss: 0.248 \t| Train acc: 0.23 \t| Test acc: 0.25\n",
      "Epoch: 85 \t| Train loss: 0.248 \t| Train acc: 0.23 \t| Test acc: 0.25\n",
      "Epoch: 86 \t| Train loss: 0.248 \t| Train acc: 0.23 \t| Test acc: 0.25\n",
      "Epoch: 87 \t| Train loss: 0.248 \t| Train acc: 0.24 \t| Test acc: 0.25\n",
      "Epoch: 88 \t| Train loss: 0.248 \t| Train acc: 0.24 \t| Test acc: 0.25\n",
      "Epoch: 89 \t| Train loss: 0.247 \t| Train acc: 0.23 \t| Test acc: 0.26\n",
      "Epoch: 90 \t| Train loss: 0.247 \t| Train acc: 0.24 \t| Test acc: 0.26\n",
      "Epoch: 91 \t| Train loss: 0.247 \t| Train acc: 0.24 \t| Test acc: 0.27\n",
      "Epoch: 92 \t| Train loss: 0.247 \t| Train acc: 0.24 \t| Test acc: 0.27\n",
      "Epoch: 93 \t| Train loss: 0.247 \t| Train acc: 0.25 \t| Test acc: 0.27\n",
      "Epoch: 94 \t| Train loss: 0.247 \t| Train acc: 0.25 \t| Test acc: 0.27\n",
      "Epoch: 95 \t| Train loss: 0.247 \t| Train acc: 0.25 \t| Test acc: 0.28\n",
      "Epoch: 96 \t| Train loss: 0.247 \t| Train acc: 0.24 \t| Test acc: 0.28\n",
      "Epoch: 97 \t| Train loss: 0.247 \t| Train acc: 0.26 \t| Test acc: 0.28\n",
      "Epoch: 98 \t| Train loss: 0.247 \t| Train acc: 0.26 \t| Test acc: 0.29\n",
      "Epoch: 99 \t| Train loss: 0.247 \t| Train acc: 0.25 \t| Test acc: 0.29\n",
      "Epoch: 100 \t| Train loss: 0.246 \t| Train acc: 0.24 \t| Test acc: 0.3\n",
      "Epoch: 101 \t| Train loss: 0.246 \t| Train acc: 0.25 \t| Test acc: 0.3\n",
      "Epoch: 102 \t| Train loss: 0.246 \t| Train acc: 0.25 \t| Test acc: 0.3\n",
      "Epoch: 103 \t| Train loss: 0.246 \t| Train acc: 0.25 \t| Test acc: 0.31\n",
      "Epoch: 104 \t| Train loss: 0.246 \t| Train acc: 0.26 \t| Test acc: 0.31\n",
      "Epoch: 105 \t| Train loss: 0.246 \t| Train acc: 0.27 \t| Test acc: 0.32\n",
      "Epoch: 106 \t| Train loss: 0.246 \t| Train acc: 0.28 \t| Test acc: 0.32\n",
      "Epoch: 107 \t| Train loss: 0.246 \t| Train acc: 0.29 \t| Test acc: 0.32\n",
      "Epoch: 108 \t| Train loss: 0.246 \t| Train acc: 0.3 \t| Test acc: 0.32\n",
      "Epoch: 109 \t| Train loss: 0.246 \t| Train acc: 0.3 \t| Test acc: 0.33\n",
      "Epoch: 110 \t| Train loss: 0.245 \t| Train acc: 0.3 \t| Test acc: 0.33\n",
      "Epoch: 111 \t| Train loss: 0.245 \t| Train acc: 0.3 \t| Test acc: 0.34\n",
      "Epoch: 112 \t| Train loss: 0.245 \t| Train acc: 0.3 \t| Test acc: 0.34\n",
      "Epoch: 113 \t| Train loss: 0.245 \t| Train acc: 0.32 \t| Test acc: 0.35\n",
      "Epoch: 114 \t| Train loss: 0.245 \t| Train acc: 0.35 \t| Test acc: 0.35\n",
      "Epoch: 115 \t| Train loss: 0.245 \t| Train acc: 0.35 \t| Test acc: 0.35\n",
      "Epoch: 116 \t| Train loss: 0.245 \t| Train acc: 0.35 \t| Test acc: 0.36\n",
      "Epoch: 117 \t| Train loss: 0.245 \t| Train acc: 0.36 \t| Test acc: 0.36\n",
      "Epoch: 118 \t| Train loss: 0.245 \t| Train acc: 0.37 \t| Test acc: 0.37\n",
      "Epoch: 119 \t| Train loss: 0.245 \t| Train acc: 0.37 \t| Test acc: 0.37\n",
      "Epoch: 120 \t| Train loss: 0.244 \t| Train acc: 0.37 \t| Test acc: 0.37\n",
      "Epoch: 121 \t| Train loss: 0.244 \t| Train acc: 0.37 \t| Test acc: 0.38\n",
      "Epoch: 122 \t| Train loss: 0.244 \t| Train acc: 0.37 \t| Test acc: 0.38\n",
      "Epoch: 123 \t| Train loss: 0.244 \t| Train acc: 0.37 \t| Test acc: 0.38\n",
      "Epoch: 124 \t| Train loss: 0.244 \t| Train acc: 0.37 \t| Test acc: 0.39\n",
      "Epoch: 125 \t| Train loss: 0.244 \t| Train acc: 0.37 \t| Test acc: 0.39\n",
      "Epoch: 126 \t| Train loss: 0.244 \t| Train acc: 0.38 \t| Test acc: 0.39\n",
      "Epoch: 127 \t| Train loss: 0.244 \t| Train acc: 0.38 \t| Test acc: 0.39\n",
      "Epoch: 128 \t| Train loss: 0.244 \t| Train acc: 0.4 \t| Test acc: 0.4\n",
      "Epoch: 129 \t| Train loss: 0.243 \t| Train acc: 0.39 \t| Test acc: 0.4\n",
      "Epoch: 130 \t| Train loss: 0.243 \t| Train acc: 0.4 \t| Test acc: 0.4\n",
      "Epoch: 131 \t| Train loss: 0.243 \t| Train acc: 0.4 \t| Test acc: 0.4\n",
      "Epoch: 132 \t| Train loss: 0.243 \t| Train acc: 0.4 \t| Test acc: 0.41\n",
      "Epoch: 133 \t| Train loss: 0.243 \t| Train acc: 0.41 \t| Test acc: 0.41\n",
      "Epoch: 134 \t| Train loss: 0.243 \t| Train acc: 0.41 \t| Test acc: 0.41\n",
      "Epoch: 135 \t| Train loss: 0.243 \t| Train acc: 0.41 \t| Test acc: 0.42\n",
      "Epoch: 136 \t| Train loss: 0.243 \t| Train acc: 0.41 \t| Test acc: 0.42\n",
      "Epoch: 137 \t| Train loss: 0.243 \t| Train acc: 0.41 \t| Test acc: 0.42\n",
      "Epoch: 138 \t| Train loss: 0.243 \t| Train acc: 0.41 \t| Test acc: 0.42\n",
      "Epoch: 139 \t| Train loss: 0.242 \t| Train acc: 0.41 \t| Test acc: 0.43\n",
      "Epoch: 140 \t| Train loss: 0.242 \t| Train acc: 0.42 \t| Test acc: 0.43\n",
      "Epoch: 141 \t| Train loss: 0.242 \t| Train acc: 0.43 \t| Test acc: 0.43\n",
      "Epoch: 142 \t| Train loss: 0.242 \t| Train acc: 0.43 \t| Test acc: 0.43\n",
      "Epoch: 143 \t| Train loss: 0.242 \t| Train acc: 0.42 \t| Test acc: 0.43\n",
      "Epoch: 144 \t| Train loss: 0.242 \t| Train acc: 0.42 \t| Test acc: 0.44\n",
      "Epoch: 145 \t| Train loss: 0.242 \t| Train acc: 0.42 \t| Test acc: 0.44\n",
      "Epoch: 146 \t| Train loss: 0.242 \t| Train acc: 0.43 \t| Test acc: 0.44\n",
      "Epoch: 147 \t| Train loss: 0.242 \t| Train acc: 0.43 \t| Test acc: 0.44\n",
      "Epoch: 148 \t| Train loss: 0.241 \t| Train acc: 0.43 \t| Test acc: 0.44\n",
      "Epoch: 149 \t| Train loss: 0.241 \t| Train acc: 0.44 \t| Test acc: 0.44\n",
      "Epoch: 150 \t| Train loss: 0.241 \t| Train acc: 0.44 \t| Test acc: 0.45\n",
      "Epoch: 151 \t| Train loss: 0.241 \t| Train acc: 0.44 \t| Test acc: 0.45\n",
      "Epoch: 152 \t| Train loss: 0.241 \t| Train acc: 0.44 \t| Test acc: 0.45\n",
      "Epoch: 153 \t| Train loss: 0.241 \t| Train acc: 0.45 \t| Test acc: 0.45\n",
      "Epoch: 154 \t| Train loss: 0.241 \t| Train acc: 0.45 \t| Test acc: 0.46\n",
      "Epoch: 155 \t| Train loss: 0.241 \t| Train acc: 0.45 \t| Test acc: 0.46\n",
      "Epoch: 156 \t| Train loss: 0.241 \t| Train acc: 0.45 \t| Test acc: 0.46\n",
      "Epoch: 157 \t| Train loss: 0.24 \t| Train acc: 0.45 \t| Test acc: 0.46\n",
      "Epoch: 158 \t| Train loss: 0.24 \t| Train acc: 0.47 \t| Test acc: 0.47\n",
      "Epoch: 159 \t| Train loss: 0.24 \t| Train acc: 0.47 \t| Test acc: 0.47\n",
      "Epoch: 160 \t| Train loss: 0.24 \t| Train acc: 0.47 \t| Test acc: 0.47\n",
      "Epoch: 161 \t| Train loss: 0.24 \t| Train acc: 0.47 \t| Test acc: 0.47\n",
      "Epoch: 162 \t| Train loss: 0.24 \t| Train acc: 0.47 \t| Test acc: 0.48\n",
      "Epoch: 163 \t| Train loss: 0.24 \t| Train acc: 0.47 \t| Test acc: 0.48\n",
      "Epoch: 164 \t| Train loss: 0.24 \t| Train acc: 0.48 \t| Test acc: 0.48\n",
      "Epoch: 165 \t| Train loss: 0.24 \t| Train acc: 0.49 \t| Test acc: 0.48\n",
      "Epoch: 166 \t| Train loss: 0.239 \t| Train acc: 0.49 \t| Test acc: 0.48\n",
      "Epoch: 167 \t| Train loss: 0.239 \t| Train acc: 0.5 \t| Test acc: 0.48\n",
      "Epoch: 168 \t| Train loss: 0.239 \t| Train acc: 0.5 \t| Test acc: 0.48\n",
      "Epoch: 169 \t| Train loss: 0.239 \t| Train acc: 0.5 \t| Test acc: 0.49\n",
      "Epoch: 170 \t| Train loss: 0.239 \t| Train acc: 0.5 \t| Test acc: 0.49\n",
      "Epoch: 171 \t| Train loss: 0.239 \t| Train acc: 0.5 \t| Test acc: 0.49\n",
      "Epoch: 172 \t| Train loss: 0.239 \t| Train acc: 0.5 \t| Test acc: 0.49\n",
      "Epoch: 173 \t| Train loss: 0.239 \t| Train acc: 0.5 \t| Test acc: 0.49\n",
      "Epoch: 174 \t| Train loss: 0.238 \t| Train acc: 0.5 \t| Test acc: 0.49\n",
      "Epoch: 175 \t| Train loss: 0.238 \t| Train acc: 0.5 \t| Test acc: 0.5\n",
      "Epoch: 176 \t| Train loss: 0.238 \t| Train acc: 0.5 \t| Test acc: 0.5\n",
      "Epoch: 177 \t| Train loss: 0.238 \t| Train acc: 0.51 \t| Test acc: 0.5\n",
      "Epoch: 178 \t| Train loss: 0.238 \t| Train acc: 0.52 \t| Test acc: 0.5\n",
      "Epoch: 179 \t| Train loss: 0.238 \t| Train acc: 0.52 \t| Test acc: 0.5\n",
      "Epoch: 180 \t| Train loss: 0.238 \t| Train acc: 0.53 \t| Test acc: 0.5\n",
      "Epoch: 181 \t| Train loss: 0.238 \t| Train acc: 0.54 \t| Test acc: 0.5\n",
      "Epoch: 182 \t| Train loss: 0.237 \t| Train acc: 0.54 \t| Test acc: 0.5\n",
      "Epoch: 183 \t| Train loss: 0.237 \t| Train acc: 0.55 \t| Test acc: 0.51\n",
      "Epoch: 184 \t| Train loss: 0.237 \t| Train acc: 0.55 \t| Test acc: 0.51\n",
      "Epoch: 185 \t| Train loss: 0.237 \t| Train acc: 0.55 \t| Test acc: 0.51\n",
      "Epoch: 186 \t| Train loss: 0.237 \t| Train acc: 0.56 \t| Test acc: 0.51\n",
      "Epoch: 187 \t| Train loss: 0.237 \t| Train acc: 0.57 \t| Test acc: 0.51\n",
      "Epoch: 188 \t| Train loss: 0.236 \t| Train acc: 0.57 \t| Test acc: 0.51\n",
      "Epoch: 189 \t| Train loss: 0.236 \t| Train acc: 0.57 \t| Test acc: 0.51\n",
      "Epoch: 190 \t| Train loss: 0.236 \t| Train acc: 0.57 \t| Test acc: 0.51\n",
      "Epoch: 191 \t| Train loss: 0.236 \t| Train acc: 0.57 \t| Test acc: 0.51\n",
      "Epoch: 192 \t| Train loss: 0.236 \t| Train acc: 0.57 \t| Test acc: 0.52\n",
      "Epoch: 193 \t| Train loss: 0.236 \t| Train acc: 0.57 \t| Test acc: 0.52\n",
      "Epoch: 194 \t| Train loss: 0.235 \t| Train acc: 0.58 \t| Test acc: 0.52\n",
      "Epoch: 195 \t| Train loss: 0.235 \t| Train acc: 0.58 \t| Test acc: 0.52\n",
      "Epoch: 196 \t| Train loss: 0.235 \t| Train acc: 0.58 \t| Test acc: 0.52\n",
      "Epoch: 197 \t| Train loss: 0.235 \t| Train acc: 0.58 \t| Test acc: 0.52\n",
      "Epoch: 198 \t| Train loss: 0.235 \t| Train acc: 0.58 \t| Test acc: 0.52\n",
      "Epoch: 199 \t| Train loss: 0.235 \t| Train acc: 0.59 \t| Test acc: 0.52\n",
      "Epoch: 200 \t| Train loss: 0.234 \t| Train acc: 0.59 \t| Test acc: 0.52\n",
      "Epoch: 201 \t| Train loss: 0.234 \t| Train acc: 0.59 \t| Test acc: 0.52\n",
      "Epoch: 202 \t| Train loss: 0.234 \t| Train acc: 0.59 \t| Test acc: 0.52\n",
      "Epoch: 203 \t| Train loss: 0.234 \t| Train acc: 0.6 \t| Test acc: 0.52\n",
      "Epoch: 204 \t| Train loss: 0.234 \t| Train acc: 0.6 \t| Test acc: 0.53\n",
      "Epoch: 205 \t| Train loss: 0.233 \t| Train acc: 0.6 \t| Test acc: 0.53\n",
      "Epoch: 206 \t| Train loss: 0.233 \t| Train acc: 0.6 \t| Test acc: 0.53\n",
      "Epoch: 207 \t| Train loss: 0.233 \t| Train acc: 0.6 \t| Test acc: 0.53\n",
      "Epoch: 208 \t| Train loss: 0.233 \t| Train acc: 0.6 \t| Test acc: 0.53\n",
      "Epoch: 209 \t| Train loss: 0.233 \t| Train acc: 0.6 \t| Test acc: 0.53\n",
      "Epoch: 210 \t| Train loss: 0.232 \t| Train acc: 0.6 \t| Test acc: 0.53\n",
      "Epoch: 211 \t| Train loss: 0.232 \t| Train acc: 0.6 \t| Test acc: 0.53\n",
      "Epoch: 212 \t| Train loss: 0.232 \t| Train acc: 0.6 \t| Test acc: 0.54\n",
      "Epoch: 213 \t| Train loss: 0.232 \t| Train acc: 0.61 \t| Test acc: 0.54\n",
      "Epoch: 214 \t| Train loss: 0.232 \t| Train acc: 0.61 \t| Test acc: 0.54\n",
      "Epoch: 215 \t| Train loss: 0.231 \t| Train acc: 0.62 \t| Test acc: 0.54\n",
      "Epoch: 216 \t| Train loss: 0.231 \t| Train acc: 0.61 \t| Test acc: 0.54\n",
      "Epoch: 217 \t| Train loss: 0.231 \t| Train acc: 0.61 \t| Test acc: 0.54\n",
      "Epoch: 218 \t| Train loss: 0.231 \t| Train acc: 0.61 \t| Test acc: 0.54\n",
      "Epoch: 219 \t| Train loss: 0.231 \t| Train acc: 0.62 \t| Test acc: 0.54\n",
      "Epoch: 220 \t| Train loss: 0.23 \t| Train acc: 0.62 \t| Test acc: 0.54\n",
      "Epoch: 221 \t| Train loss: 0.23 \t| Train acc: 0.63 \t| Test acc: 0.55\n",
      "Epoch: 222 \t| Train loss: 0.23 \t| Train acc: 0.63 \t| Test acc: 0.55\n",
      "Epoch: 223 \t| Train loss: 0.23 \t| Train acc: 0.63 \t| Test acc: 0.55\n",
      "Epoch: 224 \t| Train loss: 0.23 \t| Train acc: 0.63 \t| Test acc: 0.55\n",
      "Epoch: 225 \t| Train loss: 0.229 \t| Train acc: 0.63 \t| Test acc: 0.55\n",
      "Epoch: 226 \t| Train loss: 0.229 \t| Train acc: 0.63 \t| Test acc: 0.55\n",
      "Epoch: 227 \t| Train loss: 0.229 \t| Train acc: 0.63 \t| Test acc: 0.55\n",
      "Epoch: 228 \t| Train loss: 0.229 \t| Train acc: 0.63 \t| Test acc: 0.55\n",
      "Epoch: 229 \t| Train loss: 0.229 \t| Train acc: 0.63 \t| Test acc: 0.55\n",
      "Epoch: 230 \t| Train loss: 0.229 \t| Train acc: 0.63 \t| Test acc: 0.55\n",
      "Epoch: 231 \t| Train loss: 0.228 \t| Train acc: 0.63 \t| Test acc: 0.55\n",
      "Epoch: 232 \t| Train loss: 0.228 \t| Train acc: 0.63 \t| Test acc: 0.55\n",
      "Epoch: 233 \t| Train loss: 0.228 \t| Train acc: 0.63 \t| Test acc: 0.55\n",
      "Epoch: 234 \t| Train loss: 0.228 \t| Train acc: 0.63 \t| Test acc: 0.55\n",
      "Epoch: 235 \t| Train loss: 0.228 \t| Train acc: 0.63 \t| Test acc: 0.55\n",
      "Epoch: 236 \t| Train loss: 0.227 \t| Train acc: 0.63 \t| Test acc: 0.55\n",
      "Epoch: 237 \t| Train loss: 0.227 \t| Train acc: 0.63 \t| Test acc: 0.55\n",
      "Epoch: 238 \t| Train loss: 0.227 \t| Train acc: 0.63 \t| Test acc: 0.55\n",
      "Epoch: 239 \t| Train loss: 0.227 \t| Train acc: 0.63 \t| Test acc: 0.55\n",
      "Epoch: 240 \t| Train loss: 0.227 \t| Train acc: 0.63 \t| Test acc: 0.55\n",
      "Epoch: 241 \t| Train loss: 0.227 \t| Train acc: 0.63 \t| Test acc: 0.55\n",
      "Epoch: 242 \t| Train loss: 0.226 \t| Train acc: 0.63 \t| Test acc: 0.56\n",
      "Epoch: 243 \t| Train loss: 0.226 \t| Train acc: 0.63 \t| Test acc: 0.56\n",
      "Epoch: 244 \t| Train loss: 0.226 \t| Train acc: 0.63 \t| Test acc: 0.55\n",
      "Epoch: 245 \t| Train loss: 0.226 \t| Train acc: 0.63 \t| Test acc: 0.56\n",
      "Epoch: 246 \t| Train loss: 0.226 \t| Train acc: 0.63 \t| Test acc: 0.56\n",
      "Epoch: 247 \t| Train loss: 0.226 \t| Train acc: 0.63 \t| Test acc: 0.56\n",
      "Epoch: 248 \t| Train loss: 0.225 \t| Train acc: 0.63 \t| Test acc: 0.56\n",
      "Epoch: 249 \t| Train loss: 0.225 \t| Train acc: 0.63 \t| Test acc: 0.56\n",
      "Epoch: 250 \t| Train loss: 0.225 \t| Train acc: 0.63 \t| Test acc: 0.56\n",
      "Epoch: 251 \t| Train loss: 0.225 \t| Train acc: 0.63 \t| Test acc: 0.56\n",
      "Epoch: 252 \t| Train loss: 0.225 \t| Train acc: 0.63 \t| Test acc: 0.56\n",
      "Epoch: 253 \t| Train loss: 0.225 \t| Train acc: 0.63 \t| Test acc: 0.56\n",
      "Epoch: 254 \t| Train loss: 0.225 \t| Train acc: 0.63 \t| Test acc: 0.56\n",
      "Epoch: 255 \t| Train loss: 0.224 \t| Train acc: 0.63 \t| Test acc: 0.56\n",
      "Epoch: 256 \t| Train loss: 0.224 \t| Train acc: 0.64 \t| Test acc: 0.56\n",
      "Epoch: 257 \t| Train loss: 0.224 \t| Train acc: 0.64 \t| Test acc: 0.56\n",
      "Epoch: 258 \t| Train loss: 0.224 \t| Train acc: 0.64 \t| Test acc: 0.56\n",
      "Epoch: 259 \t| Train loss: 0.224 \t| Train acc: 0.64 \t| Test acc: 0.56\n",
      "Epoch: 260 \t| Train loss: 0.224 \t| Train acc: 0.64 \t| Test acc: 0.56\n",
      "Epoch: 261 \t| Train loss: 0.224 \t| Train acc: 0.64 \t| Test acc: 0.56\n",
      "Epoch: 262 \t| Train loss: 0.223 \t| Train acc: 0.64 \t| Test acc: 0.56\n",
      "Epoch: 263 \t| Train loss: 0.223 \t| Train acc: 0.64 \t| Test acc: 0.56\n",
      "Epoch: 264 \t| Train loss: 0.223 \t| Train acc: 0.64 \t| Test acc: 0.56\n",
      "Epoch: 265 \t| Train loss: 0.223 \t| Train acc: 0.64 \t| Test acc: 0.56\n",
      "Epoch: 266 \t| Train loss: 0.223 \t| Train acc: 0.64 \t| Test acc: 0.56\n",
      "Epoch: 267 \t| Train loss: 0.223 \t| Train acc: 0.64 \t| Test acc: 0.56\n",
      "Epoch: 268 \t| Train loss: 0.223 \t| Train acc: 0.64 \t| Test acc: 0.56\n",
      "Epoch: 269 \t| Train loss: 0.222 \t| Train acc: 0.64 \t| Test acc: 0.56\n",
      "Epoch: 270 \t| Train loss: 0.222 \t| Train acc: 0.64 \t| Test acc: 0.56\n",
      "Epoch: 271 \t| Train loss: 0.222 \t| Train acc: 0.64 \t| Test acc: 0.56\n",
      "Epoch: 272 \t| Train loss: 0.222 \t| Train acc: 0.64 \t| Test acc: 0.56\n",
      "Epoch: 273 \t| Train loss: 0.222 \t| Train acc: 0.64 \t| Test acc: 0.56\n",
      "Epoch: 274 \t| Train loss: 0.222 \t| Train acc: 0.64 \t| Test acc: 0.56\n",
      "Epoch: 275 \t| Train loss: 0.222 \t| Train acc: 0.64 \t| Test acc: 0.56\n",
      "Epoch: 276 \t| Train loss: 0.222 \t| Train acc: 0.64 \t| Test acc: 0.57\n",
      "Epoch: 277 \t| Train loss: 0.221 \t| Train acc: 0.64 \t| Test acc: 0.57\n",
      "Epoch: 278 \t| Train loss: 0.221 \t| Train acc: 0.64 \t| Test acc: 0.57\n",
      "Epoch: 279 \t| Train loss: 0.221 \t| Train acc: 0.64 \t| Test acc: 0.57\n",
      "Epoch: 280 \t| Train loss: 0.221 \t| Train acc: 0.64 \t| Test acc: 0.57\n",
      "Epoch: 281 \t| Train loss: 0.221 \t| Train acc: 0.64 \t| Test acc: 0.57\n",
      "Epoch: 282 \t| Train loss: 0.221 \t| Train acc: 0.65 \t| Test acc: 0.57\n",
      "Epoch: 283 \t| Train loss: 0.221 \t| Train acc: 0.65 \t| Test acc: 0.57\n",
      "Epoch: 284 \t| Train loss: 0.221 \t| Train acc: 0.65 \t| Test acc: 0.57\n",
      "Epoch: 285 \t| Train loss: 0.221 \t| Train acc: 0.65 \t| Test acc: 0.57\n",
      "Epoch: 286 \t| Train loss: 0.22 \t| Train acc: 0.65 \t| Test acc: 0.57\n",
      "Epoch: 287 \t| Train loss: 0.22 \t| Train acc: 0.65 \t| Test acc: 0.57\n",
      "Epoch: 288 \t| Train loss: 0.22 \t| Train acc: 0.65 \t| Test acc: 0.57\n",
      "Epoch: 289 \t| Train loss: 0.22 \t| Train acc: 0.65 \t| Test acc: 0.57\n",
      "Epoch: 290 \t| Train loss: 0.22 \t| Train acc: 0.65 \t| Test acc: 0.57\n",
      "Epoch: 291 \t| Train loss: 0.22 \t| Train acc: 0.65 \t| Test acc: 0.57\n",
      "Epoch: 292 \t| Train loss: 0.22 \t| Train acc: 0.66 \t| Test acc: 0.57\n",
      "Epoch: 293 \t| Train loss: 0.22 \t| Train acc: 0.66 \t| Test acc: 0.57\n",
      "Epoch: 294 \t| Train loss: 0.22 \t| Train acc: 0.66 \t| Test acc: 0.57\n",
      "Epoch: 295 \t| Train loss: 0.22 \t| Train acc: 0.66 \t| Test acc: 0.57\n",
      "Epoch: 296 \t| Train loss: 0.219 \t| Train acc: 0.66 \t| Test acc: 0.57\n",
      "Epoch: 297 \t| Train loss: 0.219 \t| Train acc: 0.66 \t| Test acc: 0.57\n",
      "Epoch: 298 \t| Train loss: 0.219 \t| Train acc: 0.66 \t| Test acc: 0.57\n",
      "Epoch: 299 \t| Train loss: 0.219 \t| Train acc: 0.66 \t| Test acc: 0.57\n",
      "Epoch: 300 \t| Train loss: 0.219 \t| Train acc: 0.66 \t| Test acc: 0.57\n",
      "Epoch: 301 \t| Train loss: 0.219 \t| Train acc: 0.66 \t| Test acc: 0.57\n",
      "Epoch: 302 \t| Train loss: 0.219 \t| Train acc: 0.66 \t| Test acc: 0.57\n",
      "Epoch: 303 \t| Train loss: 0.219 \t| Train acc: 0.66 \t| Test acc: 0.57\n",
      "Epoch: 304 \t| Train loss: 0.219 \t| Train acc: 0.66 \t| Test acc: 0.57\n",
      "Epoch: 305 \t| Train loss: 0.219 \t| Train acc: 0.66 \t| Test acc: 0.57\n",
      "Epoch: 306 \t| Train loss: 0.219 \t| Train acc: 0.66 \t| Test acc: 0.57\n",
      "Epoch: 307 \t| Train loss: 0.219 \t| Train acc: 0.66 \t| Test acc: 0.57\n",
      "Epoch: 308 \t| Train loss: 0.218 \t| Train acc: 0.66 \t| Test acc: 0.57\n",
      "Epoch: 309 \t| Train loss: 0.218 \t| Train acc: 0.66 \t| Test acc: 0.57\n",
      "Epoch: 310 \t| Train loss: 0.218 \t| Train acc: 0.66 \t| Test acc: 0.57\n",
      "Epoch: 311 \t| Train loss: 0.218 \t| Train acc: 0.66 \t| Test acc: 0.57\n",
      "Epoch: 312 \t| Train loss: 0.218 \t| Train acc: 0.66 \t| Test acc: 0.57\n",
      "Epoch: 313 \t| Train loss: 0.218 \t| Train acc: 0.66 \t| Test acc: 0.57\n",
      "Epoch: 314 \t| Train loss: 0.218 \t| Train acc: 0.66 \t| Test acc: 0.57\n",
      "Epoch: 315 \t| Train loss: 0.218 \t| Train acc: 0.66 \t| Test acc: 0.57\n",
      "Epoch: 316 \t| Train loss: 0.218 \t| Train acc: 0.66 \t| Test acc: 0.57\n",
      "Epoch: 317 \t| Train loss: 0.218 \t| Train acc: 0.66 \t| Test acc: 0.57\n",
      "Epoch: 318 \t| Train loss: 0.218 \t| Train acc: 0.66 \t| Test acc: 0.57\n",
      "Epoch: 319 \t| Train loss: 0.218 \t| Train acc: 0.66 \t| Test acc: 0.57\n",
      "Epoch: 320 \t| Train loss: 0.218 \t| Train acc: 0.66 \t| Test acc: 0.57\n",
      "Epoch: 321 \t| Train loss: 0.217 \t| Train acc: 0.66 \t| Test acc: 0.57\n",
      "Epoch: 322 \t| Train loss: 0.217 \t| Train acc: 0.66 \t| Test acc: 0.57\n",
      "Epoch: 323 \t| Train loss: 0.217 \t| Train acc: 0.66 \t| Test acc: 0.57\n",
      "Epoch: 324 \t| Train loss: 0.217 \t| Train acc: 0.66 \t| Test acc: 0.57\n",
      "Epoch: 325 \t| Train loss: 0.217 \t| Train acc: 0.66 \t| Test acc: 0.57\n",
      "Epoch: 326 \t| Train loss: 0.217 \t| Train acc: 0.66 \t| Test acc: 0.57\n",
      "Epoch: 327 \t| Train loss: 0.217 \t| Train acc: 0.66 \t| Test acc: 0.57\n",
      "Epoch: 328 \t| Train loss: 0.217 \t| Train acc: 0.66 \t| Test acc: 0.57\n",
      "Epoch: 329 \t| Train loss: 0.217 \t| Train acc: 0.66 \t| Test acc: 0.57\n",
      "Epoch: 330 \t| Train loss: 0.217 \t| Train acc: 0.66 \t| Test acc: 0.57\n",
      "Epoch: 331 \t| Train loss: 0.217 \t| Train acc: 0.66 \t| Test acc: 0.57\n",
      "Epoch: 332 \t| Train loss: 0.217 \t| Train acc: 0.67 \t| Test acc: 0.57\n",
      "Epoch: 333 \t| Train loss: 0.217 \t| Train acc: 0.67 \t| Test acc: 0.57\n",
      "Epoch: 334 \t| Train loss: 0.217 \t| Train acc: 0.67 \t| Test acc: 0.57\n",
      "Epoch: 335 \t| Train loss: 0.217 \t| Train acc: 0.67 \t| Test acc: 0.57\n",
      "Epoch: 336 \t| Train loss: 0.216 \t| Train acc: 0.67 \t| Test acc: 0.57\n",
      "Epoch: 337 \t| Train loss: 0.216 \t| Train acc: 0.67 \t| Test acc: 0.57\n",
      "Epoch: 338 \t| Train loss: 0.216 \t| Train acc: 0.67 \t| Test acc: 0.57\n",
      "Epoch: 339 \t| Train loss: 0.216 \t| Train acc: 0.67 \t| Test acc: 0.57\n",
      "Epoch: 340 \t| Train loss: 0.216 \t| Train acc: 0.67 \t| Test acc: 0.57\n",
      "Epoch: 341 \t| Train loss: 0.216 \t| Train acc: 0.68 \t| Test acc: 0.57\n",
      "Epoch: 342 \t| Train loss: 0.216 \t| Train acc: 0.68 \t| Test acc: 0.57\n",
      "Epoch: 343 \t| Train loss: 0.216 \t| Train acc: 0.68 \t| Test acc: 0.57\n",
      "Epoch: 344 \t| Train loss: 0.216 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 345 \t| Train loss: 0.216 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 346 \t| Train loss: 0.216 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 347 \t| Train loss: 0.216 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 348 \t| Train loss: 0.216 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 349 \t| Train loss: 0.216 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 350 \t| Train loss: 0.216 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 351 \t| Train loss: 0.216 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 352 \t| Train loss: 0.216 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 353 \t| Train loss: 0.216 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 354 \t| Train loss: 0.216 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 355 \t| Train loss: 0.215 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 356 \t| Train loss: 0.215 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 357 \t| Train loss: 0.215 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 358 \t| Train loss: 0.215 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 359 \t| Train loss: 0.215 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 360 \t| Train loss: 0.215 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 361 \t| Train loss: 0.215 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 362 \t| Train loss: 0.215 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 363 \t| Train loss: 0.215 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 364 \t| Train loss: 0.215 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 365 \t| Train loss: 0.215 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 366 \t| Train loss: 0.215 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 367 \t| Train loss: 0.215 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 368 \t| Train loss: 0.215 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 369 \t| Train loss: 0.215 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 370 \t| Train loss: 0.215 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 371 \t| Train loss: 0.215 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 372 \t| Train loss: 0.215 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 373 \t| Train loss: 0.215 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 374 \t| Train loss: 0.215 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 375 \t| Train loss: 0.215 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 376 \t| Train loss: 0.214 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 377 \t| Train loss: 0.214 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 378 \t| Train loss: 0.214 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 379 \t| Train loss: 0.214 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 380 \t| Train loss: 0.214 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 381 \t| Train loss: 0.214 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 382 \t| Train loss: 0.214 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 383 \t| Train loss: 0.214 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 384 \t| Train loss: 0.214 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 385 \t| Train loss: 0.214 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 386 \t| Train loss: 0.214 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 387 \t| Train loss: 0.214 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 388 \t| Train loss: 0.214 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 389 \t| Train loss: 0.214 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 390 \t| Train loss: 0.214 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 391 \t| Train loss: 0.214 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 392 \t| Train loss: 0.214 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 393 \t| Train loss: 0.214 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 394 \t| Train loss: 0.214 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 395 \t| Train loss: 0.214 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 396 \t| Train loss: 0.214 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 397 \t| Train loss: 0.214 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 398 \t| Train loss: 0.214 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 399 \t| Train loss: 0.213 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 400 \t| Train loss: 0.213 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 401 \t| Train loss: 0.213 \t| Train acc: 0.68 \t| Test acc: 0.57\n",
      "Epoch: 402 \t| Train loss: 0.213 \t| Train acc: 0.68 \t| Test acc: 0.57\n",
      "Epoch: 403 \t| Train loss: 0.213 \t| Train acc: 0.67 \t| Test acc: 0.57\n",
      "Epoch: 404 \t| Train loss: 0.213 \t| Train acc: 0.67 \t| Test acc: 0.57\n",
      "Epoch: 405 \t| Train loss: 0.213 \t| Train acc: 0.67 \t| Test acc: 0.57\n",
      "Epoch: 406 \t| Train loss: 0.213 \t| Train acc: 0.67 \t| Test acc: 0.57\n",
      "Epoch: 407 \t| Train loss: 0.213 \t| Train acc: 0.67 \t| Test acc: 0.57\n",
      "Epoch: 408 \t| Train loss: 0.213 \t| Train acc: 0.67 \t| Test acc: 0.57\n",
      "Epoch: 409 \t| Train loss: 0.213 \t| Train acc: 0.67 \t| Test acc: 0.57\n",
      "Epoch: 410 \t| Train loss: 0.213 \t| Train acc: 0.67 \t| Test acc: 0.57\n",
      "Epoch: 411 \t| Train loss: 0.213 \t| Train acc: 0.67 \t| Test acc: 0.57\n",
      "Epoch: 412 \t| Train loss: 0.213 \t| Train acc: 0.67 \t| Test acc: 0.57\n",
      "Epoch: 413 \t| Train loss: 0.213 \t| Train acc: 0.67 \t| Test acc: 0.57\n",
      "Epoch: 414 \t| Train loss: 0.213 \t| Train acc: 0.67 \t| Test acc: 0.57\n",
      "Epoch: 415 \t| Train loss: 0.213 \t| Train acc: 0.67 \t| Test acc: 0.57\n",
      "Epoch: 416 \t| Train loss: 0.213 \t| Train acc: 0.67 \t| Test acc: 0.57\n",
      "Epoch: 417 \t| Train loss: 0.213 \t| Train acc: 0.67 \t| Test acc: 0.57\n",
      "Epoch: 418 \t| Train loss: 0.213 \t| Train acc: 0.67 \t| Test acc: 0.57\n",
      "Epoch: 419 \t| Train loss: 0.213 \t| Train acc: 0.67 \t| Test acc: 0.57\n",
      "Epoch: 420 \t| Train loss: 0.213 \t| Train acc: 0.67 \t| Test acc: 0.57\n",
      "Epoch: 421 \t| Train loss: 0.213 \t| Train acc: 0.67 \t| Test acc: 0.57\n",
      "Epoch: 422 \t| Train loss: 0.213 \t| Train acc: 0.67 \t| Test acc: 0.57\n",
      "Epoch: 423 \t| Train loss: 0.213 \t| Train acc: 0.67 \t| Test acc: 0.57\n",
      "Epoch: 424 \t| Train loss: 0.213 \t| Train acc: 0.67 \t| Test acc: 0.57\n",
      "Epoch: 425 \t| Train loss: 0.213 \t| Train acc: 0.67 \t| Test acc: 0.57\n",
      "Epoch: 426 \t| Train loss: 0.213 \t| Train acc: 0.67 \t| Test acc: 0.57\n",
      "Epoch: 427 \t| Train loss: 0.213 \t| Train acc: 0.67 \t| Test acc: 0.57\n",
      "Epoch: 428 \t| Train loss: 0.213 \t| Train acc: 0.67 \t| Test acc: 0.57\n",
      "Epoch: 429 \t| Train loss: 0.213 \t| Train acc: 0.67 \t| Test acc: 0.57\n",
      "Epoch: 430 \t| Train loss: 0.212 \t| Train acc: 0.67 \t| Test acc: 0.57\n",
      "Epoch: 431 \t| Train loss: 0.212 \t| Train acc: 0.67 \t| Test acc: 0.57\n",
      "Epoch: 432 \t| Train loss: 0.212 \t| Train acc: 0.67 \t| Test acc: 0.57\n",
      "Epoch: 433 \t| Train loss: 0.212 \t| Train acc: 0.67 \t| Test acc: 0.57\n",
      "Epoch: 434 \t| Train loss: 0.212 \t| Train acc: 0.67 \t| Test acc: 0.57\n",
      "Epoch: 435 \t| Train loss: 0.212 \t| Train acc: 0.67 \t| Test acc: 0.57\n",
      "Epoch: 436 \t| Train loss: 0.212 \t| Train acc: 0.67 \t| Test acc: 0.57\n",
      "Epoch: 437 \t| Train loss: 0.212 \t| Train acc: 0.67 \t| Test acc: 0.57\n",
      "Epoch: 438 \t| Train loss: 0.212 \t| Train acc: 0.67 \t| Test acc: 0.57\n",
      "Epoch: 439 \t| Train loss: 0.212 \t| Train acc: 0.67 \t| Test acc: 0.57\n",
      "Epoch: 440 \t| Train loss: 0.212 \t| Train acc: 0.67 \t| Test acc: 0.57\n",
      "Epoch: 441 \t| Train loss: 0.212 \t| Train acc: 0.67 \t| Test acc: 0.57\n",
      "Epoch: 442 \t| Train loss: 0.212 \t| Train acc: 0.67 \t| Test acc: 0.57\n",
      "Epoch: 443 \t| Train loss: 0.212 \t| Train acc: 0.67 \t| Test acc: 0.58\n",
      "Epoch: 444 \t| Train loss: 0.212 \t| Train acc: 0.67 \t| Test acc: 0.57\n",
      "Epoch: 445 \t| Train loss: 0.212 \t| Train acc: 0.67 \t| Test acc: 0.58\n",
      "Epoch: 446 \t| Train loss: 0.212 \t| Train acc: 0.67 \t| Test acc: 0.58\n",
      "Epoch: 447 \t| Train loss: 0.212 \t| Train acc: 0.67 \t| Test acc: 0.58\n",
      "Epoch: 448 \t| Train loss: 0.212 \t| Train acc: 0.67 \t| Test acc: 0.58\n",
      "Epoch: 449 \t| Train loss: 0.212 \t| Train acc: 0.67 \t| Test acc: 0.57\n",
      "Epoch: 450 \t| Train loss: 0.212 \t| Train acc: 0.67 \t| Test acc: 0.58\n",
      "Epoch: 451 \t| Train loss: 0.212 \t| Train acc: 0.67 \t| Test acc: 0.58\n",
      "Epoch: 452 \t| Train loss: 0.212 \t| Train acc: 0.67 \t| Test acc: 0.58\n",
      "Epoch: 453 \t| Train loss: 0.212 \t| Train acc: 0.67 \t| Test acc: 0.58\n",
      "Epoch: 454 \t| Train loss: 0.212 \t| Train acc: 0.67 \t| Test acc: 0.58\n",
      "Epoch: 455 \t| Train loss: 0.212 \t| Train acc: 0.67 \t| Test acc: 0.57\n",
      "Epoch: 456 \t| Train loss: 0.212 \t| Train acc: 0.67 \t| Test acc: 0.57\n",
      "Epoch: 457 \t| Train loss: 0.212 \t| Train acc: 0.67 \t| Test acc: 0.57\n",
      "Epoch: 458 \t| Train loss: 0.212 \t| Train acc: 0.67 \t| Test acc: 0.57\n",
      "Epoch: 459 \t| Train loss: 0.212 \t| Train acc: 0.67 \t| Test acc: 0.57\n",
      "Epoch: 460 \t| Train loss: 0.212 \t| Train acc: 0.67 \t| Test acc: 0.57\n",
      "Epoch: 461 \t| Train loss: 0.212 \t| Train acc: 0.67 \t| Test acc: 0.57\n",
      "Epoch: 462 \t| Train loss: 0.211 \t| Train acc: 0.67 \t| Test acc: 0.57\n",
      "Epoch: 463 \t| Train loss: 0.211 \t| Train acc: 0.67 \t| Test acc: 0.57\n",
      "Epoch: 464 \t| Train loss: 0.211 \t| Train acc: 0.67 \t| Test acc: 0.57\n",
      "Epoch: 465 \t| Train loss: 0.211 \t| Train acc: 0.67 \t| Test acc: 0.57\n",
      "Epoch: 466 \t| Train loss: 0.211 \t| Train acc: 0.67 \t| Test acc: 0.57\n",
      "Epoch: 467 \t| Train loss: 0.211 \t| Train acc: 0.67 \t| Test acc: 0.57\n",
      "Epoch: 468 \t| Train loss: 0.211 \t| Train acc: 0.67 \t| Test acc: 0.57\n",
      "Epoch: 469 \t| Train loss: 0.211 \t| Train acc: 0.67 \t| Test acc: 0.57\n",
      "Epoch: 470 \t| Train loss: 0.211 \t| Train acc: 0.67 \t| Test acc: 0.57\n",
      "Epoch: 471 \t| Train loss: 0.211 \t| Train acc: 0.67 \t| Test acc: 0.57\n",
      "Epoch: 472 \t| Train loss: 0.211 \t| Train acc: 0.67 \t| Test acc: 0.57\n",
      "Epoch: 473 \t| Train loss: 0.211 \t| Train acc: 0.67 \t| Test acc: 0.57\n",
      "Epoch: 474 \t| Train loss: 0.211 \t| Train acc: 0.67 \t| Test acc: 0.57\n",
      "Epoch: 475 \t| Train loss: 0.211 \t| Train acc: 0.67 \t| Test acc: 0.57\n",
      "Epoch: 476 \t| Train loss: 0.211 \t| Train acc: 0.68 \t| Test acc: 0.57\n",
      "Epoch: 477 \t| Train loss: 0.211 \t| Train acc: 0.68 \t| Test acc: 0.57\n",
      "Epoch: 478 \t| Train loss: 0.211 \t| Train acc: 0.68 \t| Test acc: 0.57\n",
      "Epoch: 479 \t| Train loss: 0.211 \t| Train acc: 0.68 \t| Test acc: 0.57\n",
      "Epoch: 480 \t| Train loss: 0.211 \t| Train acc: 0.68 \t| Test acc: 0.57\n",
      "Epoch: 481 \t| Train loss: 0.211 \t| Train acc: 0.68 \t| Test acc: 0.57\n",
      "Epoch: 482 \t| Train loss: 0.211 \t| Train acc: 0.68 \t| Test acc: 0.57\n",
      "Epoch: 483 \t| Train loss: 0.211 \t| Train acc: 0.68 \t| Test acc: 0.57\n",
      "Epoch: 484 \t| Train loss: 0.211 \t| Train acc: 0.68 \t| Test acc: 0.57\n",
      "Epoch: 485 \t| Train loss: 0.211 \t| Train acc: 0.68 \t| Test acc: 0.57\n",
      "Epoch: 486 \t| Train loss: 0.211 \t| Train acc: 0.68 \t| Test acc: 0.57\n",
      "Epoch: 487 \t| Train loss: 0.211 \t| Train acc: 0.68 \t| Test acc: 0.57\n",
      "Epoch: 488 \t| Train loss: 0.211 \t| Train acc: 0.68 \t| Test acc: 0.57\n",
      "Epoch: 489 \t| Train loss: 0.21 \t| Train acc: 0.68 \t| Test acc: 0.57\n",
      "Epoch: 490 \t| Train loss: 0.21 \t| Train acc: 0.68 \t| Test acc: 0.57\n",
      "Epoch: 491 \t| Train loss: 0.21 \t| Train acc: 0.68 \t| Test acc: 0.57\n",
      "Epoch: 492 \t| Train loss: 0.21 \t| Train acc: 0.68 \t| Test acc: 0.57\n",
      "Epoch: 493 \t| Train loss: 0.21 \t| Train acc: 0.68 \t| Test acc: 0.57\n",
      "Epoch: 494 \t| Train loss: 0.21 \t| Train acc: 0.68 \t| Test acc: 0.57\n",
      "Epoch: 495 \t| Train loss: 0.21 \t| Train acc: 0.68 \t| Test acc: 0.57\n",
      "Epoch: 496 \t| Train loss: 0.21 \t| Train acc: 0.68 \t| Test acc: 0.57\n",
      "Epoch: 497 \t| Train loss: 0.21 \t| Train acc: 0.68 \t| Test acc: 0.57\n",
      "Epoch: 498 \t| Train loss: 0.21 \t| Train acc: 0.68 \t| Test acc: 0.57\n",
      "Epoch: 499 \t| Train loss: 0.21 \t| Train acc: 0.68 \t| Test acc: 0.57\n",
      "Epoch: 500 \t| Train loss: 0.21 \t| Train acc: 0.68 \t| Test acc: 0.57\n",
      "Epoch: 501 \t| Train loss: 0.21 \t| Train acc: 0.68 \t| Test acc: 0.57\n",
      "Epoch: 502 \t| Train loss: 0.21 \t| Train acc: 0.68 \t| Test acc: 0.57\n",
      "Epoch: 503 \t| Train loss: 0.21 \t| Train acc: 0.68 \t| Test acc: 0.57\n",
      "Epoch: 504 \t| Train loss: 0.21 \t| Train acc: 0.68 \t| Test acc: 0.57\n",
      "Epoch: 505 \t| Train loss: 0.21 \t| Train acc: 0.68 \t| Test acc: 0.57\n",
      "Epoch: 506 \t| Train loss: 0.21 \t| Train acc: 0.68 \t| Test acc: 0.57\n",
      "Epoch: 507 \t| Train loss: 0.21 \t| Train acc: 0.68 \t| Test acc: 0.57\n",
      "Epoch: 508 \t| Train loss: 0.21 \t| Train acc: 0.68 \t| Test acc: 0.57\n",
      "Epoch: 509 \t| Train loss: 0.21 \t| Train acc: 0.68 \t| Test acc: 0.57\n",
      "Epoch: 510 \t| Train loss: 0.21 \t| Train acc: 0.68 \t| Test acc: 0.57\n",
      "Epoch: 511 \t| Train loss: 0.21 \t| Train acc: 0.68 \t| Test acc: 0.57\n",
      "Epoch: 512 \t| Train loss: 0.21 \t| Train acc: 0.68 \t| Test acc: 0.57\n",
      "Epoch: 513 \t| Train loss: 0.21 \t| Train acc: 0.68 \t| Test acc: 0.57\n",
      "Epoch: 514 \t| Train loss: 0.21 \t| Train acc: 0.68 \t| Test acc: 0.57\n",
      "Epoch: 515 \t| Train loss: 0.21 \t| Train acc: 0.68 \t| Test acc: 0.57\n",
      "Epoch: 516 \t| Train loss: 0.21 \t| Train acc: 0.68 \t| Test acc: 0.57\n",
      "Epoch: 517 \t| Train loss: 0.21 \t| Train acc: 0.68 \t| Test acc: 0.57\n",
      "Epoch: 518 \t| Train loss: 0.21 \t| Train acc: 0.68 \t| Test acc: 0.57\n",
      "Epoch: 519 \t| Train loss: 0.21 \t| Train acc: 0.68 \t| Test acc: 0.57\n",
      "Epoch: 520 \t| Train loss: 0.21 \t| Train acc: 0.68 \t| Test acc: 0.57\n",
      "Epoch: 521 \t| Train loss: 0.21 \t| Train acc: 0.67 \t| Test acc: 0.57\n",
      "Epoch: 522 \t| Train loss: 0.21 \t| Train acc: 0.67 \t| Test acc: 0.57\n",
      "Epoch: 523 \t| Train loss: 0.209 \t| Train acc: 0.67 \t| Test acc: 0.57\n",
      "Epoch: 524 \t| Train loss: 0.209 \t| Train acc: 0.67 \t| Test acc: 0.57\n",
      "Epoch: 525 \t| Train loss: 0.209 \t| Train acc: 0.67 \t| Test acc: 0.57\n",
      "Epoch: 526 \t| Train loss: 0.209 \t| Train acc: 0.67 \t| Test acc: 0.57\n",
      "Epoch: 527 \t| Train loss: 0.209 \t| Train acc: 0.67 \t| Test acc: 0.57\n",
      "Epoch: 528 \t| Train loss: 0.209 \t| Train acc: 0.67 \t| Test acc: 0.57\n",
      "Epoch: 529 \t| Train loss: 0.209 \t| Train acc: 0.67 \t| Test acc: 0.57\n",
      "Epoch: 530 \t| Train loss: 0.209 \t| Train acc: 0.67 \t| Test acc: 0.57\n",
      "Epoch: 531 \t| Train loss: 0.209 \t| Train acc: 0.67 \t| Test acc: 0.57\n",
      "Epoch: 532 \t| Train loss: 0.209 \t| Train acc: 0.67 \t| Test acc: 0.57\n",
      "Epoch: 533 \t| Train loss: 0.209 \t| Train acc: 0.68 \t| Test acc: 0.57\n",
      "Epoch: 534 \t| Train loss: 0.209 \t| Train acc: 0.68 \t| Test acc: 0.57\n",
      "Epoch: 535 \t| Train loss: 0.209 \t| Train acc: 0.68 \t| Test acc: 0.57\n",
      "Epoch: 536 \t| Train loss: 0.209 \t| Train acc: 0.67 \t| Test acc: 0.57\n",
      "Epoch: 537 \t| Train loss: 0.209 \t| Train acc: 0.67 \t| Test acc: 0.57\n",
      "Epoch: 538 \t| Train loss: 0.209 \t| Train acc: 0.67 \t| Test acc: 0.57\n",
      "Epoch: 539 \t| Train loss: 0.209 \t| Train acc: 0.67 \t| Test acc: 0.57\n",
      "Epoch: 540 \t| Train loss: 0.209 \t| Train acc: 0.67 \t| Test acc: 0.57\n",
      "Epoch: 541 \t| Train loss: 0.209 \t| Train acc: 0.67 \t| Test acc: 0.57\n",
      "Epoch: 542 \t| Train loss: 0.209 \t| Train acc: 0.67 \t| Test acc: 0.57\n",
      "Epoch: 543 \t| Train loss: 0.209 \t| Train acc: 0.67 \t| Test acc: 0.57\n",
      "Epoch: 544 \t| Train loss: 0.209 \t| Train acc: 0.67 \t| Test acc: 0.57\n",
      "Epoch: 545 \t| Train loss: 0.209 \t| Train acc: 0.67 \t| Test acc: 0.58\n",
      "Epoch: 546 \t| Train loss: 0.209 \t| Train acc: 0.67 \t| Test acc: 0.58\n",
      "Epoch: 547 \t| Train loss: 0.209 \t| Train acc: 0.67 \t| Test acc: 0.58\n",
      "Epoch: 548 \t| Train loss: 0.209 \t| Train acc: 0.67 \t| Test acc: 0.58\n",
      "Epoch: 549 \t| Train loss: 0.209 \t| Train acc: 0.67 \t| Test acc: 0.58\n",
      "Epoch: 550 \t| Train loss: 0.209 \t| Train acc: 0.67 \t| Test acc: 0.58\n",
      "Epoch: 551 \t| Train loss: 0.209 \t| Train acc: 0.68 \t| Test acc: 0.58\n",
      "Epoch: 552 \t| Train loss: 0.209 \t| Train acc: 0.68 \t| Test acc: 0.58\n",
      "Epoch: 553 \t| Train loss: 0.209 \t| Train acc: 0.68 \t| Test acc: 0.58\n",
      "Epoch: 554 \t| Train loss: 0.209 \t| Train acc: 0.68 \t| Test acc: 0.58\n",
      "Epoch: 555 \t| Train loss: 0.209 \t| Train acc: 0.68 \t| Test acc: 0.58\n",
      "Epoch: 556 \t| Train loss: 0.209 \t| Train acc: 0.68 \t| Test acc: 0.58\n",
      "Epoch: 557 \t| Train loss: 0.209 \t| Train acc: 0.68 \t| Test acc: 0.58\n",
      "Epoch: 558 \t| Train loss: 0.209 \t| Train acc: 0.68 \t| Test acc: 0.58\n",
      "Epoch: 559 \t| Train loss: 0.209 \t| Train acc: 0.68 \t| Test acc: 0.58\n",
      "Epoch: 560 \t| Train loss: 0.209 \t| Train acc: 0.68 \t| Test acc: 0.58\n",
      "Epoch: 561 \t| Train loss: 0.209 \t| Train acc: 0.68 \t| Test acc: 0.58\n",
      "Epoch: 562 \t| Train loss: 0.209 \t| Train acc: 0.68 \t| Test acc: 0.58\n",
      "Epoch: 563 \t| Train loss: 0.209 \t| Train acc: 0.68 \t| Test acc: 0.58\n",
      "Epoch: 564 \t| Train loss: 0.209 \t| Train acc: 0.68 \t| Test acc: 0.58\n",
      "Epoch: 565 \t| Train loss: 0.209 \t| Train acc: 0.68 \t| Test acc: 0.58\n",
      "Epoch: 566 \t| Train loss: 0.209 \t| Train acc: 0.68 \t| Test acc: 0.58\n",
      "Epoch: 567 \t| Train loss: 0.209 \t| Train acc: 0.68 \t| Test acc: 0.58\n",
      "Epoch: 568 \t| Train loss: 0.208 \t| Train acc: 0.68 \t| Test acc: 0.58\n",
      "Epoch: 569 \t| Train loss: 0.208 \t| Train acc: 0.68 \t| Test acc: 0.58\n",
      "Epoch: 570 \t| Train loss: 0.208 \t| Train acc: 0.68 \t| Test acc: 0.58\n",
      "Epoch: 571 \t| Train loss: 0.208 \t| Train acc: 0.68 \t| Test acc: 0.58\n",
      "Epoch: 572 \t| Train loss: 0.208 \t| Train acc: 0.68 \t| Test acc: 0.58\n",
      "Epoch: 573 \t| Train loss: 0.208 \t| Train acc: 0.68 \t| Test acc: 0.58\n",
      "Epoch: 574 \t| Train loss: 0.208 \t| Train acc: 0.68 \t| Test acc: 0.58\n",
      "Epoch: 575 \t| Train loss: 0.208 \t| Train acc: 0.68 \t| Test acc: 0.58\n",
      "Epoch: 576 \t| Train loss: 0.208 \t| Train acc: 0.68 \t| Test acc: 0.58\n",
      "Epoch: 577 \t| Train loss: 0.208 \t| Train acc: 0.68 \t| Test acc: 0.58\n",
      "Epoch: 578 \t| Train loss: 0.208 \t| Train acc: 0.68 \t| Test acc: 0.58\n",
      "Epoch: 579 \t| Train loss: 0.208 \t| Train acc: 0.68 \t| Test acc: 0.58\n",
      "Epoch: 580 \t| Train loss: 0.208 \t| Train acc: 0.68 \t| Test acc: 0.58\n",
      "Epoch: 581 \t| Train loss: 0.208 \t| Train acc: 0.68 \t| Test acc: 0.58\n",
      "Epoch: 582 \t| Train loss: 0.208 \t| Train acc: 0.68 \t| Test acc: 0.58\n",
      "Epoch: 583 \t| Train loss: 0.208 \t| Train acc: 0.68 \t| Test acc: 0.58\n",
      "Epoch: 584 \t| Train loss: 0.208 \t| Train acc: 0.68 \t| Test acc: 0.58\n",
      "Epoch: 585 \t| Train loss: 0.208 \t| Train acc: 0.68 \t| Test acc: 0.58\n",
      "Epoch: 586 \t| Train loss: 0.208 \t| Train acc: 0.68 \t| Test acc: 0.58\n",
      "Epoch: 587 \t| Train loss: 0.208 \t| Train acc: 0.68 \t| Test acc: 0.58\n",
      "Epoch: 588 \t| Train loss: 0.208 \t| Train acc: 0.68 \t| Test acc: 0.58\n",
      "Epoch: 589 \t| Train loss: 0.208 \t| Train acc: 0.68 \t| Test acc: 0.58\n",
      "Epoch: 590 \t| Train loss: 0.208 \t| Train acc: 0.68 \t| Test acc: 0.58\n",
      "Epoch: 591 \t| Train loss: 0.208 \t| Train acc: 0.68 \t| Test acc: 0.58\n",
      "Epoch: 592 \t| Train loss: 0.208 \t| Train acc: 0.68 \t| Test acc: 0.58\n",
      "Epoch: 593 \t| Train loss: 0.208 \t| Train acc: 0.68 \t| Test acc: 0.58\n",
      "Epoch: 594 \t| Train loss: 0.208 \t| Train acc: 0.68 \t| Test acc: 0.58\n",
      "Epoch: 595 \t| Train loss: 0.208 \t| Train acc: 0.68 \t| Test acc: 0.58\n",
      "Epoch: 596 \t| Train loss: 0.208 \t| Train acc: 0.68 \t| Test acc: 0.58\n",
      "Epoch: 597 \t| Train loss: 0.208 \t| Train acc: 0.68 \t| Test acc: 0.58\n",
      "Epoch: 598 \t| Train loss: 0.208 \t| Train acc: 0.68 \t| Test acc: 0.58\n",
      "Epoch: 599 \t| Train loss: 0.208 \t| Train acc: 0.69 \t| Test acc: 0.58\n",
      "Epoch: 600 \t| Train loss: 0.208 \t| Train acc: 0.69 \t| Test acc: 0.58\n",
      "Epoch: 601 \t| Train loss: 0.208 \t| Train acc: 0.69 \t| Test acc: 0.58\n",
      "Epoch: 602 \t| Train loss: 0.208 \t| Train acc: 0.69 \t| Test acc: 0.58\n",
      "Epoch: 603 \t| Train loss: 0.208 \t| Train acc: 0.69 \t| Test acc: 0.58\n",
      "Epoch: 604 \t| Train loss: 0.208 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 605 \t| Train loss: 0.208 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 606 \t| Train loss: 0.208 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 607 \t| Train loss: 0.208 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 608 \t| Train loss: 0.208 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 609 \t| Train loss: 0.208 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 610 \t| Train loss: 0.208 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 611 \t| Train loss: 0.208 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 612 \t| Train loss: 0.208 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 613 \t| Train loss: 0.208 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 614 \t| Train loss: 0.208 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 615 \t| Train loss: 0.207 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 616 \t| Train loss: 0.207 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 617 \t| Train loss: 0.207 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 618 \t| Train loss: 0.207 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 619 \t| Train loss: 0.207 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 620 \t| Train loss: 0.207 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 621 \t| Train loss: 0.207 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 622 \t| Train loss: 0.207 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 623 \t| Train loss: 0.207 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 624 \t| Train loss: 0.207 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 625 \t| Train loss: 0.207 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 626 \t| Train loss: 0.207 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 627 \t| Train loss: 0.207 \t| Train acc: 0.69 \t| Test acc: 0.58\n",
      "Epoch: 628 \t| Train loss: 0.207 \t| Train acc: 0.69 \t| Test acc: 0.58\n",
      "Epoch: 629 \t| Train loss: 0.207 \t| Train acc: 0.69 \t| Test acc: 0.58\n",
      "Epoch: 630 \t| Train loss: 0.207 \t| Train acc: 0.69 \t| Test acc: 0.58\n",
      "Epoch: 631 \t| Train loss: 0.207 \t| Train acc: 0.69 \t| Test acc: 0.58\n",
      "Epoch: 632 \t| Train loss: 0.207 \t| Train acc: 0.69 \t| Test acc: 0.58\n",
      "Epoch: 633 \t| Train loss: 0.207 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 634 \t| Train loss: 0.207 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 635 \t| Train loss: 0.207 \t| Train acc: 0.69 \t| Test acc: 0.58\n",
      "Epoch: 636 \t| Train loss: 0.207 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 637 \t| Train loss: 0.207 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 638 \t| Train loss: 0.207 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 639 \t| Train loss: 0.207 \t| Train acc: 0.68 \t| Test acc: 0.57\n",
      "Epoch: 640 \t| Train loss: 0.207 \t| Train acc: 0.68 \t| Test acc: 0.57\n",
      "Epoch: 641 \t| Train loss: 0.207 \t| Train acc: 0.68 \t| Test acc: 0.57\n",
      "Epoch: 642 \t| Train loss: 0.207 \t| Train acc: 0.68 \t| Test acc: 0.57\n",
      "Epoch: 643 \t| Train loss: 0.207 \t| Train acc: 0.68 \t| Test acc: 0.57\n",
      "Epoch: 644 \t| Train loss: 0.207 \t| Train acc: 0.68 \t| Test acc: 0.57\n",
      "Epoch: 645 \t| Train loss: 0.207 \t| Train acc: 0.68 \t| Test acc: 0.57\n",
      "Epoch: 646 \t| Train loss: 0.207 \t| Train acc: 0.68 \t| Test acc: 0.57\n",
      "Epoch: 647 \t| Train loss: 0.207 \t| Train acc: 0.68 \t| Test acc: 0.57\n",
      "Epoch: 648 \t| Train loss: 0.207 \t| Train acc: 0.68 \t| Test acc: 0.57\n",
      "Epoch: 649 \t| Train loss: 0.206 \t| Train acc: 0.68 \t| Test acc: 0.58\n",
      "Epoch: 650 \t| Train loss: 0.206 \t| Train acc: 0.68 \t| Test acc: 0.58\n",
      "Epoch: 651 \t| Train loss: 0.206 \t| Train acc: 0.68 \t| Test acc: 0.58\n",
      "Epoch: 652 \t| Train loss: 0.206 \t| Train acc: 0.68 \t| Test acc: 0.58\n",
      "Epoch: 653 \t| Train loss: 0.206 \t| Train acc: 0.68 \t| Test acc: 0.58\n",
      "Epoch: 654 \t| Train loss: 0.206 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 655 \t| Train loss: 0.206 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 656 \t| Train loss: 0.206 \t| Train acc: 0.69 \t| Test acc: 0.58\n",
      "Epoch: 657 \t| Train loss: 0.206 \t| Train acc: 0.69 \t| Test acc: 0.58\n",
      "Epoch: 658 \t| Train loss: 0.206 \t| Train acc: 0.7 \t| Test acc: 0.57\n",
      "Epoch: 659 \t| Train loss: 0.206 \t| Train acc: 0.7 \t| Test acc: 0.57\n",
      "Epoch: 660 \t| Train loss: 0.206 \t| Train acc: 0.7 \t| Test acc: 0.57\n",
      "Epoch: 661 \t| Train loss: 0.206 \t| Train acc: 0.7 \t| Test acc: 0.57\n",
      "Epoch: 662 \t| Train loss: 0.206 \t| Train acc: 0.7 \t| Test acc: 0.57\n",
      "Epoch: 663 \t| Train loss: 0.206 \t| Train acc: 0.7 \t| Test acc: 0.57\n",
      "Epoch: 664 \t| Train loss: 0.206 \t| Train acc: 0.7 \t| Test acc: 0.57\n",
      "Epoch: 665 \t| Train loss: 0.206 \t| Train acc: 0.7 \t| Test acc: 0.57\n",
      "Epoch: 666 \t| Train loss: 0.206 \t| Train acc: 0.7 \t| Test acc: 0.57\n",
      "Epoch: 667 \t| Train loss: 0.206 \t| Train acc: 0.7 \t| Test acc: 0.57\n",
      "Epoch: 668 \t| Train loss: 0.205 \t| Train acc: 0.7 \t| Test acc: 0.57\n",
      "Epoch: 669 \t| Train loss: 0.205 \t| Train acc: 0.7 \t| Test acc: 0.57\n",
      "Epoch: 670 \t| Train loss: 0.205 \t| Train acc: 0.7 \t| Test acc: 0.57\n",
      "Epoch: 671 \t| Train loss: 0.205 \t| Train acc: 0.7 \t| Test acc: 0.57\n",
      "Epoch: 672 \t| Train loss: 0.205 \t| Train acc: 0.7 \t| Test acc: 0.57\n",
      "Epoch: 673 \t| Train loss: 0.205 \t| Train acc: 0.7 \t| Test acc: 0.57\n",
      "Epoch: 674 \t| Train loss: 0.205 \t| Train acc: 0.7 \t| Test acc: 0.57\n",
      "Epoch: 675 \t| Train loss: 0.205 \t| Train acc: 0.7 \t| Test acc: 0.57\n",
      "Epoch: 676 \t| Train loss: 0.205 \t| Train acc: 0.7 \t| Test acc: 0.57\n",
      "Epoch: 677 \t| Train loss: 0.205 \t| Train acc: 0.7 \t| Test acc: 0.57\n",
      "Epoch: 678 \t| Train loss: 0.205 \t| Train acc: 0.7 \t| Test acc: 0.57\n",
      "Epoch: 679 \t| Train loss: 0.205 \t| Train acc: 0.7 \t| Test acc: 0.57\n",
      "Epoch: 680 \t| Train loss: 0.205 \t| Train acc: 0.7 \t| Test acc: 0.57\n",
      "Epoch: 681 \t| Train loss: 0.205 \t| Train acc: 0.7 \t| Test acc: 0.57\n",
      "Epoch: 682 \t| Train loss: 0.205 \t| Train acc: 0.7 \t| Test acc: 0.57\n",
      "Epoch: 683 \t| Train loss: 0.205 \t| Train acc: 0.7 \t| Test acc: 0.57\n",
      "Epoch: 684 \t| Train loss: 0.205 \t| Train acc: 0.7 \t| Test acc: 0.57\n",
      "Epoch: 685 \t| Train loss: 0.205 \t| Train acc: 0.7 \t| Test acc: 0.57\n",
      "Epoch: 686 \t| Train loss: 0.205 \t| Train acc: 0.7 \t| Test acc: 0.57\n",
      "Epoch: 687 \t| Train loss: 0.205 \t| Train acc: 0.7 \t| Test acc: 0.57\n",
      "Epoch: 688 \t| Train loss: 0.205 \t| Train acc: 0.7 \t| Test acc: 0.57\n",
      "Epoch: 689 \t| Train loss: 0.205 \t| Train acc: 0.7 \t| Test acc: 0.57\n",
      "Epoch: 690 \t| Train loss: 0.205 \t| Train acc: 0.7 \t| Test acc: 0.57\n",
      "Epoch: 691 \t| Train loss: 0.205 \t| Train acc: 0.7 \t| Test acc: 0.57\n",
      "Epoch: 692 \t| Train loss: 0.205 \t| Train acc: 0.7 \t| Test acc: 0.57\n",
      "Epoch: 693 \t| Train loss: 0.205 \t| Train acc: 0.7 \t| Test acc: 0.57\n",
      "Epoch: 694 \t| Train loss: 0.205 \t| Train acc: 0.7 \t| Test acc: 0.57\n",
      "Epoch: 695 \t| Train loss: 0.204 \t| Train acc: 0.7 \t| Test acc: 0.57\n",
      "Epoch: 696 \t| Train loss: 0.204 \t| Train acc: 0.7 \t| Test acc: 0.57\n",
      "Epoch: 697 \t| Train loss: 0.204 \t| Train acc: 0.7 \t| Test acc: 0.57\n",
      "Epoch: 698 \t| Train loss: 0.204 \t| Train acc: 0.7 \t| Test acc: 0.57\n",
      "Epoch: 699 \t| Train loss: 0.204 \t| Train acc: 0.7 \t| Test acc: 0.57\n",
      "Epoch: 700 \t| Train loss: 0.204 \t| Train acc: 0.7 \t| Test acc: 0.57\n",
      "Epoch: 701 \t| Train loss: 0.204 \t| Train acc: 0.7 \t| Test acc: 0.57\n",
      "Epoch: 702 \t| Train loss: 0.204 \t| Train acc: 0.7 \t| Test acc: 0.57\n",
      "Epoch: 703 \t| Train loss: 0.204 \t| Train acc: 0.7 \t| Test acc: 0.57\n",
      "Epoch: 704 \t| Train loss: 0.204 \t| Train acc: 0.7 \t| Test acc: 0.57\n",
      "Epoch: 705 \t| Train loss: 0.204 \t| Train acc: 0.7 \t| Test acc: 0.57\n",
      "Epoch: 706 \t| Train loss: 0.204 \t| Train acc: 0.7 \t| Test acc: 0.57\n",
      "Epoch: 707 \t| Train loss: 0.204 \t| Train acc: 0.7 \t| Test acc: 0.57\n",
      "Epoch: 708 \t| Train loss: 0.204 \t| Train acc: 0.7 \t| Test acc: 0.57\n",
      "Epoch: 709 \t| Train loss: 0.204 \t| Train acc: 0.7 \t| Test acc: 0.57\n",
      "Epoch: 710 \t| Train loss: 0.204 \t| Train acc: 0.7 \t| Test acc: 0.57\n",
      "Epoch: 711 \t| Train loss: 0.204 \t| Train acc: 0.7 \t| Test acc: 0.57\n",
      "Epoch: 712 \t| Train loss: 0.204 \t| Train acc: 0.7 \t| Test acc: 0.57\n",
      "Epoch: 713 \t| Train loss: 0.204 \t| Train acc: 0.7 \t| Test acc: 0.57\n",
      "Epoch: 714 \t| Train loss: 0.204 \t| Train acc: 0.7 \t| Test acc: 0.57\n",
      "Epoch: 715 \t| Train loss: 0.204 \t| Train acc: 0.7 \t| Test acc: 0.57\n",
      "Epoch: 716 \t| Train loss: 0.204 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 717 \t| Train loss: 0.204 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 718 \t| Train loss: 0.204 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 719 \t| Train loss: 0.204 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 720 \t| Train loss: 0.204 \t| Train acc: 0.68 \t| Test acc: 0.57\n",
      "Epoch: 721 \t| Train loss: 0.204 \t| Train acc: 0.68 \t| Test acc: 0.57\n",
      "Epoch: 722 \t| Train loss: 0.204 \t| Train acc: 0.68 \t| Test acc: 0.57\n",
      "Epoch: 723 \t| Train loss: 0.204 \t| Train acc: 0.68 \t| Test acc: 0.57\n",
      "Epoch: 724 \t| Train loss: 0.203 \t| Train acc: 0.68 \t| Test acc: 0.57\n",
      "Epoch: 725 \t| Train loss: 0.203 \t| Train acc: 0.68 \t| Test acc: 0.57\n",
      "Epoch: 726 \t| Train loss: 0.203 \t| Train acc: 0.68 \t| Test acc: 0.57\n",
      "Epoch: 727 \t| Train loss: 0.203 \t| Train acc: 0.68 \t| Test acc: 0.57\n",
      "Epoch: 728 \t| Train loss: 0.203 \t| Train acc: 0.68 \t| Test acc: 0.57\n",
      "Epoch: 729 \t| Train loss: 0.203 \t| Train acc: 0.68 \t| Test acc: 0.57\n",
      "Epoch: 730 \t| Train loss: 0.203 \t| Train acc: 0.68 \t| Test acc: 0.57\n",
      "Epoch: 731 \t| Train loss: 0.203 \t| Train acc: 0.68 \t| Test acc: 0.57\n",
      "Epoch: 732 \t| Train loss: 0.203 \t| Train acc: 0.68 \t| Test acc: 0.57\n",
      "Epoch: 733 \t| Train loss: 0.203 \t| Train acc: 0.68 \t| Test acc: 0.57\n",
      "Epoch: 734 \t| Train loss: 0.203 \t| Train acc: 0.68 \t| Test acc: 0.57\n",
      "Epoch: 735 \t| Train loss: 0.203 \t| Train acc: 0.68 \t| Test acc: 0.57\n",
      "Epoch: 736 \t| Train loss: 0.203 \t| Train acc: 0.68 \t| Test acc: 0.57\n",
      "Epoch: 737 \t| Train loss: 0.203 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 738 \t| Train loss: 0.203 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 739 \t| Train loss: 0.203 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 740 \t| Train loss: 0.203 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 741 \t| Train loss: 0.203 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 742 \t| Train loss: 0.203 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 743 \t| Train loss: 0.203 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 744 \t| Train loss: 0.203 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 745 \t| Train loss: 0.203 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 746 \t| Train loss: 0.203 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 747 \t| Train loss: 0.202 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 748 \t| Train loss: 0.202 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 749 \t| Train loss: 0.202 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 750 \t| Train loss: 0.202 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 751 \t| Train loss: 0.202 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 752 \t| Train loss: 0.202 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 753 \t| Train loss: 0.202 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 754 \t| Train loss: 0.202 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 755 \t| Train loss: 0.202 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 756 \t| Train loss: 0.202 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 757 \t| Train loss: 0.202 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 758 \t| Train loss: 0.202 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 759 \t| Train loss: 0.202 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 760 \t| Train loss: 0.202 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 761 \t| Train loss: 0.202 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 762 \t| Train loss: 0.202 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 763 \t| Train loss: 0.202 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 764 \t| Train loss: 0.202 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 765 \t| Train loss: 0.202 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 766 \t| Train loss: 0.202 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 767 \t| Train loss: 0.202 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 768 \t| Train loss: 0.201 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 769 \t| Train loss: 0.201 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 770 \t| Train loss: 0.201 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 771 \t| Train loss: 0.201 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 772 \t| Train loss: 0.201 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 773 \t| Train loss: 0.201 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 774 \t| Train loss: 0.201 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 775 \t| Train loss: 0.201 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 776 \t| Train loss: 0.201 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 777 \t| Train loss: 0.201 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 778 \t| Train loss: 0.201 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 779 \t| Train loss: 0.201 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 780 \t| Train loss: 0.201 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 781 \t| Train loss: 0.201 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 782 \t| Train loss: 0.201 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 783 \t| Train loss: 0.201 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 784 \t| Train loss: 0.201 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 785 \t| Train loss: 0.201 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 786 \t| Train loss: 0.201 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 787 \t| Train loss: 0.201 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 788 \t| Train loss: 0.201 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 789 \t| Train loss: 0.201 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 790 \t| Train loss: 0.201 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 791 \t| Train loss: 0.2 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 792 \t| Train loss: 0.2 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 793 \t| Train loss: 0.2 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 794 \t| Train loss: 0.2 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 795 \t| Train loss: 0.2 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 796 \t| Train loss: 0.2 \t| Train acc: 0.7 \t| Test acc: 0.57\n",
      "Epoch: 797 \t| Train loss: 0.2 \t| Train acc: 0.7 \t| Test acc: 0.57\n",
      "Epoch: 798 \t| Train loss: 0.2 \t| Train acc: 0.7 \t| Test acc: 0.57\n",
      "Epoch: 799 \t| Train loss: 0.2 \t| Train acc: 0.7 \t| Test acc: 0.57\n",
      "Epoch: 800 \t| Train loss: 0.2 \t| Train acc: 0.7 \t| Test acc: 0.57\n",
      "Epoch: 801 \t| Train loss: 0.2 \t| Train acc: 0.7 \t| Test acc: 0.57\n",
      "Epoch: 802 \t| Train loss: 0.2 \t| Train acc: 0.7 \t| Test acc: 0.57\n",
      "Epoch: 803 \t| Train loss: 0.2 \t| Train acc: 0.7 \t| Test acc: 0.57\n",
      "Epoch: 804 \t| Train loss: 0.2 \t| Train acc: 0.7 \t| Test acc: 0.57\n",
      "Epoch: 805 \t| Train loss: 0.2 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 806 \t| Train loss: 0.2 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 807 \t| Train loss: 0.2 \t| Train acc: 0.69 \t| Test acc: 0.57\n",
      "Epoch: 808 \t| Train loss: 0.2 \t| Train acc: 0.69 \t| Test acc: 0.58\n",
      "Epoch: 809 \t| Train loss: 0.2 \t| Train acc: 0.69 \t| Test acc: 0.58\n",
      "Epoch: 810 \t| Train loss: 0.2 \t| Train acc: 0.69 \t| Test acc: 0.58\n",
      "Epoch: 811 \t| Train loss: 0.2 \t| Train acc: 0.69 \t| Test acc: 0.58\n",
      "Epoch: 812 \t| Train loss: 0.199 \t| Train acc: 0.69 \t| Test acc: 0.58\n",
      "Epoch: 813 \t| Train loss: 0.199 \t| Train acc: 0.69 \t| Test acc: 0.58\n",
      "Epoch: 814 \t| Train loss: 0.199 \t| Train acc: 0.69 \t| Test acc: 0.58\n",
      "Epoch: 815 \t| Train loss: 0.199 \t| Train acc: 0.69 \t| Test acc: 0.58\n",
      "Epoch: 816 \t| Train loss: 0.199 \t| Train acc: 0.69 \t| Test acc: 0.58\n",
      "Epoch: 817 \t| Train loss: 0.199 \t| Train acc: 0.69 \t| Test acc: 0.58\n",
      "Epoch: 818 \t| Train loss: 0.199 \t| Train acc: 0.69 \t| Test acc: 0.58\n",
      "Epoch: 819 \t| Train loss: 0.199 \t| Train acc: 0.69 \t| Test acc: 0.58\n",
      "Epoch: 820 \t| Train loss: 0.199 \t| Train acc: 0.69 \t| Test acc: 0.58\n",
      "Epoch: 821 \t| Train loss: 0.199 \t| Train acc: 0.69 \t| Test acc: 0.58\n",
      "Epoch: 822 \t| Train loss: 0.199 \t| Train acc: 0.69 \t| Test acc: 0.58\n",
      "Epoch: 823 \t| Train loss: 0.199 \t| Train acc: 0.69 \t| Test acc: 0.58\n",
      "Epoch: 824 \t| Train loss: 0.199 \t| Train acc: 0.69 \t| Test acc: 0.58\n",
      "Epoch: 825 \t| Train loss: 0.199 \t| Train acc: 0.69 \t| Test acc: 0.58\n",
      "Epoch: 826 \t| Train loss: 0.199 \t| Train acc: 0.69 \t| Test acc: 0.58\n",
      "Epoch: 827 \t| Train loss: 0.199 \t| Train acc: 0.69 \t| Test acc: 0.58\n",
      "Epoch: 828 \t| Train loss: 0.199 \t| Train acc: 0.69 \t| Test acc: 0.58\n",
      "Epoch: 829 \t| Train loss: 0.199 \t| Train acc: 0.69 \t| Test acc: 0.58\n",
      "Epoch: 830 \t| Train loss: 0.199 \t| Train acc: 0.69 \t| Test acc: 0.58\n",
      "Epoch: 831 \t| Train loss: 0.198 \t| Train acc: 0.69 \t| Test acc: 0.58\n",
      "Epoch: 832 \t| Train loss: 0.198 \t| Train acc: 0.69 \t| Test acc: 0.58\n",
      "Epoch: 833 \t| Train loss: 0.198 \t| Train acc: 0.69 \t| Test acc: 0.58\n",
      "Epoch: 834 \t| Train loss: 0.198 \t| Train acc: 0.69 \t| Test acc: 0.58\n",
      "Epoch: 835 \t| Train loss: 0.198 \t| Train acc: 0.69 \t| Test acc: 0.58\n",
      "Epoch: 836 \t| Train loss: 0.198 \t| Train acc: 0.69 \t| Test acc: 0.58\n",
      "Epoch: 837 \t| Train loss: 0.198 \t| Train acc: 0.69 \t| Test acc: 0.58\n",
      "Epoch: 838 \t| Train loss: 0.198 \t| Train acc: 0.69 \t| Test acc: 0.58\n",
      "Epoch: 839 \t| Train loss: 0.198 \t| Train acc: 0.69 \t| Test acc: 0.58\n",
      "Epoch: 840 \t| Train loss: 0.198 \t| Train acc: 0.69 \t| Test acc: 0.58\n",
      "Epoch: 841 \t| Train loss: 0.198 \t| Train acc: 0.69 \t| Test acc: 0.58\n",
      "Epoch: 842 \t| Train loss: 0.198 \t| Train acc: 0.69 \t| Test acc: 0.58\n",
      "Epoch: 843 \t| Train loss: 0.198 \t| Train acc: 0.69 \t| Test acc: 0.58\n",
      "Epoch: 844 \t| Train loss: 0.198 \t| Train acc: 0.69 \t| Test acc: 0.58\n",
      "Epoch: 845 \t| Train loss: 0.198 \t| Train acc: 0.7 \t| Test acc: 0.58\n",
      "Epoch: 846 \t| Train loss: 0.198 \t| Train acc: 0.7 \t| Test acc: 0.58\n",
      "Epoch: 847 \t| Train loss: 0.198 \t| Train acc: 0.7 \t| Test acc: 0.58\n",
      "Epoch: 848 \t| Train loss: 0.198 \t| Train acc: 0.7 \t| Test acc: 0.58\n",
      "Epoch: 849 \t| Train loss: 0.198 \t| Train acc: 0.7 \t| Test acc: 0.58\n",
      "Epoch: 850 \t| Train loss: 0.198 \t| Train acc: 0.7 \t| Test acc: 0.58\n",
      "Epoch: 851 \t| Train loss: 0.198 \t| Train acc: 0.7 \t| Test acc: 0.58\n",
      "Epoch: 852 \t| Train loss: 0.198 \t| Train acc: 0.7 \t| Test acc: 0.58\n",
      "Epoch: 853 \t| Train loss: 0.197 \t| Train acc: 0.7 \t| Test acc: 0.58\n",
      "Epoch: 854 \t| Train loss: 0.197 \t| Train acc: 0.7 \t| Test acc: 0.58\n",
      "Epoch: 855 \t| Train loss: 0.197 \t| Train acc: 0.7 \t| Test acc: 0.58\n",
      "Epoch: 856 \t| Train loss: 0.197 \t| Train acc: 0.7 \t| Test acc: 0.58\n",
      "Epoch: 857 \t| Train loss: 0.197 \t| Train acc: 0.7 \t| Test acc: 0.58\n",
      "Epoch: 858 \t| Train loss: 0.197 \t| Train acc: 0.7 \t| Test acc: 0.58\n",
      "Epoch: 859 \t| Train loss: 0.197 \t| Train acc: 0.7 \t| Test acc: 0.58\n",
      "Epoch: 860 \t| Train loss: 0.197 \t| Train acc: 0.7 \t| Test acc: 0.58\n",
      "Epoch: 861 \t| Train loss: 0.197 \t| Train acc: 0.7 \t| Test acc: 0.58\n",
      "Epoch: 862 \t| Train loss: 0.197 \t| Train acc: 0.7 \t| Test acc: 0.58\n",
      "Epoch: 863 \t| Train loss: 0.197 \t| Train acc: 0.7 \t| Test acc: 0.58\n",
      "Epoch: 864 \t| Train loss: 0.197 \t| Train acc: 0.7 \t| Test acc: 0.58\n",
      "Epoch: 865 \t| Train loss: 0.197 \t| Train acc: 0.7 \t| Test acc: 0.59\n",
      "Epoch: 866 \t| Train loss: 0.197 \t| Train acc: 0.7 \t| Test acc: 0.59\n",
      "Epoch: 867 \t| Train loss: 0.197 \t| Train acc: 0.7 \t| Test acc: 0.59\n",
      "Epoch: 868 \t| Train loss: 0.197 \t| Train acc: 0.7 \t| Test acc: 0.59\n",
      "Epoch: 869 \t| Train loss: 0.197 \t| Train acc: 0.7 \t| Test acc: 0.59\n",
      "Epoch: 870 \t| Train loss: 0.197 \t| Train acc: 0.7 \t| Test acc: 0.59\n",
      "Epoch: 871 \t| Train loss: 0.197 \t| Train acc: 0.7 \t| Test acc: 0.59\n",
      "Epoch: 872 \t| Train loss: 0.197 \t| Train acc: 0.7 \t| Test acc: 0.59\n",
      "Epoch: 873 \t| Train loss: 0.197 \t| Train acc: 0.7 \t| Test acc: 0.59\n",
      "Epoch: 874 \t| Train loss: 0.197 \t| Train acc: 0.7 \t| Test acc: 0.59\n",
      "Epoch: 875 \t| Train loss: 0.197 \t| Train acc: 0.7 \t| Test acc: 0.59\n",
      "Epoch: 876 \t| Train loss: 0.197 \t| Train acc: 0.7 \t| Test acc: 0.59\n",
      "Epoch: 877 \t| Train loss: 0.197 \t| Train acc: 0.7 \t| Test acc: 0.59\n",
      "Epoch: 878 \t| Train loss: 0.197 \t| Train acc: 0.7 \t| Test acc: 0.59\n",
      "Epoch: 879 \t| Train loss: 0.197 \t| Train acc: 0.7 \t| Test acc: 0.59\n",
      "Epoch: 880 \t| Train loss: 0.197 \t| Train acc: 0.7 \t| Test acc: 0.59\n",
      "Epoch: 881 \t| Train loss: 0.197 \t| Train acc: 0.7 \t| Test acc: 0.59\n",
      "Epoch: 882 \t| Train loss: 0.197 \t| Train acc: 0.7 \t| Test acc: 0.59\n",
      "Epoch: 883 \t| Train loss: 0.197 \t| Train acc: 0.7 \t| Test acc: 0.59\n",
      "Epoch: 884 \t| Train loss: 0.197 \t| Train acc: 0.7 \t| Test acc: 0.59\n",
      "Epoch: 885 \t| Train loss: 0.197 \t| Train acc: 0.7 \t| Test acc: 0.59\n",
      "Epoch: 886 \t| Train loss: 0.196 \t| Train acc: 0.7 \t| Test acc: 0.59\n",
      "Epoch: 887 \t| Train loss: 0.196 \t| Train acc: 0.7 \t| Test acc: 0.59\n",
      "Epoch: 888 \t| Train loss: 0.196 \t| Train acc: 0.7 \t| Test acc: 0.59\n",
      "Epoch: 889 \t| Train loss: 0.196 \t| Train acc: 0.7 \t| Test acc: 0.59\n",
      "Epoch: 890 \t| Train loss: 0.196 \t| Train acc: 0.7 \t| Test acc: 0.59\n",
      "Epoch: 891 \t| Train loss: 0.196 \t| Train acc: 0.7 \t| Test acc: 0.59\n",
      "Epoch: 892 \t| Train loss: 0.196 \t| Train acc: 0.7 \t| Test acc: 0.59\n",
      "Epoch: 893 \t| Train loss: 0.196 \t| Train acc: 0.7 \t| Test acc: 0.59\n",
      "Epoch: 894 \t| Train loss: 0.196 \t| Train acc: 0.7 \t| Test acc: 0.59\n",
      "Epoch: 895 \t| Train loss: 0.196 \t| Train acc: 0.7 \t| Test acc: 0.59\n",
      "Epoch: 896 \t| Train loss: 0.196 \t| Train acc: 0.7 \t| Test acc: 0.59\n",
      "Epoch: 897 \t| Train loss: 0.196 \t| Train acc: 0.7 \t| Test acc: 0.59\n",
      "Epoch: 898 \t| Train loss: 0.196 \t| Train acc: 0.7 \t| Test acc: 0.59\n",
      "Epoch: 899 \t| Train loss: 0.196 \t| Train acc: 0.7 \t| Test acc: 0.59\n",
      "Epoch: 900 \t| Train loss: 0.196 \t| Train acc: 0.7 \t| Test acc: 0.59\n",
      "Epoch: 901 \t| Train loss: 0.196 \t| Train acc: 0.7 \t| Test acc: 0.59\n",
      "Epoch: 902 \t| Train loss: 0.196 \t| Train acc: 0.7 \t| Test acc: 0.59\n",
      "Epoch: 903 \t| Train loss: 0.196 \t| Train acc: 0.7 \t| Test acc: 0.59\n",
      "Epoch: 904 \t| Train loss: 0.196 \t| Train acc: 0.7 \t| Test acc: 0.59\n",
      "Epoch: 905 \t| Train loss: 0.196 \t| Train acc: 0.7 \t| Test acc: 0.59\n",
      "Epoch: 906 \t| Train loss: 0.196 \t| Train acc: 0.7 \t| Test acc: 0.59\n",
      "Epoch: 907 \t| Train loss: 0.196 \t| Train acc: 0.7 \t| Test acc: 0.59\n",
      "Epoch: 908 \t| Train loss: 0.196 \t| Train acc: 0.7 \t| Test acc: 0.59\n",
      "Epoch: 909 \t| Train loss: 0.196 \t| Train acc: 0.7 \t| Test acc: 0.59\n",
      "Epoch: 910 \t| Train loss: 0.196 \t| Train acc: 0.7 \t| Test acc: 0.59\n",
      "Epoch: 911 \t| Train loss: 0.196 \t| Train acc: 0.7 \t| Test acc: 0.59\n",
      "Epoch: 912 \t| Train loss: 0.196 \t| Train acc: 0.7 \t| Test acc: 0.59\n",
      "Epoch: 913 \t| Train loss: 0.196 \t| Train acc: 0.7 \t| Test acc: 0.59\n",
      "Epoch: 914 \t| Train loss: 0.196 \t| Train acc: 0.7 \t| Test acc: 0.59\n",
      "Epoch: 915 \t| Train loss: 0.196 \t| Train acc: 0.7 \t| Test acc: 0.59\n",
      "Epoch: 916 \t| Train loss: 0.196 \t| Train acc: 0.7 \t| Test acc: 0.59\n",
      "Epoch: 917 \t| Train loss: 0.196 \t| Train acc: 0.7 \t| Test acc: 0.59\n",
      "Epoch: 918 \t| Train loss: 0.196 \t| Train acc: 0.7 \t| Test acc: 0.59\n",
      "Epoch: 919 \t| Train loss: 0.196 \t| Train acc: 0.7 \t| Test acc: 0.59\n",
      "Epoch: 920 \t| Train loss: 0.196 \t| Train acc: 0.7 \t| Test acc: 0.59\n",
      "Epoch: 921 \t| Train loss: 0.196 \t| Train acc: 0.71 \t| Test acc: 0.59\n",
      "Epoch: 922 \t| Train loss: 0.196 \t| Train acc: 0.71 \t| Test acc: 0.59\n",
      "Epoch: 923 \t| Train loss: 0.196 \t| Train acc: 0.71 \t| Test acc: 0.59\n",
      "Epoch: 924 \t| Train loss: 0.196 \t| Train acc: 0.71 \t| Test acc: 0.59\n",
      "Epoch: 925 \t| Train loss: 0.196 \t| Train acc: 0.71 \t| Test acc: 0.58\n",
      "Epoch: 926 \t| Train loss: 0.196 \t| Train acc: 0.71 \t| Test acc: 0.59\n",
      "Epoch: 927 \t| Train loss: 0.196 \t| Train acc: 0.71 \t| Test acc: 0.59\n",
      "Epoch: 928 \t| Train loss: 0.196 \t| Train acc: 0.71 \t| Test acc: 0.59\n",
      "Epoch: 929 \t| Train loss: 0.195 \t| Train acc: 0.71 \t| Test acc: 0.58\n",
      "Epoch: 930 \t| Train loss: 0.195 \t| Train acc: 0.71 \t| Test acc: 0.58\n",
      "Epoch: 931 \t| Train loss: 0.195 \t| Train acc: 0.71 \t| Test acc: 0.58\n",
      "Epoch: 932 \t| Train loss: 0.195 \t| Train acc: 0.71 \t| Test acc: 0.58\n",
      "Epoch: 933 \t| Train loss: 0.195 \t| Train acc: 0.71 \t| Test acc: 0.58\n",
      "Epoch: 934 \t| Train loss: 0.195 \t| Train acc: 0.71 \t| Test acc: 0.58\n",
      "Epoch: 935 \t| Train loss: 0.195 \t| Train acc: 0.71 \t| Test acc: 0.58\n",
      "Epoch: 936 \t| Train loss: 0.195 \t| Train acc: 0.71 \t| Test acc: 0.58\n",
      "Epoch: 937 \t| Train loss: 0.195 \t| Train acc: 0.71 \t| Test acc: 0.58\n",
      "Epoch: 938 \t| Train loss: 0.195 \t| Train acc: 0.71 \t| Test acc: 0.58\n",
      "Epoch: 939 \t| Train loss: 0.195 \t| Train acc: 0.71 \t| Test acc: 0.58\n",
      "Epoch: 940 \t| Train loss: 0.195 \t| Train acc: 0.71 \t| Test acc: 0.58\n",
      "Epoch: 941 \t| Train loss: 0.195 \t| Train acc: 0.71 \t| Test acc: 0.58\n",
      "Epoch: 942 \t| Train loss: 0.195 \t| Train acc: 0.71 \t| Test acc: 0.58\n",
      "Epoch: 943 \t| Train loss: 0.195 \t| Train acc: 0.71 \t| Test acc: 0.58\n",
      "Epoch: 944 \t| Train loss: 0.195 \t| Train acc: 0.71 \t| Test acc: 0.58\n",
      "Epoch: 945 \t| Train loss: 0.195 \t| Train acc: 0.71 \t| Test acc: 0.58\n",
      "Epoch: 946 \t| Train loss: 0.195 \t| Train acc: 0.71 \t| Test acc: 0.58\n",
      "Epoch: 947 \t| Train loss: 0.195 \t| Train acc: 0.71 \t| Test acc: 0.58\n",
      "Epoch: 948 \t| Train loss: 0.195 \t| Train acc: 0.71 \t| Test acc: 0.58\n",
      "Epoch: 949 \t| Train loss: 0.195 \t| Train acc: 0.71 \t| Test acc: 0.58\n",
      "Epoch: 950 \t| Train loss: 0.195 \t| Train acc: 0.71 \t| Test acc: 0.58\n",
      "Epoch: 951 \t| Train loss: 0.195 \t| Train acc: 0.71 \t| Test acc: 0.58\n",
      "Epoch: 952 \t| Train loss: 0.195 \t| Train acc: 0.71 \t| Test acc: 0.58\n",
      "Epoch: 953 \t| Train loss: 0.195 \t| Train acc: 0.71 \t| Test acc: 0.58\n",
      "Epoch: 954 \t| Train loss: 0.195 \t| Train acc: 0.71 \t| Test acc: 0.58\n",
      "Epoch: 955 \t| Train loss: 0.195 \t| Train acc: 0.71 \t| Test acc: 0.58\n",
      "Epoch: 956 \t| Train loss: 0.195 \t| Train acc: 0.71 \t| Test acc: 0.58\n",
      "Epoch: 957 \t| Train loss: 0.195 \t| Train acc: 0.71 \t| Test acc: 0.58\n",
      "Epoch: 958 \t| Train loss: 0.195 \t| Train acc: 0.71 \t| Test acc: 0.58\n",
      "Epoch: 959 \t| Train loss: 0.195 \t| Train acc: 0.71 \t| Test acc: 0.58\n",
      "Epoch: 960 \t| Train loss: 0.195 \t| Train acc: 0.71 \t| Test acc: 0.58\n",
      "Epoch: 961 \t| Train loss: 0.195 \t| Train acc: 0.71 \t| Test acc: 0.58\n",
      "Epoch: 962 \t| Train loss: 0.195 \t| Train acc: 0.71 \t| Test acc: 0.58\n",
      "Epoch: 963 \t| Train loss: 0.195 \t| Train acc: 0.71 \t| Test acc: 0.58\n",
      "Epoch: 964 \t| Train loss: 0.195 \t| Train acc: 0.71 \t| Test acc: 0.58\n",
      "Epoch: 965 \t| Train loss: 0.195 \t| Train acc: 0.71 \t| Test acc: 0.58\n",
      "Epoch: 966 \t| Train loss: 0.195 \t| Train acc: 0.71 \t| Test acc: 0.58\n",
      "Epoch: 967 \t| Train loss: 0.195 \t| Train acc: 0.71 \t| Test acc: 0.58\n",
      "Epoch: 968 \t| Train loss: 0.195 \t| Train acc: 0.71 \t| Test acc: 0.58\n",
      "Epoch: 969 \t| Train loss: 0.195 \t| Train acc: 0.71 \t| Test acc: 0.58\n",
      "Epoch: 970 \t| Train loss: 0.195 \t| Train acc: 0.71 \t| Test acc: 0.58\n",
      "Epoch: 971 \t| Train loss: 0.195 \t| Train acc: 0.71 \t| Test acc: 0.58\n",
      "Epoch: 972 \t| Train loss: 0.195 \t| Train acc: 0.71 \t| Test acc: 0.58\n",
      "Epoch: 973 \t| Train loss: 0.195 \t| Train acc: 0.71 \t| Test acc: 0.58\n",
      "Epoch: 974 \t| Train loss: 0.195 \t| Train acc: 0.71 \t| Test acc: 0.58\n",
      "Epoch: 975 \t| Train loss: 0.195 \t| Train acc: 0.71 \t| Test acc: 0.58\n",
      "Epoch: 976 \t| Train loss: 0.195 \t| Train acc: 0.71 \t| Test acc: 0.58\n",
      "Epoch: 977 \t| Train loss: 0.194 \t| Train acc: 0.71 \t| Test acc: 0.58\n",
      "Epoch: 978 \t| Train loss: 0.194 \t| Train acc: 0.71 \t| Test acc: 0.58\n",
      "Epoch: 979 \t| Train loss: 0.194 \t| Train acc: 0.71 \t| Test acc: 0.58\n",
      "Epoch: 980 \t| Train loss: 0.194 \t| Train acc: 0.71 \t| Test acc: 0.58\n",
      "Epoch: 981 \t| Train loss: 0.194 \t| Train acc: 0.71 \t| Test acc: 0.58\n",
      "Epoch: 982 \t| Train loss: 0.194 \t| Train acc: 0.71 \t| Test acc: 0.58\n",
      "Epoch: 983 \t| Train loss: 0.194 \t| Train acc: 0.71 \t| Test acc: 0.58\n",
      "Epoch: 984 \t| Train loss: 0.194 \t| Train acc: 0.71 \t| Test acc: 0.58\n",
      "Epoch: 985 \t| Train loss: 0.194 \t| Train acc: 0.71 \t| Test acc: 0.59\n",
      "Epoch: 986 \t| Train loss: 0.194 \t| Train acc: 0.71 \t| Test acc: 0.59\n",
      "Epoch: 987 \t| Train loss: 0.194 \t| Train acc: 0.71 \t| Test acc: 0.59\n",
      "Epoch: 988 \t| Train loss: 0.194 \t| Train acc: 0.71 \t| Test acc: 0.59\n",
      "Epoch: 989 \t| Train loss: 0.194 \t| Train acc: 0.71 \t| Test acc: 0.59\n",
      "Epoch: 990 \t| Train loss: 0.194 \t| Train acc: 0.71 \t| Test acc: 0.59\n",
      "Epoch: 991 \t| Train loss: 0.194 \t| Train acc: 0.71 \t| Test acc: 0.59\n",
      "Epoch: 992 \t| Train loss: 0.194 \t| Train acc: 0.71 \t| Test acc: 0.59\n",
      "Epoch: 993 \t| Train loss: 0.194 \t| Train acc: 0.71 \t| Test acc: 0.59\n",
      "Epoch: 994 \t| Train loss: 0.194 \t| Train acc: 0.71 \t| Test acc: 0.59\n",
      "Epoch: 995 \t| Train loss: 0.194 \t| Train acc: 0.71 \t| Test acc: 0.59\n",
      "Epoch: 996 \t| Train loss: 0.194 \t| Train acc: 0.71 \t| Test acc: 0.59\n",
      "Epoch: 997 \t| Train loss: 0.194 \t| Train acc: 0.71 \t| Test acc: 0.59\n",
      "Epoch: 998 \t| Train loss: 0.194 \t| Train acc: 0.71 \t| Test acc: 0.59\n",
      "Epoch: 999 \t| Train loss: 0.194 \t| Train acc: 0.71 \t| Test acc: 0.59\n"
     ]
    }
   ],
   "source": [
    "train_losses_mlp_lim = []\n",
    "#test_losses  = []\n",
    "train_accs_mlp_lim = []\n",
    "test_accs_mlp_lim  = []\n",
    "\n",
    "for epoch in range(1000):\n",
    "    for features, labels in data_loader_lim:\n",
    "        train_preds_mlp_lim = mlp_model_lim(features)\n",
    "        class_weights_batch = class_weights_original[labels.long()]\n",
    "        loss_function = nn.BCELoss(weight=class_weights_batch)\n",
    "\n",
    "        train_loss_mlp_lim  = loss_function(train_preds_mlp_lim, labels)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            test_preds_mlp_lim = mlp_model_lim(X_validation_tensor_lim)\n",
    "            #test_loss  = loss_function(test_preds, y_test)\n",
    "\n",
    "        train_acc = calculate_accuracy(train_preds_mlp_lim, labels)\n",
    "        test_acc  = calculate_accuracy(test_preds_mlp_lim, y_validation_tensor)\n",
    "\n",
    "        optimizer_mlp_lim.zero_grad()\n",
    "        train_loss_mlp_lim.backward()\n",
    "\n",
    "        optimizer_mlp_lim.step()\n",
    "\n",
    "        train_losses_mlp_lim.append(train_loss_mlp_lim.item())\n",
    "        #test_losses.append(test_loss.item())\n",
    "        train_accs_mlp_lim.append(train_acc.item())\n",
    "        test_accs_mlp_lim.append(test_acc.item())\n",
    "\n",
    "    #if epoch%100==0:\n",
    "    print(f'Epoch: {epoch} \\t|' \\\n",
    "          f' Train loss: {np.round(train_loss_mlp_lim.item(),3)} \\t|' \\\n",
    "              #f' Test loss: {np.round(test_loss.item(),3)} \\t|' \\\n",
    "          f' Train acc: {np.round(train_acc.item(),2)} \\t|' \\\n",
    "          f' Test acc: {np.round(test_acc.item(),2)}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-22T16:28:51.228664900Z",
     "start_time": "2024-06-22T16:28:44.189960700Z"
    }
   },
   "id": "6318082f7d63ea1c"
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Result:\n",
      "================================================\n",
      "Accuracy Score: 58.67%\n",
      "_______________________________________________\n",
      "CLASSIFICATION REPORT:\n",
      "                   0.0         1.0  accuracy    macro avg  weighted avg\n",
      "precision     0.799896    0.282949  0.586736     0.541422      0.678793\n",
      "recall        0.613873    0.498034  0.586736     0.555954      0.586736\n",
      "f1-score      0.694646    0.360874  0.586736     0.527760      0.616455\n",
      "support    2494.000000  763.000000  0.586736  3257.000000   3257.000000\n",
      "_______________________________________________\n",
      "Confusion Matrix: \n",
      " [[1531  963]\n",
      " [ 383  380]]\n"
     ]
    }
   ],
   "source": [
    "evaluate_nn(y_validation_tensor.clone().detach(), test_preds_mlp_lim.clone().detach().round(), train=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-22T16:28:53.205280Z",
     "start_time": "2024-06-22T16:28:52.998487400Z"
    }
   },
   "id": "d8cd464e64eb3c08"
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Result:\n",
      "================================================\n",
      "Accuracy Score: 56.95%\n",
      "_______________________________________________\n",
      "CLASSIFICATION REPORT:\n",
      "                   0.0         1.0  accuracy    macro avg  weighted avg\n",
      "precision     0.768330    0.279018  0.569528     0.523674      0.645558\n",
      "recall        0.608959    0.451807  0.569528     0.530383      0.569528\n",
      "f1-score      0.679424    0.344986  0.569528     0.512205      0.595511\n",
      "support    2478.000000  830.000000  0.569528  3308.000000   3308.000000\n",
      "_______________________________________________\n",
      "Confusion Matrix: \n",
      " [[1509  969]\n",
      " [ 455  375]]\n"
     ]
    }
   ],
   "source": [
    "test_preds_mlp_lim = mlp_model_lim(X_test_tensor_lim)\n",
    "evaluate_nn(y_test_tensor.clone().detach(), test_preds_mlp_lim.clone().detach().round(), train=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-22T16:28:55.944092100Z",
     "start_time": "2024-06-22T16:28:55.800917800Z"
    }
   },
   "id": "350a03ac2c7768d5"
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "outputs": [
    {
     "data": {
      "text/plain": "==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nMLP                                      [115, 1]                  --\n├─Sequential: 1-1                        [115, 1]                  --\n│    └─Linear: 2-1                       [115, 3]                  21\n│    └─ReLU: 2-2                         [115, 3]                  --\n│    └─Linear: 2-3                       [115, 1]                  4\n│    └─Sigmoid: 2-4                      [115, 1]                  --\n==========================================================================================\nTotal params: 25\nTrainable params: 25\nNon-trainable params: 0\nTotal mult-adds (Units.MEGABYTES): 0.00\n==========================================================================================\nInput size (MB): 0.00\nForward/backward pass size (MB): 0.00\nParams size (MB): 0.00\nEstimated Total Size (MB): 0.01\n=========================================================================================="
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlpsl_model_lim = MLP(num_features=6)\n",
    "summary(mlpsl_model_lim, input_size=X_train_lim.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-22T16:48:09.731572Z",
     "start_time": "2024-06-22T16:48:09.605657700Z"
    }
   },
   "id": "bd79e53f6061b768"
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "outputs": [],
   "source": [
    "optimizer_mlpsl_lim = optim.Adam(mlpsl_model_lim.parameters())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-22T16:48:10.447580200Z",
     "start_time": "2024-06-22T16:48:10.440658500Z"
    }
   },
   "id": "288be49c8e18972a"
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \t| Train loss: 0.227 \t| Train acc: 0.23 \t| Test acc: 0.28\n",
      "Epoch: 1 \t| Train loss: 0.227 \t| Train acc: 0.23 \t| Test acc: 0.28\n",
      "Epoch: 2 \t| Train loss: 0.226 \t| Train acc: 0.24 \t| Test acc: 0.28\n",
      "Epoch: 3 \t| Train loss: 0.226 \t| Train acc: 0.24 \t| Test acc: 0.28\n",
      "Epoch: 4 \t| Train loss: 0.226 \t| Train acc: 0.24 \t| Test acc: 0.28\n",
      "Epoch: 5 \t| Train loss: 0.226 \t| Train acc: 0.24 \t| Test acc: 0.28\n",
      "Epoch: 6 \t| Train loss: 0.226 \t| Train acc: 0.24 \t| Test acc: 0.28\n",
      "Epoch: 7 \t| Train loss: 0.226 \t| Train acc: 0.24 \t| Test acc: 0.28\n",
      "Epoch: 8 \t| Train loss: 0.226 \t| Train acc: 0.23 \t| Test acc: 0.28\n",
      "Epoch: 9 \t| Train loss: 0.225 \t| Train acc: 0.23 \t| Test acc: 0.28\n",
      "Epoch: 10 \t| Train loss: 0.225 \t| Train acc: 0.23 \t| Test acc: 0.28\n",
      "Epoch: 11 \t| Train loss: 0.225 \t| Train acc: 0.24 \t| Test acc: 0.28\n",
      "Epoch: 12 \t| Train loss: 0.225 \t| Train acc: 0.24 \t| Test acc: 0.28\n",
      "Epoch: 13 \t| Train loss: 0.225 \t| Train acc: 0.24 \t| Test acc: 0.28\n",
      "Epoch: 14 \t| Train loss: 0.225 \t| Train acc: 0.24 \t| Test acc: 0.28\n",
      "Epoch: 15 \t| Train loss: 0.224 \t| Train acc: 0.24 \t| Test acc: 0.27\n",
      "Epoch: 16 \t| Train loss: 0.224 \t| Train acc: 0.23 \t| Test acc: 0.27\n",
      "Epoch: 17 \t| Train loss: 0.224 \t| Train acc: 0.23 \t| Test acc: 0.27\n",
      "Epoch: 18 \t| Train loss: 0.224 \t| Train acc: 0.23 \t| Test acc: 0.27\n",
      "Epoch: 19 \t| Train loss: 0.224 \t| Train acc: 0.23 \t| Test acc: 0.27\n",
      "Epoch: 20 \t| Train loss: 0.224 \t| Train acc: 0.23 \t| Test acc: 0.27\n",
      "Epoch: 21 \t| Train loss: 0.224 \t| Train acc: 0.23 \t| Test acc: 0.27\n",
      "Epoch: 22 \t| Train loss: 0.223 \t| Train acc: 0.23 \t| Test acc: 0.27\n",
      "Epoch: 23 \t| Train loss: 0.223 \t| Train acc: 0.23 \t| Test acc: 0.27\n",
      "Epoch: 24 \t| Train loss: 0.223 \t| Train acc: 0.23 \t| Test acc: 0.27\n",
      "Epoch: 25 \t| Train loss: 0.223 \t| Train acc: 0.24 \t| Test acc: 0.27\n",
      "Epoch: 26 \t| Train loss: 0.223 \t| Train acc: 0.24 \t| Test acc: 0.27\n",
      "Epoch: 27 \t| Train loss: 0.223 \t| Train acc: 0.24 \t| Test acc: 0.27\n",
      "Epoch: 28 \t| Train loss: 0.223 \t| Train acc: 0.24 \t| Test acc: 0.27\n",
      "Epoch: 29 \t| Train loss: 0.223 \t| Train acc: 0.24 \t| Test acc: 0.27\n",
      "Epoch: 30 \t| Train loss: 0.222 \t| Train acc: 0.24 \t| Test acc: 0.27\n",
      "Epoch: 31 \t| Train loss: 0.222 \t| Train acc: 0.24 \t| Test acc: 0.27\n",
      "Epoch: 32 \t| Train loss: 0.222 \t| Train acc: 0.24 \t| Test acc: 0.26\n",
      "Epoch: 33 \t| Train loss: 0.222 \t| Train acc: 0.24 \t| Test acc: 0.26\n",
      "Epoch: 34 \t| Train loss: 0.222 \t| Train acc: 0.24 \t| Test acc: 0.26\n",
      "Epoch: 35 \t| Train loss: 0.222 \t| Train acc: 0.24 \t| Test acc: 0.26\n",
      "Epoch: 36 \t| Train loss: 0.222 \t| Train acc: 0.23 \t| Test acc: 0.26\n",
      "Epoch: 37 \t| Train loss: 0.222 \t| Train acc: 0.23 \t| Test acc: 0.26\n",
      "Epoch: 38 \t| Train loss: 0.221 \t| Train acc: 0.23 \t| Test acc: 0.26\n",
      "Epoch: 39 \t| Train loss: 0.221 \t| Train acc: 0.23 \t| Test acc: 0.26\n",
      "Epoch: 40 \t| Train loss: 0.221 \t| Train acc: 0.24 \t| Test acc: 0.26\n",
      "Epoch: 41 \t| Train loss: 0.221 \t| Train acc: 0.24 \t| Test acc: 0.26\n",
      "Epoch: 42 \t| Train loss: 0.221 \t| Train acc: 0.24 \t| Test acc: 0.26\n",
      "Epoch: 43 \t| Train loss: 0.221 \t| Train acc: 0.24 \t| Test acc: 0.26\n",
      "Epoch: 44 \t| Train loss: 0.221 \t| Train acc: 0.24 \t| Test acc: 0.26\n",
      "Epoch: 45 \t| Train loss: 0.221 \t| Train acc: 0.24 \t| Test acc: 0.26\n",
      "Epoch: 46 \t| Train loss: 0.22 \t| Train acc: 0.24 \t| Test acc: 0.26\n",
      "Epoch: 47 \t| Train loss: 0.22 \t| Train acc: 0.24 \t| Test acc: 0.26\n",
      "Epoch: 48 \t| Train loss: 0.22 \t| Train acc: 0.24 \t| Test acc: 0.26\n",
      "Epoch: 49 \t| Train loss: 0.22 \t| Train acc: 0.24 \t| Test acc: 0.26\n",
      "Epoch: 50 \t| Train loss: 0.22 \t| Train acc: 0.24 \t| Test acc: 0.26\n",
      "Epoch: 51 \t| Train loss: 0.22 \t| Train acc: 0.24 \t| Test acc: 0.26\n",
      "Epoch: 52 \t| Train loss: 0.22 \t| Train acc: 0.24 \t| Test acc: 0.26\n",
      "Epoch: 53 \t| Train loss: 0.22 \t| Train acc: 0.24 \t| Test acc: 0.25\n",
      "Epoch: 54 \t| Train loss: 0.22 \t| Train acc: 0.24 \t| Test acc: 0.25\n",
      "Epoch: 55 \t| Train loss: 0.219 \t| Train acc: 0.24 \t| Test acc: 0.25\n",
      "Epoch: 56 \t| Train loss: 0.219 \t| Train acc: 0.24 \t| Test acc: 0.25\n",
      "Epoch: 57 \t| Train loss: 0.219 \t| Train acc: 0.24 \t| Test acc: 0.25\n",
      "Epoch: 58 \t| Train loss: 0.219 \t| Train acc: 0.25 \t| Test acc: 0.25\n",
      "Epoch: 59 \t| Train loss: 0.219 \t| Train acc: 0.25 \t| Test acc: 0.25\n",
      "Epoch: 60 \t| Train loss: 0.219 \t| Train acc: 0.25 \t| Test acc: 0.25\n",
      "Epoch: 61 \t| Train loss: 0.219 \t| Train acc: 0.25 \t| Test acc: 0.25\n",
      "Epoch: 62 \t| Train loss: 0.219 \t| Train acc: 0.25 \t| Test acc: 0.25\n",
      "Epoch: 63 \t| Train loss: 0.219 \t| Train acc: 0.25 \t| Test acc: 0.25\n",
      "Epoch: 64 \t| Train loss: 0.219 \t| Train acc: 0.24 \t| Test acc: 0.25\n",
      "Epoch: 65 \t| Train loss: 0.218 \t| Train acc: 0.24 \t| Test acc: 0.25\n",
      "Epoch: 66 \t| Train loss: 0.218 \t| Train acc: 0.24 \t| Test acc: 0.25\n",
      "Epoch: 67 \t| Train loss: 0.218 \t| Train acc: 0.24 \t| Test acc: 0.25\n",
      "Epoch: 68 \t| Train loss: 0.218 \t| Train acc: 0.24 \t| Test acc: 0.25\n",
      "Epoch: 69 \t| Train loss: 0.218 \t| Train acc: 0.24 \t| Test acc: 0.25\n",
      "Epoch: 70 \t| Train loss: 0.218 \t| Train acc: 0.24 \t| Test acc: 0.25\n",
      "Epoch: 71 \t| Train loss: 0.218 \t| Train acc: 0.24 \t| Test acc: 0.25\n",
      "Epoch: 72 \t| Train loss: 0.218 \t| Train acc: 0.25 \t| Test acc: 0.25\n",
      "Epoch: 73 \t| Train loss: 0.218 \t| Train acc: 0.25 \t| Test acc: 0.25\n",
      "Epoch: 74 \t| Train loss: 0.218 \t| Train acc: 0.26 \t| Test acc: 0.25\n",
      "Epoch: 75 \t| Train loss: 0.218 \t| Train acc: 0.26 \t| Test acc: 0.25\n",
      "Epoch: 76 \t| Train loss: 0.217 \t| Train acc: 0.26 \t| Test acc: 0.25\n",
      "Epoch: 77 \t| Train loss: 0.217 \t| Train acc: 0.26 \t| Test acc: 0.25\n",
      "Epoch: 78 \t| Train loss: 0.217 \t| Train acc: 0.26 \t| Test acc: 0.25\n",
      "Epoch: 79 \t| Train loss: 0.217 \t| Train acc: 0.26 \t| Test acc: 0.25\n",
      "Epoch: 80 \t| Train loss: 0.217 \t| Train acc: 0.25 \t| Test acc: 0.25\n",
      "Epoch: 81 \t| Train loss: 0.217 \t| Train acc: 0.25 \t| Test acc: 0.25\n",
      "Epoch: 82 \t| Train loss: 0.217 \t| Train acc: 0.25 \t| Test acc: 0.25\n",
      "Epoch: 83 \t| Train loss: 0.217 \t| Train acc: 0.25 \t| Test acc: 0.25\n",
      "Epoch: 84 \t| Train loss: 0.217 \t| Train acc: 0.25 \t| Test acc: 0.25\n",
      "Epoch: 85 \t| Train loss: 0.217 \t| Train acc: 0.25 \t| Test acc: 0.25\n",
      "Epoch: 86 \t| Train loss: 0.217 \t| Train acc: 0.25 \t| Test acc: 0.25\n",
      "Epoch: 87 \t| Train loss: 0.217 \t| Train acc: 0.25 \t| Test acc: 0.25\n",
      "Epoch: 88 \t| Train loss: 0.217 \t| Train acc: 0.25 \t| Test acc: 0.25\n",
      "Epoch: 89 \t| Train loss: 0.216 \t| Train acc: 0.26 \t| Test acc: 0.25\n",
      "Epoch: 90 \t| Train loss: 0.216 \t| Train acc: 0.26 \t| Test acc: 0.24\n",
      "Epoch: 91 \t| Train loss: 0.216 \t| Train acc: 0.26 \t| Test acc: 0.24\n",
      "Epoch: 92 \t| Train loss: 0.216 \t| Train acc: 0.26 \t| Test acc: 0.24\n",
      "Epoch: 93 \t| Train loss: 0.216 \t| Train acc: 0.25 \t| Test acc: 0.24\n",
      "Epoch: 94 \t| Train loss: 0.216 \t| Train acc: 0.25 \t| Test acc: 0.24\n",
      "Epoch: 95 \t| Train loss: 0.216 \t| Train acc: 0.25 \t| Test acc: 0.24\n",
      "Epoch: 96 \t| Train loss: 0.216 \t| Train acc: 0.25 \t| Test acc: 0.24\n",
      "Epoch: 97 \t| Train loss: 0.216 \t| Train acc: 0.25 \t| Test acc: 0.24\n",
      "Epoch: 98 \t| Train loss: 0.216 \t| Train acc: 0.25 \t| Test acc: 0.24\n",
      "Epoch: 99 \t| Train loss: 0.216 \t| Train acc: 0.25 \t| Test acc: 0.24\n",
      "Epoch: 100 \t| Train loss: 0.216 \t| Train acc: 0.25 \t| Test acc: 0.24\n",
      "Epoch: 101 \t| Train loss: 0.216 \t| Train acc: 0.25 \t| Test acc: 0.24\n",
      "Epoch: 102 \t| Train loss: 0.215 \t| Train acc: 0.25 \t| Test acc: 0.24\n",
      "Epoch: 103 \t| Train loss: 0.215 \t| Train acc: 0.25 \t| Test acc: 0.24\n",
      "Epoch: 104 \t| Train loss: 0.215 \t| Train acc: 0.25 \t| Test acc: 0.24\n",
      "Epoch: 105 \t| Train loss: 0.215 \t| Train acc: 0.25 \t| Test acc: 0.24\n",
      "Epoch: 106 \t| Train loss: 0.215 \t| Train acc: 0.25 \t| Test acc: 0.24\n",
      "Epoch: 107 \t| Train loss: 0.215 \t| Train acc: 0.25 \t| Test acc: 0.24\n",
      "Epoch: 108 \t| Train loss: 0.215 \t| Train acc: 0.25 \t| Test acc: 0.24\n",
      "Epoch: 109 \t| Train loss: 0.215 \t| Train acc: 0.25 \t| Test acc: 0.24\n",
      "Epoch: 110 \t| Train loss: 0.215 \t| Train acc: 0.25 \t| Test acc: 0.24\n",
      "Epoch: 111 \t| Train loss: 0.215 \t| Train acc: 0.25 \t| Test acc: 0.24\n",
      "Epoch: 112 \t| Train loss: 0.215 \t| Train acc: 0.25 \t| Test acc: 0.24\n",
      "Epoch: 113 \t| Train loss: 0.215 \t| Train acc: 0.25 \t| Test acc: 0.24\n",
      "Epoch: 114 \t| Train loss: 0.215 \t| Train acc: 0.25 \t| Test acc: 0.24\n",
      "Epoch: 115 \t| Train loss: 0.214 \t| Train acc: 0.25 \t| Test acc: 0.24\n",
      "Epoch: 116 \t| Train loss: 0.214 \t| Train acc: 0.25 \t| Test acc: 0.24\n",
      "Epoch: 117 \t| Train loss: 0.214 \t| Train acc: 0.25 \t| Test acc: 0.24\n",
      "Epoch: 118 \t| Train loss: 0.214 \t| Train acc: 0.25 \t| Test acc: 0.24\n",
      "Epoch: 119 \t| Train loss: 0.214 \t| Train acc: 0.25 \t| Test acc: 0.24\n",
      "Epoch: 120 \t| Train loss: 0.214 \t| Train acc: 0.25 \t| Test acc: 0.24\n",
      "Epoch: 121 \t| Train loss: 0.214 \t| Train acc: 0.25 \t| Test acc: 0.24\n",
      "Epoch: 122 \t| Train loss: 0.214 \t| Train acc: 0.25 \t| Test acc: 0.24\n",
      "Epoch: 123 \t| Train loss: 0.214 \t| Train acc: 0.25 \t| Test acc: 0.24\n",
      "Epoch: 124 \t| Train loss: 0.214 \t| Train acc: 0.25 \t| Test acc: 0.24\n",
      "Epoch: 125 \t| Train loss: 0.214 \t| Train acc: 0.25 \t| Test acc: 0.24\n",
      "Epoch: 126 \t| Train loss: 0.214 \t| Train acc: 0.25 \t| Test acc: 0.24\n",
      "Epoch: 127 \t| Train loss: 0.214 \t| Train acc: 0.25 \t| Test acc: 0.24\n",
      "Epoch: 128 \t| Train loss: 0.213 \t| Train acc: 0.25 \t| Test acc: 0.24\n",
      "Epoch: 129 \t| Train loss: 0.213 \t| Train acc: 0.25 \t| Test acc: 0.24\n",
      "Epoch: 130 \t| Train loss: 0.213 \t| Train acc: 0.25 \t| Test acc: 0.24\n",
      "Epoch: 131 \t| Train loss: 0.213 \t| Train acc: 0.25 \t| Test acc: 0.24\n",
      "Epoch: 132 \t| Train loss: 0.213 \t| Train acc: 0.25 \t| Test acc: 0.24\n",
      "Epoch: 133 \t| Train loss: 0.213 \t| Train acc: 0.25 \t| Test acc: 0.24\n",
      "Epoch: 134 \t| Train loss: 0.213 \t| Train acc: 0.25 \t| Test acc: 0.24\n",
      "Epoch: 135 \t| Train loss: 0.213 \t| Train acc: 0.24 \t| Test acc: 0.24\n",
      "Epoch: 136 \t| Train loss: 0.213 \t| Train acc: 0.24 \t| Test acc: 0.24\n",
      "Epoch: 137 \t| Train loss: 0.213 \t| Train acc: 0.24 \t| Test acc: 0.24\n",
      "Epoch: 138 \t| Train loss: 0.213 \t| Train acc: 0.24 \t| Test acc: 0.24\n",
      "Epoch: 139 \t| Train loss: 0.213 \t| Train acc: 0.24 \t| Test acc: 0.24\n",
      "Epoch: 140 \t| Train loss: 0.212 \t| Train acc: 0.25 \t| Test acc: 0.24\n",
      "Epoch: 141 \t| Train loss: 0.212 \t| Train acc: 0.25 \t| Test acc: 0.24\n",
      "Epoch: 142 \t| Train loss: 0.212 \t| Train acc: 0.25 \t| Test acc: 0.24\n",
      "Epoch: 143 \t| Train loss: 0.212 \t| Train acc: 0.25 \t| Test acc: 0.24\n",
      "Epoch: 144 \t| Train loss: 0.212 \t| Train acc: 0.25 \t| Test acc: 0.24\n",
      "Epoch: 145 \t| Train loss: 0.212 \t| Train acc: 0.26 \t| Test acc: 0.24\n",
      "Epoch: 146 \t| Train loss: 0.212 \t| Train acc: 0.27 \t| Test acc: 0.24\n",
      "Epoch: 147 \t| Train loss: 0.212 \t| Train acc: 0.27 \t| Test acc: 0.24\n",
      "Epoch: 148 \t| Train loss: 0.212 \t| Train acc: 0.28 \t| Test acc: 0.24\n",
      "Epoch: 149 \t| Train loss: 0.212 \t| Train acc: 0.28 \t| Test acc: 0.25\n",
      "Epoch: 150 \t| Train loss: 0.211 \t| Train acc: 0.29 \t| Test acc: 0.25\n",
      "Epoch: 151 \t| Train loss: 0.211 \t| Train acc: 0.28 \t| Test acc: 0.25\n",
      "Epoch: 152 \t| Train loss: 0.211 \t| Train acc: 0.28 \t| Test acc: 0.25\n",
      "Epoch: 153 \t| Train loss: 0.211 \t| Train acc: 0.29 \t| Test acc: 0.25\n",
      "Epoch: 154 \t| Train loss: 0.211 \t| Train acc: 0.3 \t| Test acc: 0.25\n",
      "Epoch: 155 \t| Train loss: 0.211 \t| Train acc: 0.3 \t| Test acc: 0.25\n",
      "Epoch: 156 \t| Train loss: 0.211 \t| Train acc: 0.31 \t| Test acc: 0.25\n",
      "Epoch: 157 \t| Train loss: 0.211 \t| Train acc: 0.31 \t| Test acc: 0.25\n",
      "Epoch: 158 \t| Train loss: 0.211 \t| Train acc: 0.31 \t| Test acc: 0.26\n",
      "Epoch: 159 \t| Train loss: 0.211 \t| Train acc: 0.31 \t| Test acc: 0.26\n",
      "Epoch: 160 \t| Train loss: 0.211 \t| Train acc: 0.33 \t| Test acc: 0.26\n",
      "Epoch: 161 \t| Train loss: 0.21 \t| Train acc: 0.33 \t| Test acc: 0.26\n",
      "Epoch: 162 \t| Train loss: 0.21 \t| Train acc: 0.33 \t| Test acc: 0.26\n",
      "Epoch: 163 \t| Train loss: 0.21 \t| Train acc: 0.33 \t| Test acc: 0.26\n",
      "Epoch: 164 \t| Train loss: 0.21 \t| Train acc: 0.33 \t| Test acc: 0.26\n",
      "Epoch: 165 \t| Train loss: 0.21 \t| Train acc: 0.33 \t| Test acc: 0.26\n",
      "Epoch: 166 \t| Train loss: 0.21 \t| Train acc: 0.34 \t| Test acc: 0.26\n",
      "Epoch: 167 \t| Train loss: 0.21 \t| Train acc: 0.34 \t| Test acc: 0.27\n",
      "Epoch: 168 \t| Train loss: 0.21 \t| Train acc: 0.35 \t| Test acc: 0.27\n",
      "Epoch: 169 \t| Train loss: 0.21 \t| Train acc: 0.35 \t| Test acc: 0.27\n",
      "Epoch: 170 \t| Train loss: 0.21 \t| Train acc: 0.35 \t| Test acc: 0.27\n",
      "Epoch: 171 \t| Train loss: 0.209 \t| Train acc: 0.35 \t| Test acc: 0.27\n",
      "Epoch: 172 \t| Train loss: 0.209 \t| Train acc: 0.35 \t| Test acc: 0.28\n",
      "Epoch: 173 \t| Train loss: 0.209 \t| Train acc: 0.35 \t| Test acc: 0.28\n",
      "Epoch: 174 \t| Train loss: 0.209 \t| Train acc: 0.35 \t| Test acc: 0.28\n",
      "Epoch: 175 \t| Train loss: 0.209 \t| Train acc: 0.35 \t| Test acc: 0.28\n",
      "Epoch: 176 \t| Train loss: 0.209 \t| Train acc: 0.36 \t| Test acc: 0.28\n",
      "Epoch: 177 \t| Train loss: 0.209 \t| Train acc: 0.35 \t| Test acc: 0.28\n",
      "Epoch: 178 \t| Train loss: 0.209 \t| Train acc: 0.36 \t| Test acc: 0.29\n",
      "Epoch: 179 \t| Train loss: 0.209 \t| Train acc: 0.37 \t| Test acc: 0.29\n",
      "Epoch: 180 \t| Train loss: 0.209 \t| Train acc: 0.37 \t| Test acc: 0.29\n",
      "Epoch: 181 \t| Train loss: 0.209 \t| Train acc: 0.37 \t| Test acc: 0.3\n",
      "Epoch: 182 \t| Train loss: 0.208 \t| Train acc: 0.39 \t| Test acc: 0.3\n",
      "Epoch: 183 \t| Train loss: 0.208 \t| Train acc: 0.39 \t| Test acc: 0.3\n",
      "Epoch: 184 \t| Train loss: 0.208 \t| Train acc: 0.4 \t| Test acc: 0.3\n",
      "Epoch: 185 \t| Train loss: 0.208 \t| Train acc: 0.41 \t| Test acc: 0.3\n",
      "Epoch: 186 \t| Train loss: 0.208 \t| Train acc: 0.41 \t| Test acc: 0.3\n",
      "Epoch: 187 \t| Train loss: 0.208 \t| Train acc: 0.41 \t| Test acc: 0.31\n",
      "Epoch: 188 \t| Train loss: 0.208 \t| Train acc: 0.41 \t| Test acc: 0.31\n",
      "Epoch: 189 \t| Train loss: 0.208 \t| Train acc: 0.41 \t| Test acc: 0.31\n",
      "Epoch: 190 \t| Train loss: 0.208 \t| Train acc: 0.41 \t| Test acc: 0.31\n",
      "Epoch: 191 \t| Train loss: 0.208 \t| Train acc: 0.41 \t| Test acc: 0.31\n",
      "Epoch: 192 \t| Train loss: 0.208 \t| Train acc: 0.41 \t| Test acc: 0.31\n",
      "Epoch: 193 \t| Train loss: 0.207 \t| Train acc: 0.42 \t| Test acc: 0.31\n",
      "Epoch: 194 \t| Train loss: 0.207 \t| Train acc: 0.42 \t| Test acc: 0.31\n",
      "Epoch: 195 \t| Train loss: 0.207 \t| Train acc: 0.42 \t| Test acc: 0.32\n",
      "Epoch: 196 \t| Train loss: 0.207 \t| Train acc: 0.42 \t| Test acc: 0.32\n",
      "Epoch: 197 \t| Train loss: 0.207 \t| Train acc: 0.42 \t| Test acc: 0.32\n",
      "Epoch: 198 \t| Train loss: 0.207 \t| Train acc: 0.42 \t| Test acc: 0.32\n",
      "Epoch: 199 \t| Train loss: 0.207 \t| Train acc: 0.43 \t| Test acc: 0.32\n",
      "Epoch: 200 \t| Train loss: 0.207 \t| Train acc: 0.43 \t| Test acc: 0.32\n",
      "Epoch: 201 \t| Train loss: 0.207 \t| Train acc: 0.43 \t| Test acc: 0.32\n",
      "Epoch: 202 \t| Train loss: 0.207 \t| Train acc: 0.43 \t| Test acc: 0.33\n",
      "Epoch: 203 \t| Train loss: 0.207 \t| Train acc: 0.43 \t| Test acc: 0.33\n",
      "Epoch: 204 \t| Train loss: 0.207 \t| Train acc: 0.43 \t| Test acc: 0.33\n",
      "Epoch: 205 \t| Train loss: 0.206 \t| Train acc: 0.43 \t| Test acc: 0.33\n",
      "Epoch: 206 \t| Train loss: 0.206 \t| Train acc: 0.43 \t| Test acc: 0.33\n",
      "Epoch: 207 \t| Train loss: 0.206 \t| Train acc: 0.43 \t| Test acc: 0.33\n",
      "Epoch: 208 \t| Train loss: 0.206 \t| Train acc: 0.43 \t| Test acc: 0.33\n",
      "Epoch: 209 \t| Train loss: 0.206 \t| Train acc: 0.43 \t| Test acc: 0.33\n",
      "Epoch: 210 \t| Train loss: 0.206 \t| Train acc: 0.43 \t| Test acc: 0.33\n",
      "Epoch: 211 \t| Train loss: 0.206 \t| Train acc: 0.43 \t| Test acc: 0.33\n",
      "Epoch: 212 \t| Train loss: 0.206 \t| Train acc: 0.43 \t| Test acc: 0.33\n",
      "Epoch: 213 \t| Train loss: 0.206 \t| Train acc: 0.43 \t| Test acc: 0.33\n",
      "Epoch: 214 \t| Train loss: 0.206 \t| Train acc: 0.43 \t| Test acc: 0.34\n",
      "Epoch: 215 \t| Train loss: 0.206 \t| Train acc: 0.43 \t| Test acc: 0.34\n",
      "Epoch: 216 \t| Train loss: 0.205 \t| Train acc: 0.44 \t| Test acc: 0.34\n",
      "Epoch: 217 \t| Train loss: 0.205 \t| Train acc: 0.44 \t| Test acc: 0.34\n",
      "Epoch: 218 \t| Train loss: 0.205 \t| Train acc: 0.44 \t| Test acc: 0.34\n",
      "Epoch: 219 \t| Train loss: 0.205 \t| Train acc: 0.44 \t| Test acc: 0.34\n",
      "Epoch: 220 \t| Train loss: 0.205 \t| Train acc: 0.44 \t| Test acc: 0.34\n",
      "Epoch: 221 \t| Train loss: 0.205 \t| Train acc: 0.44 \t| Test acc: 0.34\n",
      "Epoch: 222 \t| Train loss: 0.205 \t| Train acc: 0.44 \t| Test acc: 0.34\n",
      "Epoch: 223 \t| Train loss: 0.205 \t| Train acc: 0.44 \t| Test acc: 0.34\n",
      "Epoch: 224 \t| Train loss: 0.205 \t| Train acc: 0.44 \t| Test acc: 0.35\n",
      "Epoch: 225 \t| Train loss: 0.205 \t| Train acc: 0.44 \t| Test acc: 0.35\n",
      "Epoch: 226 \t| Train loss: 0.205 \t| Train acc: 0.45 \t| Test acc: 0.35\n",
      "Epoch: 227 \t| Train loss: 0.204 \t| Train acc: 0.45 \t| Test acc: 0.35\n",
      "Epoch: 228 \t| Train loss: 0.204 \t| Train acc: 0.45 \t| Test acc: 0.35\n",
      "Epoch: 229 \t| Train loss: 0.204 \t| Train acc: 0.45 \t| Test acc: 0.35\n",
      "Epoch: 230 \t| Train loss: 0.204 \t| Train acc: 0.44 \t| Test acc: 0.35\n",
      "Epoch: 231 \t| Train loss: 0.204 \t| Train acc: 0.44 \t| Test acc: 0.35\n",
      "Epoch: 232 \t| Train loss: 0.204 \t| Train acc: 0.44 \t| Test acc: 0.36\n",
      "Epoch: 233 \t| Train loss: 0.204 \t| Train acc: 0.44 \t| Test acc: 0.36\n",
      "Epoch: 234 \t| Train loss: 0.204 \t| Train acc: 0.44 \t| Test acc: 0.36\n",
      "Epoch: 235 \t| Train loss: 0.204 \t| Train acc: 0.44 \t| Test acc: 0.36\n",
      "Epoch: 236 \t| Train loss: 0.204 \t| Train acc: 0.44 \t| Test acc: 0.36\n",
      "Epoch: 237 \t| Train loss: 0.204 \t| Train acc: 0.44 \t| Test acc: 0.36\n",
      "Epoch: 238 \t| Train loss: 0.203 \t| Train acc: 0.44 \t| Test acc: 0.36\n",
      "Epoch: 239 \t| Train loss: 0.203 \t| Train acc: 0.44 \t| Test acc: 0.36\n",
      "Epoch: 240 \t| Train loss: 0.203 \t| Train acc: 0.44 \t| Test acc: 0.36\n",
      "Epoch: 241 \t| Train loss: 0.203 \t| Train acc: 0.44 \t| Test acc: 0.36\n",
      "Epoch: 242 \t| Train loss: 0.203 \t| Train acc: 0.44 \t| Test acc: 0.36\n",
      "Epoch: 243 \t| Train loss: 0.203 \t| Train acc: 0.44 \t| Test acc: 0.36\n",
      "Epoch: 244 \t| Train loss: 0.203 \t| Train acc: 0.44 \t| Test acc: 0.36\n",
      "Epoch: 245 \t| Train loss: 0.203 \t| Train acc: 0.44 \t| Test acc: 0.36\n",
      "Epoch: 246 \t| Train loss: 0.203 \t| Train acc: 0.44 \t| Test acc: 0.36\n",
      "Epoch: 247 \t| Train loss: 0.203 \t| Train acc: 0.44 \t| Test acc: 0.36\n",
      "Epoch: 248 \t| Train loss: 0.202 \t| Train acc: 0.45 \t| Test acc: 0.37\n",
      "Epoch: 249 \t| Train loss: 0.202 \t| Train acc: 0.45 \t| Test acc: 0.37\n",
      "Epoch: 250 \t| Train loss: 0.202 \t| Train acc: 0.45 \t| Test acc: 0.37\n",
      "Epoch: 251 \t| Train loss: 0.202 \t| Train acc: 0.45 \t| Test acc: 0.37\n",
      "Epoch: 252 \t| Train loss: 0.202 \t| Train acc: 0.45 \t| Test acc: 0.37\n",
      "Epoch: 253 \t| Train loss: 0.202 \t| Train acc: 0.45 \t| Test acc: 0.37\n",
      "Epoch: 254 \t| Train loss: 0.202 \t| Train acc: 0.45 \t| Test acc: 0.37\n",
      "Epoch: 255 \t| Train loss: 0.202 \t| Train acc: 0.45 \t| Test acc: 0.37\n",
      "Epoch: 256 \t| Train loss: 0.202 \t| Train acc: 0.45 \t| Test acc: 0.37\n",
      "Epoch: 257 \t| Train loss: 0.202 \t| Train acc: 0.45 \t| Test acc: 0.37\n",
      "Epoch: 258 \t| Train loss: 0.202 \t| Train acc: 0.45 \t| Test acc: 0.37\n",
      "Epoch: 259 \t| Train loss: 0.201 \t| Train acc: 0.45 \t| Test acc: 0.37\n",
      "Epoch: 260 \t| Train loss: 0.201 \t| Train acc: 0.45 \t| Test acc: 0.38\n",
      "Epoch: 261 \t| Train loss: 0.201 \t| Train acc: 0.45 \t| Test acc: 0.38\n",
      "Epoch: 262 \t| Train loss: 0.201 \t| Train acc: 0.45 \t| Test acc: 0.38\n",
      "Epoch: 263 \t| Train loss: 0.201 \t| Train acc: 0.45 \t| Test acc: 0.38\n",
      "Epoch: 264 \t| Train loss: 0.201 \t| Train acc: 0.45 \t| Test acc: 0.38\n",
      "Epoch: 265 \t| Train loss: 0.201 \t| Train acc: 0.45 \t| Test acc: 0.38\n",
      "Epoch: 266 \t| Train loss: 0.201 \t| Train acc: 0.45 \t| Test acc: 0.38\n",
      "Epoch: 267 \t| Train loss: 0.201 \t| Train acc: 0.45 \t| Test acc: 0.38\n",
      "Epoch: 268 \t| Train loss: 0.201 \t| Train acc: 0.46 \t| Test acc: 0.38\n",
      "Epoch: 269 \t| Train loss: 0.201 \t| Train acc: 0.46 \t| Test acc: 0.38\n",
      "Epoch: 270 \t| Train loss: 0.201 \t| Train acc: 0.46 \t| Test acc: 0.38\n",
      "Epoch: 271 \t| Train loss: 0.2 \t| Train acc: 0.46 \t| Test acc: 0.38\n",
      "Epoch: 272 \t| Train loss: 0.2 \t| Train acc: 0.46 \t| Test acc: 0.39\n",
      "Epoch: 273 \t| Train loss: 0.2 \t| Train acc: 0.46 \t| Test acc: 0.39\n",
      "Epoch: 274 \t| Train loss: 0.2 \t| Train acc: 0.46 \t| Test acc: 0.39\n",
      "Epoch: 275 \t| Train loss: 0.2 \t| Train acc: 0.46 \t| Test acc: 0.39\n",
      "Epoch: 276 \t| Train loss: 0.2 \t| Train acc: 0.46 \t| Test acc: 0.39\n",
      "Epoch: 277 \t| Train loss: 0.2 \t| Train acc: 0.46 \t| Test acc: 0.39\n",
      "Epoch: 278 \t| Train loss: 0.2 \t| Train acc: 0.46 \t| Test acc: 0.39\n",
      "Epoch: 279 \t| Train loss: 0.2 \t| Train acc: 0.46 \t| Test acc: 0.39\n",
      "Epoch: 280 \t| Train loss: 0.2 \t| Train acc: 0.46 \t| Test acc: 0.39\n",
      "Epoch: 281 \t| Train loss: 0.2 \t| Train acc: 0.46 \t| Test acc: 0.39\n",
      "Epoch: 282 \t| Train loss: 0.2 \t| Train acc: 0.46 \t| Test acc: 0.39\n",
      "Epoch: 283 \t| Train loss: 0.199 \t| Train acc: 0.47 \t| Test acc: 0.4\n",
      "Epoch: 284 \t| Train loss: 0.199 \t| Train acc: 0.47 \t| Test acc: 0.4\n",
      "Epoch: 285 \t| Train loss: 0.199 \t| Train acc: 0.47 \t| Test acc: 0.4\n",
      "Epoch: 286 \t| Train loss: 0.199 \t| Train acc: 0.47 \t| Test acc: 0.4\n",
      "Epoch: 287 \t| Train loss: 0.199 \t| Train acc: 0.48 \t| Test acc: 0.4\n",
      "Epoch: 288 \t| Train loss: 0.199 \t| Train acc: 0.48 \t| Test acc: 0.4\n",
      "Epoch: 289 \t| Train loss: 0.199 \t| Train acc: 0.49 \t| Test acc: 0.4\n",
      "Epoch: 290 \t| Train loss: 0.199 \t| Train acc: 0.49 \t| Test acc: 0.4\n",
      "Epoch: 291 \t| Train loss: 0.199 \t| Train acc: 0.49 \t| Test acc: 0.4\n",
      "Epoch: 292 \t| Train loss: 0.199 \t| Train acc: 0.5 \t| Test acc: 0.4\n",
      "Epoch: 293 \t| Train loss: 0.199 \t| Train acc: 0.5 \t| Test acc: 0.4\n",
      "Epoch: 294 \t| Train loss: 0.199 \t| Train acc: 0.5 \t| Test acc: 0.41\n",
      "Epoch: 295 \t| Train loss: 0.198 \t| Train acc: 0.5 \t| Test acc: 0.41\n",
      "Epoch: 296 \t| Train loss: 0.198 \t| Train acc: 0.5 \t| Test acc: 0.41\n",
      "Epoch: 297 \t| Train loss: 0.198 \t| Train acc: 0.51 \t| Test acc: 0.41\n",
      "Epoch: 298 \t| Train loss: 0.198 \t| Train acc: 0.51 \t| Test acc: 0.41\n",
      "Epoch: 299 \t| Train loss: 0.198 \t| Train acc: 0.51 \t| Test acc: 0.41\n",
      "Epoch: 300 \t| Train loss: 0.198 \t| Train acc: 0.51 \t| Test acc: 0.41\n",
      "Epoch: 301 \t| Train loss: 0.198 \t| Train acc: 0.52 \t| Test acc: 0.42\n",
      "Epoch: 302 \t| Train loss: 0.198 \t| Train acc: 0.52 \t| Test acc: 0.42\n",
      "Epoch: 303 \t| Train loss: 0.198 \t| Train acc: 0.52 \t| Test acc: 0.42\n",
      "Epoch: 304 \t| Train loss: 0.198 \t| Train acc: 0.52 \t| Test acc: 0.42\n",
      "Epoch: 305 \t| Train loss: 0.198 \t| Train acc: 0.52 \t| Test acc: 0.42\n",
      "Epoch: 306 \t| Train loss: 0.198 \t| Train acc: 0.52 \t| Test acc: 0.42\n",
      "Epoch: 307 \t| Train loss: 0.198 \t| Train acc: 0.52 \t| Test acc: 0.42\n",
      "Epoch: 308 \t| Train loss: 0.197 \t| Train acc: 0.53 \t| Test acc: 0.42\n",
      "Epoch: 309 \t| Train loss: 0.197 \t| Train acc: 0.53 \t| Test acc: 0.42\n",
      "Epoch: 310 \t| Train loss: 0.197 \t| Train acc: 0.53 \t| Test acc: 0.42\n",
      "Epoch: 311 \t| Train loss: 0.197 \t| Train acc: 0.53 \t| Test acc: 0.42\n",
      "Epoch: 312 \t| Train loss: 0.197 \t| Train acc: 0.54 \t| Test acc: 0.42\n",
      "Epoch: 313 \t| Train loss: 0.197 \t| Train acc: 0.54 \t| Test acc: 0.42\n",
      "Epoch: 314 \t| Train loss: 0.197 \t| Train acc: 0.54 \t| Test acc: 0.43\n",
      "Epoch: 315 \t| Train loss: 0.197 \t| Train acc: 0.54 \t| Test acc: 0.43\n",
      "Epoch: 316 \t| Train loss: 0.197 \t| Train acc: 0.54 \t| Test acc: 0.43\n",
      "Epoch: 317 \t| Train loss: 0.197 \t| Train acc: 0.55 \t| Test acc: 0.43\n",
      "Epoch: 318 \t| Train loss: 0.197 \t| Train acc: 0.55 \t| Test acc: 0.43\n",
      "Epoch: 319 \t| Train loss: 0.197 \t| Train acc: 0.55 \t| Test acc: 0.43\n",
      "Epoch: 320 \t| Train loss: 0.197 \t| Train acc: 0.55 \t| Test acc: 0.43\n",
      "Epoch: 321 \t| Train loss: 0.196 \t| Train acc: 0.55 \t| Test acc: 0.43\n",
      "Epoch: 322 \t| Train loss: 0.196 \t| Train acc: 0.55 \t| Test acc: 0.43\n",
      "Epoch: 323 \t| Train loss: 0.196 \t| Train acc: 0.55 \t| Test acc: 0.44\n",
      "Epoch: 324 \t| Train loss: 0.196 \t| Train acc: 0.55 \t| Test acc: 0.44\n",
      "Epoch: 325 \t| Train loss: 0.196 \t| Train acc: 0.55 \t| Test acc: 0.44\n",
      "Epoch: 326 \t| Train loss: 0.196 \t| Train acc: 0.55 \t| Test acc: 0.44\n",
      "Epoch: 327 \t| Train loss: 0.196 \t| Train acc: 0.55 \t| Test acc: 0.44\n",
      "Epoch: 328 \t| Train loss: 0.196 \t| Train acc: 0.53 \t| Test acc: 0.44\n",
      "Epoch: 329 \t| Train loss: 0.196 \t| Train acc: 0.53 \t| Test acc: 0.44\n",
      "Epoch: 330 \t| Train loss: 0.196 \t| Train acc: 0.54 \t| Test acc: 0.44\n",
      "Epoch: 331 \t| Train loss: 0.196 \t| Train acc: 0.54 \t| Test acc: 0.44\n",
      "Epoch: 332 \t| Train loss: 0.196 \t| Train acc: 0.54 \t| Test acc: 0.44\n",
      "Epoch: 333 \t| Train loss: 0.195 \t| Train acc: 0.54 \t| Test acc: 0.44\n",
      "Epoch: 334 \t| Train loss: 0.195 \t| Train acc: 0.54 \t| Test acc: 0.44\n",
      "Epoch: 335 \t| Train loss: 0.195 \t| Train acc: 0.54 \t| Test acc: 0.44\n",
      "Epoch: 336 \t| Train loss: 0.195 \t| Train acc: 0.54 \t| Test acc: 0.45\n",
      "Epoch: 337 \t| Train loss: 0.195 \t| Train acc: 0.54 \t| Test acc: 0.45\n",
      "Epoch: 338 \t| Train loss: 0.195 \t| Train acc: 0.55 \t| Test acc: 0.45\n",
      "Epoch: 339 \t| Train loss: 0.195 \t| Train acc: 0.55 \t| Test acc: 0.45\n",
      "Epoch: 340 \t| Train loss: 0.195 \t| Train acc: 0.55 \t| Test acc: 0.45\n",
      "Epoch: 341 \t| Train loss: 0.195 \t| Train acc: 0.55 \t| Test acc: 0.45\n",
      "Epoch: 342 \t| Train loss: 0.195 \t| Train acc: 0.56 \t| Test acc: 0.45\n",
      "Epoch: 343 \t| Train loss: 0.195 \t| Train acc: 0.56 \t| Test acc: 0.45\n",
      "Epoch: 344 \t| Train loss: 0.195 \t| Train acc: 0.56 \t| Test acc: 0.45\n",
      "Epoch: 345 \t| Train loss: 0.195 \t| Train acc: 0.56 \t| Test acc: 0.45\n",
      "Epoch: 346 \t| Train loss: 0.195 \t| Train acc: 0.56 \t| Test acc: 0.46\n",
      "Epoch: 347 \t| Train loss: 0.194 \t| Train acc: 0.56 \t| Test acc: 0.46\n",
      "Epoch: 348 \t| Train loss: 0.194 \t| Train acc: 0.56 \t| Test acc: 0.46\n",
      "Epoch: 349 \t| Train loss: 0.194 \t| Train acc: 0.55 \t| Test acc: 0.46\n",
      "Epoch: 350 \t| Train loss: 0.194 \t| Train acc: 0.55 \t| Test acc: 0.46\n",
      "Epoch: 351 \t| Train loss: 0.194 \t| Train acc: 0.55 \t| Test acc: 0.46\n",
      "Epoch: 352 \t| Train loss: 0.194 \t| Train acc: 0.55 \t| Test acc: 0.46\n",
      "Epoch: 353 \t| Train loss: 0.194 \t| Train acc: 0.55 \t| Test acc: 0.46\n",
      "Epoch: 354 \t| Train loss: 0.194 \t| Train acc: 0.56 \t| Test acc: 0.46\n",
      "Epoch: 355 \t| Train loss: 0.194 \t| Train acc: 0.56 \t| Test acc: 0.46\n",
      "Epoch: 356 \t| Train loss: 0.194 \t| Train acc: 0.56 \t| Test acc: 0.46\n",
      "Epoch: 357 \t| Train loss: 0.194 \t| Train acc: 0.56 \t| Test acc: 0.47\n",
      "Epoch: 358 \t| Train loss: 0.194 \t| Train acc: 0.55 \t| Test acc: 0.47\n",
      "Epoch: 359 \t| Train loss: 0.194 \t| Train acc: 0.55 \t| Test acc: 0.47\n",
      "Epoch: 360 \t| Train loss: 0.194 \t| Train acc: 0.55 \t| Test acc: 0.47\n",
      "Epoch: 361 \t| Train loss: 0.194 \t| Train acc: 0.55 \t| Test acc: 0.47\n",
      "Epoch: 362 \t| Train loss: 0.193 \t| Train acc: 0.55 \t| Test acc: 0.47\n",
      "Epoch: 363 \t| Train loss: 0.193 \t| Train acc: 0.55 \t| Test acc: 0.47\n",
      "Epoch: 364 \t| Train loss: 0.193 \t| Train acc: 0.55 \t| Test acc: 0.47\n",
      "Epoch: 365 \t| Train loss: 0.193 \t| Train acc: 0.55 \t| Test acc: 0.47\n",
      "Epoch: 366 \t| Train loss: 0.193 \t| Train acc: 0.55 \t| Test acc: 0.47\n",
      "Epoch: 367 \t| Train loss: 0.193 \t| Train acc: 0.55 \t| Test acc: 0.47\n",
      "Epoch: 368 \t| Train loss: 0.193 \t| Train acc: 0.55 \t| Test acc: 0.47\n",
      "Epoch: 369 \t| Train loss: 0.193 \t| Train acc: 0.55 \t| Test acc: 0.47\n",
      "Epoch: 370 \t| Train loss: 0.193 \t| Train acc: 0.55 \t| Test acc: 0.47\n",
      "Epoch: 371 \t| Train loss: 0.193 \t| Train acc: 0.55 \t| Test acc: 0.47\n",
      "Epoch: 372 \t| Train loss: 0.193 \t| Train acc: 0.55 \t| Test acc: 0.47\n",
      "Epoch: 373 \t| Train loss: 0.193 \t| Train acc: 0.55 \t| Test acc: 0.47\n",
      "Epoch: 374 \t| Train loss: 0.193 \t| Train acc: 0.57 \t| Test acc: 0.47\n",
      "Epoch: 375 \t| Train loss: 0.193 \t| Train acc: 0.57 \t| Test acc: 0.47\n",
      "Epoch: 376 \t| Train loss: 0.193 \t| Train acc: 0.57 \t| Test acc: 0.48\n",
      "Epoch: 377 \t| Train loss: 0.193 \t| Train acc: 0.57 \t| Test acc: 0.48\n",
      "Epoch: 378 \t| Train loss: 0.193 \t| Train acc: 0.57 \t| Test acc: 0.48\n",
      "Epoch: 379 \t| Train loss: 0.192 \t| Train acc: 0.57 \t| Test acc: 0.48\n",
      "Epoch: 380 \t| Train loss: 0.192 \t| Train acc: 0.57 \t| Test acc: 0.48\n",
      "Epoch: 381 \t| Train loss: 0.192 \t| Train acc: 0.57 \t| Test acc: 0.48\n",
      "Epoch: 382 \t| Train loss: 0.192 \t| Train acc: 0.57 \t| Test acc: 0.48\n",
      "Epoch: 383 \t| Train loss: 0.192 \t| Train acc: 0.57 \t| Test acc: 0.48\n",
      "Epoch: 384 \t| Train loss: 0.192 \t| Train acc: 0.57 \t| Test acc: 0.48\n",
      "Epoch: 385 \t| Train loss: 0.192 \t| Train acc: 0.57 \t| Test acc: 0.48\n",
      "Epoch: 386 \t| Train loss: 0.192 \t| Train acc: 0.57 \t| Test acc: 0.48\n",
      "Epoch: 387 \t| Train loss: 0.192 \t| Train acc: 0.57 \t| Test acc: 0.48\n",
      "Epoch: 388 \t| Train loss: 0.192 \t| Train acc: 0.57 \t| Test acc: 0.48\n",
      "Epoch: 389 \t| Train loss: 0.192 \t| Train acc: 0.57 \t| Test acc: 0.48\n",
      "Epoch: 390 \t| Train loss: 0.192 \t| Train acc: 0.57 \t| Test acc: 0.48\n",
      "Epoch: 391 \t| Train loss: 0.192 \t| Train acc: 0.57 \t| Test acc: 0.48\n",
      "Epoch: 392 \t| Train loss: 0.192 \t| Train acc: 0.57 \t| Test acc: 0.48\n",
      "Epoch: 393 \t| Train loss: 0.192 \t| Train acc: 0.57 \t| Test acc: 0.48\n",
      "Epoch: 394 \t| Train loss: 0.192 \t| Train acc: 0.57 \t| Test acc: 0.48\n",
      "Epoch: 395 \t| Train loss: 0.191 \t| Train acc: 0.57 \t| Test acc: 0.48\n",
      "Epoch: 396 \t| Train loss: 0.191 \t| Train acc: 0.58 \t| Test acc: 0.48\n",
      "Epoch: 397 \t| Train loss: 0.191 \t| Train acc: 0.58 \t| Test acc: 0.48\n",
      "Epoch: 398 \t| Train loss: 0.191 \t| Train acc: 0.58 \t| Test acc: 0.48\n",
      "Epoch: 399 \t| Train loss: 0.191 \t| Train acc: 0.58 \t| Test acc: 0.48\n",
      "Epoch: 400 \t| Train loss: 0.191 \t| Train acc: 0.58 \t| Test acc: 0.48\n",
      "Epoch: 401 \t| Train loss: 0.191 \t| Train acc: 0.58 \t| Test acc: 0.48\n",
      "Epoch: 402 \t| Train loss: 0.191 \t| Train acc: 0.58 \t| Test acc: 0.48\n",
      "Epoch: 403 \t| Train loss: 0.191 \t| Train acc: 0.58 \t| Test acc: 0.49\n",
      "Epoch: 404 \t| Train loss: 0.191 \t| Train acc: 0.58 \t| Test acc: 0.49\n",
      "Epoch: 405 \t| Train loss: 0.191 \t| Train acc: 0.58 \t| Test acc: 0.49\n",
      "Epoch: 406 \t| Train loss: 0.191 \t| Train acc: 0.58 \t| Test acc: 0.49\n",
      "Epoch: 407 \t| Train loss: 0.191 \t| Train acc: 0.58 \t| Test acc: 0.49\n",
      "Epoch: 408 \t| Train loss: 0.191 \t| Train acc: 0.58 \t| Test acc: 0.49\n",
      "Epoch: 409 \t| Train loss: 0.191 \t| Train acc: 0.58 \t| Test acc: 0.49\n",
      "Epoch: 410 \t| Train loss: 0.191 \t| Train acc: 0.58 \t| Test acc: 0.49\n",
      "Epoch: 411 \t| Train loss: 0.19 \t| Train acc: 0.58 \t| Test acc: 0.49\n",
      "Epoch: 412 \t| Train loss: 0.19 \t| Train acc: 0.58 \t| Test acc: 0.49\n",
      "Epoch: 413 \t| Train loss: 0.19 \t| Train acc: 0.58 \t| Test acc: 0.49\n",
      "Epoch: 414 \t| Train loss: 0.19 \t| Train acc: 0.58 \t| Test acc: 0.49\n",
      "Epoch: 415 \t| Train loss: 0.19 \t| Train acc: 0.58 \t| Test acc: 0.49\n",
      "Epoch: 416 \t| Train loss: 0.19 \t| Train acc: 0.58 \t| Test acc: 0.49\n",
      "Epoch: 417 \t| Train loss: 0.19 \t| Train acc: 0.58 \t| Test acc: 0.49\n",
      "Epoch: 418 \t| Train loss: 0.19 \t| Train acc: 0.58 \t| Test acc: 0.49\n",
      "Epoch: 419 \t| Train loss: 0.19 \t| Train acc: 0.58 \t| Test acc: 0.49\n",
      "Epoch: 420 \t| Train loss: 0.19 \t| Train acc: 0.58 \t| Test acc: 0.49\n",
      "Epoch: 421 \t| Train loss: 0.19 \t| Train acc: 0.58 \t| Test acc: 0.49\n",
      "Epoch: 422 \t| Train loss: 0.19 \t| Train acc: 0.58 \t| Test acc: 0.49\n",
      "Epoch: 423 \t| Train loss: 0.19 \t| Train acc: 0.59 \t| Test acc: 0.49\n",
      "Epoch: 424 \t| Train loss: 0.19 \t| Train acc: 0.59 \t| Test acc: 0.49\n",
      "Epoch: 425 \t| Train loss: 0.19 \t| Train acc: 0.59 \t| Test acc: 0.49\n",
      "Epoch: 426 \t| Train loss: 0.19 \t| Train acc: 0.59 \t| Test acc: 0.49\n",
      "Epoch: 427 \t| Train loss: 0.19 \t| Train acc: 0.59 \t| Test acc: 0.49\n",
      "Epoch: 428 \t| Train loss: 0.189 \t| Train acc: 0.59 \t| Test acc: 0.49\n",
      "Epoch: 429 \t| Train loss: 0.189 \t| Train acc: 0.59 \t| Test acc: 0.49\n",
      "Epoch: 430 \t| Train loss: 0.189 \t| Train acc: 0.59 \t| Test acc: 0.49\n",
      "Epoch: 431 \t| Train loss: 0.189 \t| Train acc: 0.59 \t| Test acc: 0.49\n",
      "Epoch: 432 \t| Train loss: 0.189 \t| Train acc: 0.6 \t| Test acc: 0.49\n",
      "Epoch: 433 \t| Train loss: 0.189 \t| Train acc: 0.6 \t| Test acc: 0.49\n",
      "Epoch: 434 \t| Train loss: 0.189 \t| Train acc: 0.6 \t| Test acc: 0.49\n",
      "Epoch: 435 \t| Train loss: 0.189 \t| Train acc: 0.6 \t| Test acc: 0.49\n",
      "Epoch: 436 \t| Train loss: 0.189 \t| Train acc: 0.6 \t| Test acc: 0.49\n",
      "Epoch: 437 \t| Train loss: 0.189 \t| Train acc: 0.6 \t| Test acc: 0.49\n",
      "Epoch: 438 \t| Train loss: 0.189 \t| Train acc: 0.6 \t| Test acc: 0.49\n",
      "Epoch: 439 \t| Train loss: 0.189 \t| Train acc: 0.6 \t| Test acc: 0.49\n",
      "Epoch: 440 \t| Train loss: 0.189 \t| Train acc: 0.6 \t| Test acc: 0.49\n",
      "Epoch: 441 \t| Train loss: 0.189 \t| Train acc: 0.6 \t| Test acc: 0.49\n",
      "Epoch: 442 \t| Train loss: 0.189 \t| Train acc: 0.6 \t| Test acc: 0.49\n",
      "Epoch: 443 \t| Train loss: 0.189 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 444 \t| Train loss: 0.189 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 445 \t| Train loss: 0.189 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 446 \t| Train loss: 0.189 \t| Train acc: 0.61 \t| Test acc: 0.49\n",
      "Epoch: 447 \t| Train loss: 0.189 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 448 \t| Train loss: 0.189 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 449 \t| Train loss: 0.189 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 450 \t| Train loss: 0.188 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 451 \t| Train loss: 0.188 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 452 \t| Train loss: 0.188 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 453 \t| Train loss: 0.188 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 454 \t| Train loss: 0.188 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 455 \t| Train loss: 0.188 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 456 \t| Train loss: 0.188 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 457 \t| Train loss: 0.188 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 458 \t| Train loss: 0.188 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 459 \t| Train loss: 0.188 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 460 \t| Train loss: 0.188 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 461 \t| Train loss: 0.188 \t| Train acc: 0.6 \t| Test acc: 0.5\n",
      "Epoch: 462 \t| Train loss: 0.188 \t| Train acc: 0.6 \t| Test acc: 0.5\n",
      "Epoch: 463 \t| Train loss: 0.188 \t| Train acc: 0.6 \t| Test acc: 0.5\n",
      "Epoch: 464 \t| Train loss: 0.188 \t| Train acc: 0.6 \t| Test acc: 0.5\n",
      "Epoch: 465 \t| Train loss: 0.188 \t| Train acc: 0.6 \t| Test acc: 0.5\n",
      "Epoch: 466 \t| Train loss: 0.188 \t| Train acc: 0.6 \t| Test acc: 0.5\n",
      "Epoch: 467 \t| Train loss: 0.188 \t| Train acc: 0.6 \t| Test acc: 0.5\n",
      "Epoch: 468 \t| Train loss: 0.188 \t| Train acc: 0.6 \t| Test acc: 0.5\n",
      "Epoch: 469 \t| Train loss: 0.188 \t| Train acc: 0.6 \t| Test acc: 0.5\n",
      "Epoch: 470 \t| Train loss: 0.188 \t| Train acc: 0.6 \t| Test acc: 0.5\n",
      "Epoch: 471 \t| Train loss: 0.188 \t| Train acc: 0.6 \t| Test acc: 0.5\n",
      "Epoch: 472 \t| Train loss: 0.188 \t| Train acc: 0.6 \t| Test acc: 0.5\n",
      "Epoch: 473 \t| Train loss: 0.188 \t| Train acc: 0.6 \t| Test acc: 0.5\n",
      "Epoch: 474 \t| Train loss: 0.188 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 475 \t| Train loss: 0.188 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 476 \t| Train loss: 0.188 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 477 \t| Train loss: 0.188 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 478 \t| Train loss: 0.188 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 479 \t| Train loss: 0.187 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 480 \t| Train loss: 0.187 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 481 \t| Train loss: 0.187 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 482 \t| Train loss: 0.187 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 483 \t| Train loss: 0.187 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 484 \t| Train loss: 0.187 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 485 \t| Train loss: 0.187 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 486 \t| Train loss: 0.187 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 487 \t| Train loss: 0.187 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 488 \t| Train loss: 0.187 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 489 \t| Train loss: 0.187 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 490 \t| Train loss: 0.187 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 491 \t| Train loss: 0.187 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 492 \t| Train loss: 0.187 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 493 \t| Train loss: 0.187 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 494 \t| Train loss: 0.187 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 495 \t| Train loss: 0.187 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 496 \t| Train loss: 0.187 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 497 \t| Train loss: 0.187 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 498 \t| Train loss: 0.187 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 499 \t| Train loss: 0.187 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 500 \t| Train loss: 0.187 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 501 \t| Train loss: 0.187 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 502 \t| Train loss: 0.187 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 503 \t| Train loss: 0.187 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 504 \t| Train loss: 0.187 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 505 \t| Train loss: 0.187 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 506 \t| Train loss: 0.187 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 507 \t| Train loss: 0.187 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 508 \t| Train loss: 0.187 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 509 \t| Train loss: 0.187 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 510 \t| Train loss: 0.187 \t| Train acc: 0.62 \t| Test acc: 0.5\n",
      "Epoch: 511 \t| Train loss: 0.187 \t| Train acc: 0.62 \t| Test acc: 0.5\n",
      "Epoch: 512 \t| Train loss: 0.187 \t| Train acc: 0.62 \t| Test acc: 0.5\n",
      "Epoch: 513 \t| Train loss: 0.187 \t| Train acc: 0.62 \t| Test acc: 0.5\n",
      "Epoch: 514 \t| Train loss: 0.187 \t| Train acc: 0.62 \t| Test acc: 0.5\n",
      "Epoch: 515 \t| Train loss: 0.186 \t| Train acc: 0.62 \t| Test acc: 0.5\n",
      "Epoch: 516 \t| Train loss: 0.186 \t| Train acc: 0.62 \t| Test acc: 0.5\n",
      "Epoch: 517 \t| Train loss: 0.186 \t| Train acc: 0.62 \t| Test acc: 0.5\n",
      "Epoch: 518 \t| Train loss: 0.186 \t| Train acc: 0.62 \t| Test acc: 0.5\n",
      "Epoch: 519 \t| Train loss: 0.186 \t| Train acc: 0.62 \t| Test acc: 0.5\n",
      "Epoch: 520 \t| Train loss: 0.186 \t| Train acc: 0.62 \t| Test acc: 0.5\n",
      "Epoch: 521 \t| Train loss: 0.186 \t| Train acc: 0.62 \t| Test acc: 0.5\n",
      "Epoch: 522 \t| Train loss: 0.186 \t| Train acc: 0.62 \t| Test acc: 0.5\n",
      "Epoch: 523 \t| Train loss: 0.186 \t| Train acc: 0.62 \t| Test acc: 0.5\n",
      "Epoch: 524 \t| Train loss: 0.186 \t| Train acc: 0.62 \t| Test acc: 0.5\n",
      "Epoch: 525 \t| Train loss: 0.186 \t| Train acc: 0.62 \t| Test acc: 0.5\n",
      "Epoch: 526 \t| Train loss: 0.186 \t| Train acc: 0.62 \t| Test acc: 0.5\n",
      "Epoch: 527 \t| Train loss: 0.186 \t| Train acc: 0.62 \t| Test acc: 0.5\n",
      "Epoch: 528 \t| Train loss: 0.186 \t| Train acc: 0.62 \t| Test acc: 0.5\n",
      "Epoch: 529 \t| Train loss: 0.186 \t| Train acc: 0.62 \t| Test acc: 0.5\n",
      "Epoch: 530 \t| Train loss: 0.186 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 531 \t| Train loss: 0.186 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 532 \t| Train loss: 0.186 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 533 \t| Train loss: 0.186 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 534 \t| Train loss: 0.186 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 535 \t| Train loss: 0.186 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 536 \t| Train loss: 0.186 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 537 \t| Train loss: 0.186 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 538 \t| Train loss: 0.186 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 539 \t| Train loss: 0.186 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 540 \t| Train loss: 0.186 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 541 \t| Train loss: 0.186 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 542 \t| Train loss: 0.186 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 543 \t| Train loss: 0.186 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 544 \t| Train loss: 0.186 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 545 \t| Train loss: 0.186 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 546 \t| Train loss: 0.186 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 547 \t| Train loss: 0.186 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 548 \t| Train loss: 0.186 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 549 \t| Train loss: 0.186 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 550 \t| Train loss: 0.186 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 551 \t| Train loss: 0.186 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 552 \t| Train loss: 0.186 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 553 \t| Train loss: 0.186 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 554 \t| Train loss: 0.186 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 555 \t| Train loss: 0.186 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 556 \t| Train loss: 0.186 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 557 \t| Train loss: 0.185 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 558 \t| Train loss: 0.185 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 559 \t| Train loss: 0.185 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 560 \t| Train loss: 0.185 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 561 \t| Train loss: 0.185 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 562 \t| Train loss: 0.185 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 563 \t| Train loss: 0.185 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 564 \t| Train loss: 0.185 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 565 \t| Train loss: 0.185 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 566 \t| Train loss: 0.185 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 567 \t| Train loss: 0.185 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 568 \t| Train loss: 0.185 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 569 \t| Train loss: 0.185 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 570 \t| Train loss: 0.185 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 571 \t| Train loss: 0.185 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 572 \t| Train loss: 0.185 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 573 \t| Train loss: 0.185 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 574 \t| Train loss: 0.185 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 575 \t| Train loss: 0.185 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 576 \t| Train loss: 0.185 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 577 \t| Train loss: 0.185 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 578 \t| Train loss: 0.185 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 579 \t| Train loss: 0.185 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 580 \t| Train loss: 0.185 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 581 \t| Train loss: 0.185 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 582 \t| Train loss: 0.185 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 583 \t| Train loss: 0.185 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 584 \t| Train loss: 0.185 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 585 \t| Train loss: 0.185 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 586 \t| Train loss: 0.185 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 587 \t| Train loss: 0.185 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 588 \t| Train loss: 0.185 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 589 \t| Train loss: 0.185 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 590 \t| Train loss: 0.185 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 591 \t| Train loss: 0.185 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 592 \t| Train loss: 0.185 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 593 \t| Train loss: 0.185 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 594 \t| Train loss: 0.185 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 595 \t| Train loss: 0.185 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 596 \t| Train loss: 0.185 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 597 \t| Train loss: 0.185 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 598 \t| Train loss: 0.185 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 599 \t| Train loss: 0.185 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 600 \t| Train loss: 0.185 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 601 \t| Train loss: 0.185 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 602 \t| Train loss: 0.185 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 603 \t| Train loss: 0.185 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 604 \t| Train loss: 0.185 \t| Train acc: 0.61 \t| Test acc: 0.5\n",
      "Epoch: 605 \t| Train loss: 0.185 \t| Train acc: 0.62 \t| Test acc: 0.5\n",
      "Epoch: 606 \t| Train loss: 0.185 \t| Train acc: 0.62 \t| Test acc: 0.5\n",
      "Epoch: 607 \t| Train loss: 0.184 \t| Train acc: 0.62 \t| Test acc: 0.5\n",
      "Epoch: 608 \t| Train loss: 0.184 \t| Train acc: 0.62 \t| Test acc: 0.5\n",
      "Epoch: 609 \t| Train loss: 0.184 \t| Train acc: 0.62 \t| Test acc: 0.5\n",
      "Epoch: 610 \t| Train loss: 0.184 \t| Train acc: 0.62 \t| Test acc: 0.5\n",
      "Epoch: 611 \t| Train loss: 0.184 \t| Train acc: 0.62 \t| Test acc: 0.5\n",
      "Epoch: 612 \t| Train loss: 0.184 \t| Train acc: 0.62 \t| Test acc: 0.5\n",
      "Epoch: 613 \t| Train loss: 0.184 \t| Train acc: 0.62 \t| Test acc: 0.5\n",
      "Epoch: 614 \t| Train loss: 0.184 \t| Train acc: 0.62 \t| Test acc: 0.5\n",
      "Epoch: 615 \t| Train loss: 0.184 \t| Train acc: 0.62 \t| Test acc: 0.5\n",
      "Epoch: 616 \t| Train loss: 0.184 \t| Train acc: 0.62 \t| Test acc: 0.5\n",
      "Epoch: 617 \t| Train loss: 0.184 \t| Train acc: 0.62 \t| Test acc: 0.5\n",
      "Epoch: 618 \t| Train loss: 0.184 \t| Train acc: 0.62 \t| Test acc: 0.5\n",
      "Epoch: 619 \t| Train loss: 0.184 \t| Train acc: 0.62 \t| Test acc: 0.5\n",
      "Epoch: 620 \t| Train loss: 0.184 \t| Train acc: 0.62 \t| Test acc: 0.5\n",
      "Epoch: 621 \t| Train loss: 0.184 \t| Train acc: 0.62 \t| Test acc: 0.5\n",
      "Epoch: 622 \t| Train loss: 0.184 \t| Train acc: 0.62 \t| Test acc: 0.5\n",
      "Epoch: 623 \t| Train loss: 0.184 \t| Train acc: 0.62 \t| Test acc: 0.5\n",
      "Epoch: 624 \t| Train loss: 0.184 \t| Train acc: 0.62 \t| Test acc: 0.5\n",
      "Epoch: 625 \t| Train loss: 0.184 \t| Train acc: 0.62 \t| Test acc: 0.5\n",
      "Epoch: 626 \t| Train loss: 0.184 \t| Train acc: 0.62 \t| Test acc: 0.5\n",
      "Epoch: 627 \t| Train loss: 0.184 \t| Train acc: 0.62 \t| Test acc: 0.5\n",
      "Epoch: 628 \t| Train loss: 0.184 \t| Train acc: 0.62 \t| Test acc: 0.5\n",
      "Epoch: 629 \t| Train loss: 0.184 \t| Train acc: 0.62 \t| Test acc: 0.5\n",
      "Epoch: 630 \t| Train loss: 0.184 \t| Train acc: 0.62 \t| Test acc: 0.5\n",
      "Epoch: 631 \t| Train loss: 0.184 \t| Train acc: 0.62 \t| Test acc: 0.5\n",
      "Epoch: 632 \t| Train loss: 0.184 \t| Train acc: 0.62 \t| Test acc: 0.5\n",
      "Epoch: 633 \t| Train loss: 0.184 \t| Train acc: 0.62 \t| Test acc: 0.5\n",
      "Epoch: 634 \t| Train loss: 0.184 \t| Train acc: 0.62 \t| Test acc: 0.51\n",
      "Epoch: 635 \t| Train loss: 0.184 \t| Train acc: 0.62 \t| Test acc: 0.5\n",
      "Epoch: 636 \t| Train loss: 0.184 \t| Train acc: 0.62 \t| Test acc: 0.51\n",
      "Epoch: 637 \t| Train loss: 0.184 \t| Train acc: 0.62 \t| Test acc: 0.51\n",
      "Epoch: 638 \t| Train loss: 0.184 \t| Train acc: 0.62 \t| Test acc: 0.51\n",
      "Epoch: 639 \t| Train loss: 0.184 \t| Train acc: 0.62 \t| Test acc: 0.51\n",
      "Epoch: 640 \t| Train loss: 0.184 \t| Train acc: 0.62 \t| Test acc: 0.51\n",
      "Epoch: 641 \t| Train loss: 0.184 \t| Train acc: 0.62 \t| Test acc: 0.51\n",
      "Epoch: 642 \t| Train loss: 0.184 \t| Train acc: 0.62 \t| Test acc: 0.51\n",
      "Epoch: 643 \t| Train loss: 0.184 \t| Train acc: 0.62 \t| Test acc: 0.51\n",
      "Epoch: 644 \t| Train loss: 0.184 \t| Train acc: 0.62 \t| Test acc: 0.51\n",
      "Epoch: 645 \t| Train loss: 0.184 \t| Train acc: 0.62 \t| Test acc: 0.51\n",
      "Epoch: 646 \t| Train loss: 0.184 \t| Train acc: 0.62 \t| Test acc: 0.51\n",
      "Epoch: 647 \t| Train loss: 0.184 \t| Train acc: 0.62 \t| Test acc: 0.51\n",
      "Epoch: 648 \t| Train loss: 0.184 \t| Train acc: 0.62 \t| Test acc: 0.51\n",
      "Epoch: 649 \t| Train loss: 0.184 \t| Train acc: 0.62 \t| Test acc: 0.51\n",
      "Epoch: 650 \t| Train loss: 0.184 \t| Train acc: 0.62 \t| Test acc: 0.51\n",
      "Epoch: 651 \t| Train loss: 0.184 \t| Train acc: 0.62 \t| Test acc: 0.51\n",
      "Epoch: 652 \t| Train loss: 0.184 \t| Train acc: 0.62 \t| Test acc: 0.51\n",
      "Epoch: 653 \t| Train loss: 0.184 \t| Train acc: 0.62 \t| Test acc: 0.51\n",
      "Epoch: 654 \t| Train loss: 0.184 \t| Train acc: 0.62 \t| Test acc: 0.51\n",
      "Epoch: 655 \t| Train loss: 0.184 \t| Train acc: 0.62 \t| Test acc: 0.51\n",
      "Epoch: 656 \t| Train loss: 0.184 \t| Train acc: 0.62 \t| Test acc: 0.51\n",
      "Epoch: 657 \t| Train loss: 0.184 \t| Train acc: 0.62 \t| Test acc: 0.51\n",
      "Epoch: 658 \t| Train loss: 0.184 \t| Train acc: 0.62 \t| Test acc: 0.51\n",
      "Epoch: 659 \t| Train loss: 0.184 \t| Train acc: 0.62 \t| Test acc: 0.51\n",
      "Epoch: 660 \t| Train loss: 0.184 \t| Train acc: 0.62 \t| Test acc: 0.51\n",
      "Epoch: 661 \t| Train loss: 0.184 \t| Train acc: 0.62 \t| Test acc: 0.51\n",
      "Epoch: 662 \t| Train loss: 0.183 \t| Train acc: 0.62 \t| Test acc: 0.51\n",
      "Epoch: 663 \t| Train loss: 0.183 \t| Train acc: 0.62 \t| Test acc: 0.51\n",
      "Epoch: 664 \t| Train loss: 0.183 \t| Train acc: 0.62 \t| Test acc: 0.51\n",
      "Epoch: 665 \t| Train loss: 0.183 \t| Train acc: 0.62 \t| Test acc: 0.51\n",
      "Epoch: 666 \t| Train loss: 0.183 \t| Train acc: 0.62 \t| Test acc: 0.51\n",
      "Epoch: 667 \t| Train loss: 0.183 \t| Train acc: 0.62 \t| Test acc: 0.51\n",
      "Epoch: 668 \t| Train loss: 0.183 \t| Train acc: 0.62 \t| Test acc: 0.51\n",
      "Epoch: 669 \t| Train loss: 0.183 \t| Train acc: 0.62 \t| Test acc: 0.51\n",
      "Epoch: 670 \t| Train loss: 0.183 \t| Train acc: 0.62 \t| Test acc: 0.51\n",
      "Epoch: 671 \t| Train loss: 0.183 \t| Train acc: 0.62 \t| Test acc: 0.51\n",
      "Epoch: 672 \t| Train loss: 0.183 \t| Train acc: 0.62 \t| Test acc: 0.51\n",
      "Epoch: 673 \t| Train loss: 0.183 \t| Train acc: 0.62 \t| Test acc: 0.51\n",
      "Epoch: 674 \t| Train loss: 0.183 \t| Train acc: 0.62 \t| Test acc: 0.51\n",
      "Epoch: 675 \t| Train loss: 0.183 \t| Train acc: 0.62 \t| Test acc: 0.51\n",
      "Epoch: 676 \t| Train loss: 0.183 \t| Train acc: 0.62 \t| Test acc: 0.51\n",
      "Epoch: 677 \t| Train loss: 0.183 \t| Train acc: 0.62 \t| Test acc: 0.51\n",
      "Epoch: 678 \t| Train loss: 0.183 \t| Train acc: 0.62 \t| Test acc: 0.51\n",
      "Epoch: 679 \t| Train loss: 0.183 \t| Train acc: 0.62 \t| Test acc: 0.51\n",
      "Epoch: 680 \t| Train loss: 0.183 \t| Train acc: 0.62 \t| Test acc: 0.51\n",
      "Epoch: 681 \t| Train loss: 0.183 \t| Train acc: 0.62 \t| Test acc: 0.51\n",
      "Epoch: 682 \t| Train loss: 0.183 \t| Train acc: 0.62 \t| Test acc: 0.51\n",
      "Epoch: 683 \t| Train loss: 0.183 \t| Train acc: 0.62 \t| Test acc: 0.51\n",
      "Epoch: 684 \t| Train loss: 0.183 \t| Train acc: 0.62 \t| Test acc: 0.51\n",
      "Epoch: 685 \t| Train loss: 0.183 \t| Train acc: 0.62 \t| Test acc: 0.51\n",
      "Epoch: 686 \t| Train loss: 0.183 \t| Train acc: 0.62 \t| Test acc: 0.51\n",
      "Epoch: 687 \t| Train loss: 0.183 \t| Train acc: 0.62 \t| Test acc: 0.51\n",
      "Epoch: 688 \t| Train loss: 0.183 \t| Train acc: 0.62 \t| Test acc: 0.51\n",
      "Epoch: 689 \t| Train loss: 0.183 \t| Train acc: 0.62 \t| Test acc: 0.51\n",
      "Epoch: 690 \t| Train loss: 0.183 \t| Train acc: 0.62 \t| Test acc: 0.51\n",
      "Epoch: 691 \t| Train loss: 0.183 \t| Train acc: 0.62 \t| Test acc: 0.51\n",
      "Epoch: 692 \t| Train loss: 0.183 \t| Train acc: 0.62 \t| Test acc: 0.51\n",
      "Epoch: 693 \t| Train loss: 0.183 \t| Train acc: 0.62 \t| Test acc: 0.51\n",
      "Epoch: 694 \t| Train loss: 0.183 \t| Train acc: 0.62 \t| Test acc: 0.51\n",
      "Epoch: 695 \t| Train loss: 0.183 \t| Train acc: 0.62 \t| Test acc: 0.51\n",
      "Epoch: 696 \t| Train loss: 0.183 \t| Train acc: 0.62 \t| Test acc: 0.51\n",
      "Epoch: 697 \t| Train loss: 0.183 \t| Train acc: 0.62 \t| Test acc: 0.51\n",
      "Epoch: 698 \t| Train loss: 0.183 \t| Train acc: 0.62 \t| Test acc: 0.51\n",
      "Epoch: 699 \t| Train loss: 0.183 \t| Train acc: 0.62 \t| Test acc: 0.51\n",
      "Epoch: 700 \t| Train loss: 0.183 \t| Train acc: 0.62 \t| Test acc: 0.51\n",
      "Epoch: 701 \t| Train loss: 0.183 \t| Train acc: 0.62 \t| Test acc: 0.51\n",
      "Epoch: 702 \t| Train loss: 0.183 \t| Train acc: 0.62 \t| Test acc: 0.51\n",
      "Epoch: 703 \t| Train loss: 0.183 \t| Train acc: 0.62 \t| Test acc: 0.51\n",
      "Epoch: 704 \t| Train loss: 0.183 \t| Train acc: 0.62 \t| Test acc: 0.51\n",
      "Epoch: 705 \t| Train loss: 0.183 \t| Train acc: 0.62 \t| Test acc: 0.51\n",
      "Epoch: 706 \t| Train loss: 0.183 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 707 \t| Train loss: 0.183 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 708 \t| Train loss: 0.183 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 709 \t| Train loss: 0.183 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 710 \t| Train loss: 0.183 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 711 \t| Train loss: 0.183 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 712 \t| Train loss: 0.183 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 713 \t| Train loss: 0.183 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 714 \t| Train loss: 0.183 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 715 \t| Train loss: 0.183 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 716 \t| Train loss: 0.183 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 717 \t| Train loss: 0.183 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 718 \t| Train loss: 0.183 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 719 \t| Train loss: 0.183 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 720 \t| Train loss: 0.183 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 721 \t| Train loss: 0.183 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 722 \t| Train loss: 0.183 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 723 \t| Train loss: 0.183 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 724 \t| Train loss: 0.183 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 725 \t| Train loss: 0.183 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 726 \t| Train loss: 0.183 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 727 \t| Train loss: 0.183 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 728 \t| Train loss: 0.183 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 729 \t| Train loss: 0.183 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 730 \t| Train loss: 0.183 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 731 \t| Train loss: 0.183 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 732 \t| Train loss: 0.183 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 733 \t| Train loss: 0.183 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 734 \t| Train loss: 0.183 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 735 \t| Train loss: 0.183 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 736 \t| Train loss: 0.183 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 737 \t| Train loss: 0.183 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 738 \t| Train loss: 0.183 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 739 \t| Train loss: 0.183 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 740 \t| Train loss: 0.183 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 741 \t| Train loss: 0.183 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 742 \t| Train loss: 0.183 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 743 \t| Train loss: 0.183 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 744 \t| Train loss: 0.183 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 745 \t| Train loss: 0.183 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 746 \t| Train loss: 0.183 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 747 \t| Train loss: 0.182 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 748 \t| Train loss: 0.182 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 749 \t| Train loss: 0.182 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 750 \t| Train loss: 0.182 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 751 \t| Train loss: 0.182 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 752 \t| Train loss: 0.182 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 753 \t| Train loss: 0.182 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 754 \t| Train loss: 0.182 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 755 \t| Train loss: 0.182 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 756 \t| Train loss: 0.182 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 757 \t| Train loss: 0.182 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 758 \t| Train loss: 0.182 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 759 \t| Train loss: 0.182 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 760 \t| Train loss: 0.182 \t| Train acc: 0.63 \t| Test acc: 0.52\n",
      "Epoch: 761 \t| Train loss: 0.182 \t| Train acc: 0.63 \t| Test acc: 0.52\n",
      "Epoch: 762 \t| Train loss: 0.182 \t| Train acc: 0.63 \t| Test acc: 0.52\n",
      "Epoch: 763 \t| Train loss: 0.182 \t| Train acc: 0.63 \t| Test acc: 0.52\n",
      "Epoch: 764 \t| Train loss: 0.182 \t| Train acc: 0.63 \t| Test acc: 0.52\n",
      "Epoch: 765 \t| Train loss: 0.182 \t| Train acc: 0.63 \t| Test acc: 0.52\n",
      "Epoch: 766 \t| Train loss: 0.182 \t| Train acc: 0.63 \t| Test acc: 0.52\n",
      "Epoch: 767 \t| Train loss: 0.182 \t| Train acc: 0.63 \t| Test acc: 0.52\n",
      "Epoch: 768 \t| Train loss: 0.182 \t| Train acc: 0.63 \t| Test acc: 0.52\n",
      "Epoch: 769 \t| Train loss: 0.182 \t| Train acc: 0.63 \t| Test acc: 0.52\n",
      "Epoch: 770 \t| Train loss: 0.182 \t| Train acc: 0.63 \t| Test acc: 0.52\n",
      "Epoch: 771 \t| Train loss: 0.182 \t| Train acc: 0.63 \t| Test acc: 0.52\n",
      "Epoch: 772 \t| Train loss: 0.182 \t| Train acc: 0.63 \t| Test acc: 0.52\n",
      "Epoch: 773 \t| Train loss: 0.182 \t| Train acc: 0.63 \t| Test acc: 0.52\n",
      "Epoch: 774 \t| Train loss: 0.182 \t| Train acc: 0.63 \t| Test acc: 0.52\n",
      "Epoch: 775 \t| Train loss: 0.182 \t| Train acc: 0.63 \t| Test acc: 0.52\n",
      "Epoch: 776 \t| Train loss: 0.182 \t| Train acc: 0.63 \t| Test acc: 0.52\n",
      "Epoch: 777 \t| Train loss: 0.182 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 778 \t| Train loss: 0.182 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 779 \t| Train loss: 0.182 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 780 \t| Train loss: 0.182 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 781 \t| Train loss: 0.182 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 782 \t| Train loss: 0.182 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 783 \t| Train loss: 0.182 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 784 \t| Train loss: 0.182 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 785 \t| Train loss: 0.182 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 786 \t| Train loss: 0.182 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 787 \t| Train loss: 0.182 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 788 \t| Train loss: 0.182 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 789 \t| Train loss: 0.182 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 790 \t| Train loss: 0.182 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 791 \t| Train loss: 0.182 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 792 \t| Train loss: 0.182 \t| Train acc: 0.63 \t| Test acc: 0.52\n",
      "Epoch: 793 \t| Train loss: 0.182 \t| Train acc: 0.63 \t| Test acc: 0.52\n",
      "Epoch: 794 \t| Train loss: 0.182 \t| Train acc: 0.63 \t| Test acc: 0.52\n",
      "Epoch: 795 \t| Train loss: 0.182 \t| Train acc: 0.63 \t| Test acc: 0.52\n",
      "Epoch: 796 \t| Train loss: 0.182 \t| Train acc: 0.63 \t| Test acc: 0.52\n",
      "Epoch: 797 \t| Train loss: 0.182 \t| Train acc: 0.63 \t| Test acc: 0.52\n",
      "Epoch: 798 \t| Train loss: 0.182 \t| Train acc: 0.63 \t| Test acc: 0.52\n",
      "Epoch: 799 \t| Train loss: 0.182 \t| Train acc: 0.63 \t| Test acc: 0.52\n",
      "Epoch: 800 \t| Train loss: 0.182 \t| Train acc: 0.63 \t| Test acc: 0.52\n",
      "Epoch: 801 \t| Train loss: 0.182 \t| Train acc: 0.63 \t| Test acc: 0.52\n",
      "Epoch: 802 \t| Train loss: 0.182 \t| Train acc: 0.63 \t| Test acc: 0.52\n",
      "Epoch: 803 \t| Train loss: 0.182 \t| Train acc: 0.63 \t| Test acc: 0.52\n",
      "Epoch: 804 \t| Train loss: 0.182 \t| Train acc: 0.63 \t| Test acc: 0.52\n",
      "Epoch: 805 \t| Train loss: 0.182 \t| Train acc: 0.63 \t| Test acc: 0.52\n",
      "Epoch: 806 \t| Train loss: 0.182 \t| Train acc: 0.63 \t| Test acc: 0.52\n",
      "Epoch: 807 \t| Train loss: 0.182 \t| Train acc: 0.63 \t| Test acc: 0.52\n",
      "Epoch: 808 \t| Train loss: 0.182 \t| Train acc: 0.63 \t| Test acc: 0.52\n",
      "Epoch: 809 \t| Train loss: 0.182 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 810 \t| Train loss: 0.182 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 811 \t| Train loss: 0.182 \t| Train acc: 0.63 \t| Test acc: 0.52\n",
      "Epoch: 812 \t| Train loss: 0.182 \t| Train acc: 0.63 \t| Test acc: 0.52\n",
      "Epoch: 813 \t| Train loss: 0.182 \t| Train acc: 0.63 \t| Test acc: 0.52\n",
      "Epoch: 814 \t| Train loss: 0.182 \t| Train acc: 0.63 \t| Test acc: 0.52\n",
      "Epoch: 815 \t| Train loss: 0.182 \t| Train acc: 0.63 \t| Test acc: 0.52\n",
      "Epoch: 816 \t| Train loss: 0.182 \t| Train acc: 0.63 \t| Test acc: 0.52\n",
      "Epoch: 817 \t| Train loss: 0.182 \t| Train acc: 0.63 \t| Test acc: 0.52\n",
      "Epoch: 818 \t| Train loss: 0.182 \t| Train acc: 0.63 \t| Test acc: 0.52\n",
      "Epoch: 819 \t| Train loss: 0.182 \t| Train acc: 0.63 \t| Test acc: 0.52\n",
      "Epoch: 820 \t| Train loss: 0.182 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 821 \t| Train loss: 0.182 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 822 \t| Train loss: 0.182 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 823 \t| Train loss: 0.182 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 824 \t| Train loss: 0.182 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 825 \t| Train loss: 0.182 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 826 \t| Train loss: 0.182 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 827 \t| Train loss: 0.182 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 828 \t| Train loss: 0.182 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 829 \t| Train loss: 0.182 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 830 \t| Train loss: 0.182 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 831 \t| Train loss: 0.182 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 832 \t| Train loss: 0.182 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 833 \t| Train loss: 0.182 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 834 \t| Train loss: 0.182 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 835 \t| Train loss: 0.182 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 836 \t| Train loss: 0.182 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 837 \t| Train loss: 0.182 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 838 \t| Train loss: 0.182 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 839 \t| Train loss: 0.182 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 840 \t| Train loss: 0.182 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 841 \t| Train loss: 0.182 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 842 \t| Train loss: 0.182 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 843 \t| Train loss: 0.182 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 844 \t| Train loss: 0.182 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 845 \t| Train loss: 0.182 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 846 \t| Train loss: 0.182 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 847 \t| Train loss: 0.182 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 848 \t| Train loss: 0.182 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 849 \t| Train loss: 0.182 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 850 \t| Train loss: 0.182 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 851 \t| Train loss: 0.182 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 852 \t| Train loss: 0.182 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 853 \t| Train loss: 0.181 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 854 \t| Train loss: 0.181 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 855 \t| Train loss: 0.181 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 856 \t| Train loss: 0.181 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 857 \t| Train loss: 0.181 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 858 \t| Train loss: 0.181 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 859 \t| Train loss: 0.181 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 860 \t| Train loss: 0.181 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 861 \t| Train loss: 0.181 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 862 \t| Train loss: 0.181 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 863 \t| Train loss: 0.181 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 864 \t| Train loss: 0.181 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 865 \t| Train loss: 0.181 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 866 \t| Train loss: 0.181 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 867 \t| Train loss: 0.181 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 868 \t| Train loss: 0.181 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 869 \t| Train loss: 0.181 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 870 \t| Train loss: 0.181 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 871 \t| Train loss: 0.181 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 872 \t| Train loss: 0.181 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 873 \t| Train loss: 0.181 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 874 \t| Train loss: 0.181 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 875 \t| Train loss: 0.181 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 876 \t| Train loss: 0.181 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 877 \t| Train loss: 0.181 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 878 \t| Train loss: 0.181 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 879 \t| Train loss: 0.181 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 880 \t| Train loss: 0.181 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 881 \t| Train loss: 0.181 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 882 \t| Train loss: 0.181 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 883 \t| Train loss: 0.181 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 884 \t| Train loss: 0.181 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 885 \t| Train loss: 0.181 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 886 \t| Train loss: 0.181 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 887 \t| Train loss: 0.181 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 888 \t| Train loss: 0.181 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 889 \t| Train loss: 0.181 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 890 \t| Train loss: 0.181 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 891 \t| Train loss: 0.181 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 892 \t| Train loss: 0.181 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 893 \t| Train loss: 0.181 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 894 \t| Train loss: 0.181 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 895 \t| Train loss: 0.181 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 896 \t| Train loss: 0.181 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 897 \t| Train loss: 0.181 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 898 \t| Train loss: 0.181 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 899 \t| Train loss: 0.181 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 900 \t| Train loss: 0.181 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 901 \t| Train loss: 0.181 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 902 \t| Train loss: 0.181 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 903 \t| Train loss: 0.181 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 904 \t| Train loss: 0.181 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 905 \t| Train loss: 0.181 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 906 \t| Train loss: 0.181 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 907 \t| Train loss: 0.181 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 908 \t| Train loss: 0.181 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 909 \t| Train loss: 0.181 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 910 \t| Train loss: 0.181 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 911 \t| Train loss: 0.181 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 912 \t| Train loss: 0.181 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 913 \t| Train loss: 0.181 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 914 \t| Train loss: 0.181 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 915 \t| Train loss: 0.181 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 916 \t| Train loss: 0.181 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 917 \t| Train loss: 0.181 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 918 \t| Train loss: 0.181 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 919 \t| Train loss: 0.181 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 920 \t| Train loss: 0.181 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 921 \t| Train loss: 0.181 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 922 \t| Train loss: 0.181 \t| Train acc: 0.63 \t| Test acc: 0.51\n",
      "Epoch: 923 \t| Train loss: 0.181 \t| Train acc: 0.63 \t| Test acc: 0.52\n",
      "Epoch: 924 \t| Train loss: 0.181 \t| Train acc: 0.63 \t| Test acc: 0.52\n",
      "Epoch: 925 \t| Train loss: 0.181 \t| Train acc: 0.63 \t| Test acc: 0.52\n",
      "Epoch: 926 \t| Train loss: 0.181 \t| Train acc: 0.63 \t| Test acc: 0.52\n",
      "Epoch: 927 \t| Train loss: 0.181 \t| Train acc: 0.63 \t| Test acc: 0.52\n",
      "Epoch: 928 \t| Train loss: 0.181 \t| Train acc: 0.63 \t| Test acc: 0.52\n",
      "Epoch: 929 \t| Train loss: 0.181 \t| Train acc: 0.63 \t| Test acc: 0.52\n",
      "Epoch: 930 \t| Train loss: 0.181 \t| Train acc: 0.63 \t| Test acc: 0.52\n",
      "Epoch: 931 \t| Train loss: 0.181 \t| Train acc: 0.63 \t| Test acc: 0.52\n",
      "Epoch: 932 \t| Train loss: 0.181 \t| Train acc: 0.63 \t| Test acc: 0.52\n",
      "Epoch: 933 \t| Train loss: 0.181 \t| Train acc: 0.63 \t| Test acc: 0.52\n",
      "Epoch: 934 \t| Train loss: 0.181 \t| Train acc: 0.63 \t| Test acc: 0.52\n",
      "Epoch: 935 \t| Train loss: 0.181 \t| Train acc: 0.63 \t| Test acc: 0.52\n",
      "Epoch: 936 \t| Train loss: 0.181 \t| Train acc: 0.63 \t| Test acc: 0.52\n",
      "Epoch: 937 \t| Train loss: 0.181 \t| Train acc: 0.63 \t| Test acc: 0.52\n",
      "Epoch: 938 \t| Train loss: 0.181 \t| Train acc: 0.63 \t| Test acc: 0.52\n",
      "Epoch: 939 \t| Train loss: 0.181 \t| Train acc: 0.63 \t| Test acc: 0.52\n",
      "Epoch: 940 \t| Train loss: 0.181 \t| Train acc: 0.63 \t| Test acc: 0.52\n",
      "Epoch: 941 \t| Train loss: 0.181 \t| Train acc: 0.63 \t| Test acc: 0.52\n",
      "Epoch: 942 \t| Train loss: 0.181 \t| Train acc: 0.63 \t| Test acc: 0.52\n",
      "Epoch: 943 \t| Train loss: 0.18 \t| Train acc: 0.63 \t| Test acc: 0.52\n",
      "Epoch: 944 \t| Train loss: 0.18 \t| Train acc: 0.63 \t| Test acc: 0.52\n",
      "Epoch: 945 \t| Train loss: 0.18 \t| Train acc: 0.63 \t| Test acc: 0.52\n",
      "Epoch: 946 \t| Train loss: 0.18 \t| Train acc: 0.63 \t| Test acc: 0.52\n",
      "Epoch: 947 \t| Train loss: 0.18 \t| Train acc: 0.63 \t| Test acc: 0.52\n",
      "Epoch: 948 \t| Train loss: 0.18 \t| Train acc: 0.63 \t| Test acc: 0.52\n",
      "Epoch: 949 \t| Train loss: 0.18 \t| Train acc: 0.63 \t| Test acc: 0.52\n",
      "Epoch: 950 \t| Train loss: 0.18 \t| Train acc: 0.63 \t| Test acc: 0.52\n",
      "Epoch: 951 \t| Train loss: 0.18 \t| Train acc: 0.63 \t| Test acc: 0.52\n",
      "Epoch: 952 \t| Train loss: 0.18 \t| Train acc: 0.63 \t| Test acc: 0.52\n",
      "Epoch: 953 \t| Train loss: 0.18 \t| Train acc: 0.63 \t| Test acc: 0.52\n",
      "Epoch: 954 \t| Train loss: 0.18 \t| Train acc: 0.63 \t| Test acc: 0.52\n",
      "Epoch: 955 \t| Train loss: 0.18 \t| Train acc: 0.63 \t| Test acc: 0.52\n",
      "Epoch: 956 \t| Train loss: 0.18 \t| Train acc: 0.63 \t| Test acc: 0.52\n",
      "Epoch: 957 \t| Train loss: 0.18 \t| Train acc: 0.63 \t| Test acc: 0.52\n",
      "Epoch: 958 \t| Train loss: 0.18 \t| Train acc: 0.63 \t| Test acc: 0.52\n",
      "Epoch: 959 \t| Train loss: 0.18 \t| Train acc: 0.63 \t| Test acc: 0.52\n",
      "Epoch: 960 \t| Train loss: 0.18 \t| Train acc: 0.63 \t| Test acc: 0.52\n",
      "Epoch: 961 \t| Train loss: 0.18 \t| Train acc: 0.63 \t| Test acc: 0.52\n",
      "Epoch: 962 \t| Train loss: 0.18 \t| Train acc: 0.63 \t| Test acc: 0.52\n",
      "Epoch: 963 \t| Train loss: 0.18 \t| Train acc: 0.63 \t| Test acc: 0.52\n",
      "Epoch: 964 \t| Train loss: 0.18 \t| Train acc: 0.63 \t| Test acc: 0.52\n",
      "Epoch: 965 \t| Train loss: 0.18 \t| Train acc: 0.63 \t| Test acc: 0.52\n",
      "Epoch: 966 \t| Train loss: 0.18 \t| Train acc: 0.63 \t| Test acc: 0.52\n",
      "Epoch: 967 \t| Train loss: 0.18 \t| Train acc: 0.63 \t| Test acc: 0.52\n",
      "Epoch: 968 \t| Train loss: 0.18 \t| Train acc: 0.63 \t| Test acc: 0.52\n",
      "Epoch: 969 \t| Train loss: 0.18 \t| Train acc: 0.64 \t| Test acc: 0.52\n",
      "Epoch: 970 \t| Train loss: 0.18 \t| Train acc: 0.64 \t| Test acc: 0.52\n",
      "Epoch: 971 \t| Train loss: 0.18 \t| Train acc: 0.64 \t| Test acc: 0.52\n",
      "Epoch: 972 \t| Train loss: 0.18 \t| Train acc: 0.64 \t| Test acc: 0.52\n",
      "Epoch: 973 \t| Train loss: 0.18 \t| Train acc: 0.64 \t| Test acc: 0.52\n",
      "Epoch: 974 \t| Train loss: 0.18 \t| Train acc: 0.64 \t| Test acc: 0.52\n",
      "Epoch: 975 \t| Train loss: 0.18 \t| Train acc: 0.64 \t| Test acc: 0.52\n",
      "Epoch: 976 \t| Train loss: 0.18 \t| Train acc: 0.64 \t| Test acc: 0.52\n",
      "Epoch: 977 \t| Train loss: 0.18 \t| Train acc: 0.64 \t| Test acc: 0.52\n",
      "Epoch: 978 \t| Train loss: 0.18 \t| Train acc: 0.64 \t| Test acc: 0.52\n",
      "Epoch: 979 \t| Train loss: 0.18 \t| Train acc: 0.64 \t| Test acc: 0.52\n",
      "Epoch: 980 \t| Train loss: 0.18 \t| Train acc: 0.64 \t| Test acc: 0.52\n",
      "Epoch: 981 \t| Train loss: 0.18 \t| Train acc: 0.64 \t| Test acc: 0.52\n",
      "Epoch: 982 \t| Train loss: 0.18 \t| Train acc: 0.64 \t| Test acc: 0.52\n",
      "Epoch: 983 \t| Train loss: 0.18 \t| Train acc: 0.64 \t| Test acc: 0.52\n",
      "Epoch: 984 \t| Train loss: 0.18 \t| Train acc: 0.64 \t| Test acc: 0.52\n",
      "Epoch: 985 \t| Train loss: 0.18 \t| Train acc: 0.64 \t| Test acc: 0.52\n",
      "Epoch: 986 \t| Train loss: 0.18 \t| Train acc: 0.64 \t| Test acc: 0.52\n",
      "Epoch: 987 \t| Train loss: 0.18 \t| Train acc: 0.64 \t| Test acc: 0.52\n",
      "Epoch: 988 \t| Train loss: 0.18 \t| Train acc: 0.64 \t| Test acc: 0.52\n",
      "Epoch: 989 \t| Train loss: 0.18 \t| Train acc: 0.64 \t| Test acc: 0.52\n",
      "Epoch: 990 \t| Train loss: 0.18 \t| Train acc: 0.64 \t| Test acc: 0.52\n",
      "Epoch: 991 \t| Train loss: 0.18 \t| Train acc: 0.64 \t| Test acc: 0.52\n",
      "Epoch: 992 \t| Train loss: 0.18 \t| Train acc: 0.64 \t| Test acc: 0.52\n",
      "Epoch: 993 \t| Train loss: 0.18 \t| Train acc: 0.64 \t| Test acc: 0.52\n",
      "Epoch: 994 \t| Train loss: 0.18 \t| Train acc: 0.64 \t| Test acc: 0.52\n",
      "Epoch: 995 \t| Train loss: 0.18 \t| Train acc: 0.64 \t| Test acc: 0.52\n",
      "Epoch: 996 \t| Train loss: 0.18 \t| Train acc: 0.64 \t| Test acc: 0.52\n",
      "Epoch: 997 \t| Train loss: 0.18 \t| Train acc: 0.64 \t| Test acc: 0.52\n",
      "Epoch: 998 \t| Train loss: 0.18 \t| Train acc: 0.64 \t| Test acc: 0.52\n",
      "Epoch: 999 \t| Train loss: 0.18 \t| Train acc: 0.64 \t| Test acc: 0.52\n"
     ]
    }
   ],
   "source": [
    "train_losses_mlpsl_lim = []\n",
    "#test_losses  = []\n",
    "train_accs_mlpsl_lim = []\n",
    "test_accs_mlpsl_lim  = []\n",
    "\n",
    "for epoch in range(1000):\n",
    "    for features, labels in data_loader_lim:\n",
    "        train_preds_mlpsl_lim = mlpsl_model_lim(features)\n",
    "        class_weights_batch = class_weights_original[labels.long()]\n",
    "        loss_function = nn.BCELoss(weight=class_weights_batch)\n",
    "\n",
    "        credit_score_mlpsl_lim = features[:, credit_score_index_lim].reshape(-1, 1)\n",
    "        short_term_mlpsl_lim = features[:, short_term_index_lim].reshape(-1, 1)\n",
    "        annual_income_mlpsl_lim = features[:, annual_income_index_lim].reshape(-1, 1)\n",
    "\n",
    "        rule_mlpsl_lim = torch.logical_or(torch.logical_and(credit_score_mlpsl_lim<=684.5,  annual_income_mlpsl_lim<=2489636.5), torch.logical_and(credit_score_mlpsl_lim>684.5, annual_income_mlpsl_lim<=1271242.5))\n",
    "        #rule_mlpsl_lim= torch.logical_or(credit_score_mlpsl_lim>3300.5, torch.logical_and(credit_score_mlpsl_lim<=3300.5,torch.logical_and(short_term_mlpsl_lim<=0.5, annual_income_mlpsl_lim<=1405107))).float()\n",
    "        \n",
    "        train_loss_mlpsl_lim = semantic_loss(train_preds_mlpsl_lim, labels, rule_mlpsl_lim, class_weights_batch, 0.01)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            test_preds_mlpsl_lim = mlpsl_model_lim(X_validation_tensor_lim)\n",
    "            #test_loss  = loss_function(test_preds, y_test)\n",
    "\n",
    "        train_acc = calculate_accuracy(train_preds_mlpsl_lim, labels)\n",
    "        test_acc  = calculate_accuracy(test_preds_mlpsl_lim, y_validation_tensor)\n",
    "\n",
    "        optimizer_mlpsl_lim.zero_grad()\n",
    "        train_loss_mlpsl_lim.backward()\n",
    "\n",
    "        optimizer_mlpsl_lim.step()\n",
    "\n",
    "        train_losses_mlpsl_lim.append(train_loss_mlpsl_lim.item())\n",
    "        #test_losses.append(test_loss.item())\n",
    "        train_accs_mlpsl_lim.append(train_acc.item())\n",
    "        test_accs_mlpsl_lim.append(test_acc.item())\n",
    "\n",
    "    #if epoch%100==0:\n",
    "    print(f'Epoch: {epoch} \\t|' \\\n",
    "          f' Train loss: {np.round(train_loss_mlpsl_lim.item(),3)} \\t|' \\\n",
    "              #f' Test loss: {np.round(test_loss.item(),3)} \\t|' \\\n",
    "          f' Train acc: {np.round(train_acc.item(),2)} \\t|' \\\n",
    "          f' Test acc: {np.round(test_acc.item(),2)}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-22T16:48:24.032222800Z",
     "start_time": "2024-06-22T16:48:16.321852900Z"
    }
   },
   "id": "18b704485be63434"
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Result:\n",
      "================================================\n",
      "Accuracy Score: 52.23%\n",
      "_______________________________________________\n",
      "CLASSIFICATION REPORT:\n",
      "                   0.0         1.0  accuracy    macro avg  weighted avg\n",
      "precision     0.787730    0.256300   0.52226     0.522015      0.663235\n",
      "recall        0.514836    0.546527   0.52226     0.530681      0.522260\n",
      "f1-score      0.622696    0.348954   0.52226     0.485825      0.558568\n",
      "support    2494.000000  763.000000   0.52226  3257.000000   3257.000000\n",
      "_______________________________________________\n",
      "Confusion Matrix: \n",
      " [[1284 1210]\n",
      " [ 346  417]]\n"
     ]
    }
   ],
   "source": [
    "evaluate_nn(y_validation_tensor.clone().detach(), test_preds_mlpsl_lim.clone().detach().round(), train=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-22T16:48:26.017179100Z",
     "start_time": "2024-06-22T16:48:25.768014Z"
    }
   },
   "id": "1c050ba10d92f23b"
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Result:\n",
      "================================================\n",
      "Accuracy Score: 51.96%\n",
      "_______________________________________________\n",
      "CLASSIFICATION REPORT:\n",
      "                   0.0         1.0  accuracy    macro avg  weighted avg\n",
      "precision     0.769557    0.271248  0.519649     0.520403      0.644528\n",
      "recall        0.512107    0.542169  0.519649     0.527138      0.519649\n",
      "f1-score      0.614975    0.361591  0.519649     0.488283      0.551399\n",
      "support    2478.000000  830.000000  0.519649  3308.000000   3308.000000\n",
      "_______________________________________________\n",
      "Confusion Matrix: \n",
      " [[1269 1209]\n",
      " [ 380  450]]\n"
     ]
    }
   ],
   "source": [
    "test_preds_mlpsl_lim = mlpsl_model_lim(X_test_tensor_lim)\n",
    "evaluate_nn(y_test_tensor.clone().detach(), test_preds_mlpsl_lim.clone().detach().round(), train=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-22T16:48:27.221913700Z",
     "start_time": "2024-06-22T16:48:27.035988700Z"
    }
   },
   "id": "f5765a21efc815b3"
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "outputs": [
    {
     "data": {
      "text/plain": "1612"
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "credit_score_test = torch.tensor(X_test['Credit Score'].values, dtype=torch.float32).reshape(len(X_test), 1)\n",
    "short_term_test = torch.tensor(X_test['Term_Short Term'].values, dtype=torch.float32).reshape(len(X_test), 1)\n",
    "annual_income_test = torch.tensor(X_test['Annual Income'].values, dtype=torch.float32).reshape(len(X_test), 1)\n",
    "\n",
    "rule_test = torch.logical_or(\n",
    "    torch.logical_and(credit_score_test <= 684.5, annual_income_test <= 2489636.5),\n",
    "    torch.logical_and(credit_score_test > 684.5, annual_income_test <= 1271242.5)\n",
    ")\n",
    "rule_matched = torch.nonzero(rule_test.float(), as_tuple=True)[0].tolist()\n",
    "yespred = torch.nonzero(test_preds_mlpsl_lim.round(), as_tuple=True)[0].tolist()\n",
    "same = [value for value in yespred if value in rule_matched]\n",
    "len(same)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-22T16:33:03.204341600Z",
     "start_time": "2024-06-22T16:33:02.804916700Z"
    }
   },
   "id": "54f363a4d43dd06b"
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Result:\n",
      "================================================\n",
      "Accuracy Score: 61.76%\n",
      "_______________________________________________\n",
      "CLASSIFICATION REPORT:\n",
      "                   0.0         1.0  accuracy    macro avg  weighted avg\n",
      "precision     0.751973    0.258602  0.617594     0.505287      0.628183\n",
      "recall        0.730428    0.280723  0.617594     0.505575      0.617594\n",
      "f1-score      0.741044    0.269209  0.617594     0.505126      0.622657\n",
      "support    2478.000000  830.000000  0.617594  3308.000000   3308.000000\n",
      "_______________________________________________\n",
      "Confusion Matrix: \n",
      " [[1810  668]\n",
      " [ 597  233]]\n"
     ]
    }
   ],
   "source": [
    "dt_model_lim = DecisionTreeClassifier(class_weight='balanced')\n",
    "dt_model_lim.fit(X_train_lim_scaled, y_train_lim)\n",
    "\n",
    "predictions_dt_lim = dt_model_lim.predict(X_test_lim_scaled)\n",
    "\n",
    "evaluate_nn(y_test_tensor.clone().detach(), predictions_dt_lim.round(), train=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-13T14:38:13.401771800Z",
     "start_time": "2024-06-13T14:38:13.345340400Z"
    }
   },
   "id": "2a1aa0deb4d47cf0"
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Result:\n",
      "================================================\n",
      "Accuracy Score: 63.81%\n",
      "_______________________________________________\n",
      "CLASSIFICATION REPORT:\n",
      "                   0.0         1.0  accuracy    macro avg  weighted avg\n",
      "precision     0.763255    0.290286   0.63815     0.526770      0.644584\n",
      "recall        0.749395    0.306024   0.63815     0.527709      0.638150\n",
      "f1-score      0.756261    0.297947   0.63815     0.527104      0.641267\n",
      "support    2478.000000  830.000000   0.63815  3308.000000   3308.000000\n",
      "_______________________________________________\n",
      "Confusion Matrix: \n",
      " [[1857  621]\n",
      " [ 576  254]]\n"
     ]
    }
   ],
   "source": [
    "xgb_model_lim = xgb.XGBClassifier(scale_pos_weight=(len(y_train_lim) - sum(y_train_lim)) / sum(y_train_lim))\n",
    "\n",
    "xgb_model_lim.fit(X_train_lim_scaled, y_train_lim)\n",
    "\n",
    "predictions_xgb_lim = xgb_model_lim.predict(X_test_lim_scaled)\n",
    "\n",
    "evaluate_nn(y_test_tensor.clone().detach(), predictions_xgb_lim.round(), train=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-22T16:58:38.701824100Z",
     "start_time": "2024-06-22T16:58:38.439093900Z"
    }
   },
   "id": "8c8399f32b0ae82b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Creating conformal predictions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "67b15a4e0faa64c8"
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "outputs": [],
   "source": [
    "n = len(X_calibration)\n",
    "predictions = lr_model(X_calibration_tensor)\n",
    "true_class_probs = torch.where(y_calibration_tensor == 1, predictions, 1 - predictions)\n",
    "scores = 1-true_class_probs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-22T17:04:59.783834800Z",
     "start_time": "2024-06-22T17:04:59.658667800Z"
    }
   },
   "id": "8cf7f84656fda39"
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "outputs": [
    {
     "data": {
      "text/plain": "0.7777948"
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha = 0.05\n",
    "\n",
    "q_level = np.ceil((n+1)*(1-alpha))/n\n",
    "qhat = np.quantile(scores, q_level, method='higher')\n",
    "qhat"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-22T17:08:54.721908700Z",
     "start_time": "2024-06-22T17:08:54.532475700Z"
    }
   },
   "id": "be55d1f8b34108ce"
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'detach'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[261], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m plt\u001B[38;5;241m.\u001B[39mhist(predictions\u001B[38;5;241m.\u001B[39mdetach()\u001B[38;5;241m.\u001B[39mnumpy(), bins\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m30\u001B[39m, \u001B[38;5;28mrange\u001B[39m\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m))\n\u001B[0;32m      2\u001B[0m plt\u001B[38;5;241m.\u001B[39mxlabel(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m1 - s(y,x)\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m      3\u001B[0m plt\u001B[38;5;241m.\u001B[39mylabel(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFrequency\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'numpy.ndarray' object has no attribute 'detach'"
     ]
    }
   ],
   "source": [
    "plt.hist(predictions.detach().numpy(), bins=30, range=(0, 1))\n",
    "plt.xlabel(\"1 - s(y,x)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Histogram of non-conformity scores\")\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-22T17:07:40.554946700Z",
     "start_time": "2024-06-22T17:07:40.316057200Z"
    }
   },
   "id": "9529b61016f51223"
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Charged Off'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off'], ['Charged Off'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off'], ['Fully Paid'], ['Charged Off'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off'], ['Fully Paid'], ['Charged Off'], ['Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off'], ['Fully Paid'], ['Charged Off'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off'], ['Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off'], ['Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off'], ['Charged Off'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off'], ['Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off', 'Fully Paid'], ['Charged Off'], ['Charged Off', 'Fully Paid']]\n"
     ]
    }
   ],
   "source": [
    "prediction_sets = []\n",
    "\n",
    "for prob in predictions_xgb_lim_proba[:,1]:\n",
    "    prediction_set = []\n",
    "    if prob > 1-qhat:\n",
    "        prediction_set.append(\"Charged Off\")\n",
    "    if 1 - prob > 1-qhat:\n",
    "        prediction_set.append(\"Fully Paid\")\n",
    "    prediction_sets.append(prediction_set)\n",
    "\n",
    "print(prediction_sets)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-22T17:08:59.572730600Z",
     "start_time": "2024-06-22T17:08:59.430793800Z"
    }
   },
   "id": "e204841bc49da198"
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of prediction sets of length 1: 742\n",
      "Number of prediction sets of length 2: 2566\n"
     ]
    }
   ],
   "source": [
    "count_length_1 = 0\n",
    "count_length_2 = 0\n",
    "\n",
    "for prediction_set in prediction_sets:\n",
    "    if len(prediction_set) == 1:\n",
    "        count_length_1 += 1\n",
    "    elif len(prediction_set) == 2:\n",
    "        count_length_2 += 1\n",
    "\n",
    "print(\"Number of prediction sets of length 1:\", count_length_1)\n",
    "print(\"Number of prediction sets of length 2:\", count_length_2)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-22T17:09:03.441972800Z",
     "start_time": "2024-06-22T17:09:03.219607400Z"
    }
   },
   "id": "144f9067a73ba6b6"
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions shape: (3307, 2)\n",
      "True class probabilities shape: (3307,)\n",
      "Uncertainty scores shape: (3307,)\n",
      "Sample predictions: [[0.742601   0.25739902]\n",
      " [0.50895983 0.49104017]\n",
      " [0.66303027 0.33696973]\n",
      " [0.61551404 0.38448593]\n",
      " [0.43267548 0.5673245 ]]\n",
      "Sample true class probabilities: [0.742601   0.50895983 0.66303027 0.38448593 0.43267548]\n",
      "Sample uncertainty scores: [0.25739902 0.49104017 0.33696973 0.61551404 0.5673245 ]\n"
     ]
    }
   ],
   "source": [
    "predictions = xgb_model.predict_proba(X_calibration_scaled)\n",
    "predictions_xgb_lim_proba = xgb_model.predict_proba(X_test_scaled)\n",
    "y_calibration = np.array(y_calibration)\n",
    "\n",
    "true_class_probs = np.array([predictions[i, y_calibration[i]] for i in range(len(y_calibration))])\n",
    "\n",
    "scores = 1 - true_class_probs\n",
    "\n",
    "print(\"Predictions shape:\", predictions.shape)\n",
    "print(\"True class probabilities shape:\", true_class_probs.shape)\n",
    "print(\"Uncertainty scores shape:\", scores.shape)\n",
    "\n",
    "print(\"Sample predictions:\", predictions[:5])\n",
    "print(\"Sample true class probabilities:\", true_class_probs[:5])\n",
    "print(\"Sample uncertainty scores:\", scores[:5])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-22T17:08:44.859054900Z",
     "start_time": "2024-06-22T17:08:44.602131300Z"
    }
   },
   "id": "df394bcf07d55b31"
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "outputs": [],
   "source": [
    "def calculate_coverage(prediction_sets, y_test):\n",
    "    correct_predictions = 0\n",
    "\n",
    "    for i, prediction_set in enumerate(prediction_sets):\n",
    "        if y_test[i] in prediction_set:\n",
    "            correct_predictions += 1\n",
    "\n",
    "    coverage_percentage = (correct_predictions / len(y_test)) * 100\n",
    "    return coverage_percentage"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-22T17:42:18.331339700Z",
     "start_time": "2024-06-22T17:42:18.209717200Z"
    }
   },
   "id": "1e236d279f9f65d2"
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coverage: 99.09%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "predictions = xgb_model.predict_proba(X_calibration_scaled)\n",
    "predictions_xgb_proba = xgb_model.predict_proba(X_test_scaled)\n",
    "y_calibration = np.array(y_calibration)\n",
    "\n",
    "true_class_probs = np.array([predictions[i, y_calibration[i]] for i in range(len(y_calibration))])\n",
    "\n",
    "scores = 1 - true_class_probs\n",
    "\n",
    "alpha = 0.01\n",
    "\n",
    "q_level = np.ceil((n+1)*(1-alpha))/n\n",
    "qhat = np.quantile(scores, q_level, method='higher')\n",
    "\n",
    "prediction_sets=[]\n",
    "\n",
    "for prob in predictions_xgb_lim_proba[:,1]:\n",
    "    prediction_set = []\n",
    "    if prob > 1-qhat:\n",
    "        prediction_set.append(1)\n",
    "    if 1 - prob > 1-qhat:\n",
    "        prediction_set.append(0)\n",
    "    prediction_sets.append(prediction_set)\n",
    "\n",
    "coverage = calculate_coverage(prediction_sets, y_test.tolist())\n",
    "print(f\"Coverage: {coverage:.2f}%\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-22T17:50:10.092215Z",
     "start_time": "2024-06-22T17:50:09.874301500Z"
    }
   },
   "id": "f76472bbe23ca3fb"
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coverage: 0.7576\n",
      "Number of high-confidence predictions: 66\n"
     ]
    }
   ],
   "source": [
    "probabilities = test_preds_mlpsl_lim\n",
    "threshold = 0.95\n",
    "\n",
    "high_confidence_mask = (probabilities >= threshold) | (probabilities <= 1 - threshold)\n",
    "\n",
    "binary_predictions = (probabilities >= 0.5).float()\n",
    "\n",
    "correct_high_confidence_predictions = (binary_predictions[high_confidence_mask] == y_test_tensor[high_confidence_mask]).float()\n",
    "coverage = correct_high_confidence_predictions.mean().item()\n",
    "total_high_confidence = high_confidence_mask.sum().item()\n",
    "\n",
    "print(f\"Coverage: {round(coverage, 4)}\")\n",
    "print(f\"Number of high-confidence predictions: {total_high_confidence}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-22T17:58:59.168231300Z",
     "start_time": "2024-06-22T17:58:58.854165Z"
    }
   },
   "id": "f258272d202deae5"
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coverage: 94.56%\n"
     ]
    }
   ],
   "source": [
    "n = len(X_calibration)\n",
    "predictions = mlpsl_model_lim(X_calibration_tensor_lim)\n",
    "true_class_probs = torch.where(y_calibration_tensor == 1, predictions, 1 - predictions)\n",
    "scores = 1-true_class_probs\n",
    "\n",
    "alpha = 0.05\n",
    "\n",
    "q_level = np.ceil((n+1)*(1-alpha))/n\n",
    "#qhat = np.quantile(scores.clone().detach(), q_level, method='higher')\n",
    "qhat = np.quantile(scores.detach().clone().numpy(), q_level, method='higher')\n",
    "\n",
    "prediction_sets = []\n",
    "\n",
    "for prob in test_preds_mlpsl_lim:\n",
    "    prediction_set = []\n",
    "    if prob > 1-qhat:\n",
    "        prediction_set.append(1)\n",
    "    if 1 - prob > 1-qhat:\n",
    "        prediction_set.append(0)\n",
    "    prediction_sets.append(prediction_set)\n",
    "\n",
    "coverage = calculate_coverage(prediction_sets, y_test.tolist())\n",
    "print(f\"Coverage: {coverage:.2f}%\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-22T18:05:16.909916600Z",
     "start_time": "2024-06-22T18:05:15.939941700Z"
    }
   },
   "id": "f565812c2b1aac55"
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "outputs": [],
   "source": [
    "rounded_preds_lr = torch.round(test_preds_mlp_lim)\n",
    "rounded_preds_sl = torch.round(test_preds_mlpsl_lim)\n",
    "\n",
    "different_predictions = (rounded_preds_lr != rounded_preds_sl)\n",
    "\n",
    "correct_predictions_lr = (rounded_preds_lr == y_test_tensor)\n",
    "correct_predictions_sl = (rounded_preds_sl == y_test_tensor)\n",
    "\n",
    "different_and_correct_lr = different_predictions & correct_predictions_lr\n",
    "different_and_correct_sl = different_predictions & correct_predictions_sl\n",
    "\n",
    "indices_different_and_correct_lr = torch.nonzero(different_and_correct_lr).flatten()\n",
    "indices_different_and_correct_sl = torch.nonzero(different_and_correct_sl).flatten()\n",
    "\n",
    "only_sl_correct = torch.nonzero(different_and_correct_sl.float(), as_tuple=True)[0].tolist()\n",
    "only_lr_correct = torch.nonzero(different_and_correct_lr.float(), as_tuple=True)[0].tolist()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-06T16:04:27.489924200Z",
     "start_time": "2024-06-06T16:04:27.427342400Z"
    }
   },
   "id": "ba28d7c68e91101a"
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All the cases:  370\n",
      "Meet the rule:  200\n",
      "True class:  9\n"
     ]
    }
   ],
   "source": [
    "only_correct = only_lr_correct\n",
    "condition1 = (X_test.iloc[only_correct]['Credit Score'] <= 684.5) & (X_test.iloc[only_correct]['Annual Income'] <= 2489636.5)\n",
    "condition2 = (X_test.iloc[only_correct]['Credit Score'] > 684.5) & (X_test.iloc[only_correct]['Annual Income'] <= 1271242.5)\n",
    "\n",
    "combined_condition = condition1 | condition2\n",
    "\n",
    "print('All the cases: ',len(X_test.iloc[only_correct][['Credit Score', 'Annual Income']]))\n",
    "print('Meet the rule: ',len(X_test.iloc[only_correct][combined_condition]))\n",
    "#print('Meet the rule: ',sum(X_test.iloc[only_lr_correct]['Credit Score']<652.5))\n",
    "print('True class: ',sum(y_test.iloc[only_correct]))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-06T16:04:52.427393500Z",
     "start_time": "2024-06-06T16:04:52.273086600Z"
    }
   },
   "id": "b348e509d0a5542b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "91620b959b2a169c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
